---
title: "Feature Engineering 101: Prepping Your Ingredients for Success"
author: "Numbers around us"
date: "2025-02-11"
format: html
---

![](images/FE_1.jpg)

### **What is Feature Engineering?**

Feature engineering is the process of transforming raw data into **meaningful, informative variables** that improve the predictive power of models. While machine learning algorithms can recognize patterns in data, they rely on well-prepared inputs to make accurate predictions.

Just like a chef carefully preps ingredients before cooking, a data scientist must refine raw data **before** passing it to a model. **Messy, irrelevant, or unoptimized data can lead to poor performance, even with the best algorithms.**

Think about cooking:

-   If you use high-quality, well-prepared ingredients, the final dish will be **delicious and well-balanced**.

-   If you throw in **random, untested elements**, your dish (model) may **taste awful or even fail**.

Similarly, the difference between an **amateur and a professional chef** is knowing:\
‚úîÔ∏è What ingredients (features) work best together.\
‚úîÔ∏è Which ones to remove to avoid ruining the dish (model).\
‚úîÔ∏è How to adjust flavors (transformations) for the best final result.

### **Why Feature Engineering is Crucial in Data Science**

Even the most **advanced machine learning models rely on quality data**. No amount of hyperparameter tuning can compensate for **bad feature selection or poor transformations**.

#### **Good Feature Engineering Leads to:**

‚úÖ **Higher Predictive Accuracy:** Well-chosen features improve a model‚Äôs ability to generalize.\
‚úÖ **Faster Model Training:** Reducing unnecessary features speeds up training.\
‚úÖ **Better Interpretability:** Features that make sense allow for easier debugging and analysis.\
‚úÖ **Reduced Overfitting:** Eliminating redundant or misleading features helps prevent the model from memorizing noise.

Without feature engineering, you may be using **irrelevant, noisy, or redundant data**, which can lead to **worse results, longer training times, and overfitting**.

### **How Features Impact Model Performance: A Real-World Example**

Let‚Äôs say we are building a machine learning model to predict **house prices** based on available data. Our dataset contains:

-   **Square footage** of the house

-   **Number of bedrooms**

-   **Number of bathrooms**

-   **Location**

-   **Age of the house**

This is a **decent** starting point, but it may not be enough for a high-quality model.

#### **How Feature Engineering Improves This Dataset:**

| Feature | Raw Form | Engineered Version | Why It‚Äôs Useful |
|----|----|----|----|
| Square Footage | 2500 | Price per square foot = price / sqft | Normalizes pricing across house sizes |
| Location | ‚ÄúDowntown‚Äù | Distance to city center (km) | Captures geographic price variation |
| Bedrooms & Bathrooms | 3 beds, 2 baths | Room-to-bathroom ratio = beds/baths | Reflects usability of space |
| House Age | 15 years | Has Renovation (1/0) | Highlights recent renovations |
| Lot Size | 5000 sqft | Lot Size to House Ratio = lot sqft / house sqft | Identifies whether land size impacts pricing |

Each of these **engineered features** can add valuable insights, making the model **more robust and predictive**.

### **When Do You Need Feature Engineering?**

Not every dataset requires extensive feature engineering, but **certain situations** make it absolutely necessary:

‚úÖ **When your raw data lacks meaningful predictors:**

-   Example: Predicting user churn in an app based only on **login frequency**. A better approach might involve **time-based features** (e.g., ‚Äúdays since last login,‚Äù ‚Äúaverage session length‚Äù).

‚úÖ **When raw variables have nonlinear relationships with the target:**

-   Example: The relationship between **income and spending habits** may not be linear. **Log transformations or percentiles** can help normalize these relationships.

‚úÖ **When there are many categorical variables:**

-   Example: A dataset with **country names** will be more useful if transformed into **continent-based grouping** or encoded into **clustering-friendly formats**.

‚úÖ **When working with time-series data:**

-   Example: Instead of using **raw timestamps**, extracting **day of the week, month, quarter, and seasonality patterns** can reveal meaningful trends.

### **Common Challenges in Feature Engineering**

Feature engineering is **powerful**, but it comes with its own set of **challenges**:

‚ùå **Overfitting to Training Data** ‚Üí Adding too many engineered features **may create spurious correlations** that don‚Äôt generalize well.\
‚ùå **High-Dimensional Data** ‚Üí If you add **too many features**, it can increase complexity without meaningful improvement.\
‚ùå **Correlation Between Features** ‚Üí Some features may be **redundant or highly correlated**, reducing the benefit of adding them.\
‚ùå **Computational Cost** ‚Üí Complex transformations (e.g., **Fourier features for time-series data**) may be **too expensive for real-time applications**.

### **Summary: Why Feature Engineering is Like Cooking**

-   Just like a great meal depends on **carefully prepped ingredients**, a great model depends on **carefully chosen features**.

-   Some raw ingredients (features) need to be **cleaned, processed, and refined** to be useful.

-   Too many ingredients (features) can **overwhelm** the dish (model), making it **confusing and ineffective**.

-   **The right balance** leads to the **best results**‚Äîfor both cooking and machine learning.

## **What Makes a Good Feature?**

### **Defining a ‚ÄúGood‚Äù Feature**

Not all features are created equal. Some **enhance** a model‚Äôs predictive power, while others **add noise or redundancy**. The difference between a good and bad feature is similar to choosing the right **ingredients for a recipe**‚Äîsome elements **elevate the dish**, while others **clash or dilute the flavors**.

#### **A Good Feature Should Be:**

‚úÖ **Relevant** ‚Äì It has a meaningful relationship with the target variable.\
‚úÖ **Predictive** ‚Äì It improves the model‚Äôs ability to make accurate predictions.\
‚úÖ **Independent** ‚Äì It adds new information rather than repeating existing data.\
‚úÖ **Interpretable** ‚Äì It makes logical sense and can be explained to stakeholders.\
‚úÖ **Efficient** ‚Äì It balances performance improvement with computational cost.

### **1Ô∏è‚É£ Relevance: Does the Feature Matter for Prediction?**

A feature is **relevant** if it has a meaningful **correlation** with the outcome we‚Äôre trying to predict.

#### **Example: Predicting Car Prices**

If we are building a model to predict **car prices**, which of the following features are relevant?

| Feature | Relevant? | Why? |
|----|----|----|
| Engine Size | ‚úÖ Yes | Larger engines generally increase car price. |
| Number of Cup Holders | ‚ùå No | This has little to no impact on price. |
| Brand Name | ‚úÖ Yes | Premium brands usually have higher prices. |
| Car Color | ‚ùì Maybe | If certain colors have higher resale value, it may be relevant. |

üëâ **Lesson:** Just because a variable is available in the dataset doesn‚Äôt mean it‚Äôs useful.

#### **How to Test Feature Relevance?**

-   **Correlation (Pearson/Spearman):** Measures how strongly a numerical feature is related to the target variable.

-   **ANOVA/F-test:** Tests whether categorical variables significantly impact the target.

-   **Mutual Information:** Measures **nonlinear** relationships between features and the target.

### **2Ô∏è‚É£ Predictiveness: Does the Feature Improve Model Accuracy?**

Even if a feature is **somewhat relevant**, it may not meaningfully improve predictions. **Features with low predictive power add noise** rather than useful signals.

#### **Example: Predicting Employee Turnover**

We want to predict whether an employee will leave a company. Consider these two features:

1.  **Salary Increase in the Last Year (%)**

2.  **Employee's ID Number**

‚úîÔ∏è The **salary increase** may **predict retention** (employees with high raises may stay).\
‚ùå The **employee ID number** has no meaningful impact on turnover.

#### **How to Measure Predictiveness?**

-   **Train a simple model** using just one feature at a time.

-   **Check model accuracy:** If removing the feature **doesn‚Äôt change accuracy**, it‚Äôs probably **useless**.

-   **Feature importance scores** from decision trees or SHAP values.

### **3Ô∏è‚É£ Independence: Does the Feature Add Unique Information?**

A good feature should provide **new insights** rather than **duplicate existing data**.

### **Example: Redundant Features in a Dataset**

We are predicting **house prices** and have these features:

-   **House Size (sqft)**

-   **Number of Rooms**

-   **Number of Bedrooms**

The **number of rooms** is already captured by **house size**‚Äîit **doesn‚Äôt add much new information**.

#### **How to Detect Redundancy?**

-   **Correlation Matrix:** If two features have a correlation \> 0.9, one can often be removed.

-   **Variance Inflation Factor (VIF):** High values indicate redundant predictors.

-   **PCA (Principal Component Analysis):** Reduces redundancy by creating **new composite features**.

### **4Ô∏è‚É£ Interpretability: Can You Explain It?**

Features should make logical sense to **humans**, not just models.

### **Example: Black-Box vs. Explainable Features**

A deep learning model may create **complex interactions** between features that work well but are hard to interpret.

üí° If stakeholders need to understand **why** a model is making decisions (e.g., in finance or healthcare), **simple, well-explained features** are preferable.

#### **How to Improve Interpretability?**

-   Use **domain knowledge** to select meaningful features.

-   Avoid overly **complex transformations** (e.g., high-degree polynomial features).

-   Use **SHAP values** to explain feature impact.

### **5Ô∏è‚É£ Efficiency: Is the Feature Computationally Feasible?**

Some features require **significant processing power**. If a feature takes too long to compute, it may **not be worth the cost**.

#### **Example: Predicting Customer Churn with Clickstream Data**

-   **Basic Features (Efficient)**

    -   Number of logins in the last month

    -   Average session duration

-   **Complex Features (Expensive)**

    -   NLP-based topic modeling on user chat logs

    -   Image recognition of uploaded user photos

While complex features **may** add predictive power, they can **increase training time, require large datasets, and be difficult to scale**.

#### **When to Keep Expensive Features?**

‚úÖ If they **significantly boost accuracy**.\
‚úÖ If model **interpretability isn‚Äôt a major concern**.\
‚úÖ If computing resources **aren‚Äôt a bottleneck**.

### **Putting It All Together: A Feature Engineering Checklist**

| Criteria | Questions to Ask |
|----|----|
| Relevance | Does the feature have a logical connection to the target variable? |
| Predictiveness | Does including this feature improve model accuracy? |
| Independence | Does this feature add **new** information rather than duplicate existing features? |
| Interpretability | Can you explain why this feature matters? |
| Efficiency | Is it computationally feasible to calculate this feature? |

## **Feature Creation vs. Feature Selection vs. Feature Extraction**

Now that we understand what makes a **good feature**, it‚Äôs time to explore the **three core processes** of feature engineering:

1Ô∏è‚É£ **Feature Creation** ‚Äì Designing new features from existing data.\
2Ô∏è‚É£ **Feature Selection** ‚Äì Choosing only the most valuable features for a model.\
3Ô∏è‚É£ **Feature Extraction** ‚Äì Transforming raw features into a **more useful representation**.

Each of these steps plays a crucial role in **improving model performance** while balancing accuracy, interpretability, and efficiency.

### **1Ô∏è‚É£ Feature Creation: Generating New Insights from Data**

Feature creation is about **deriving new, meaningful variables** from raw data. This step is **often the key to unlocking hidden patterns** that improve model performance.

#### **üìå Example: Predicting House Prices**

Consider a dataset with the following raw variables:

-   **Total square footage**

-   **Number of rooms**

-   **Year built**

While these raw variables provide **some insights**, we can **create new features** that add even more value:

| **New Feature** | **Formula / Definition** | **Why It‚Äôs Useful?** |
|----|----|----|
| **Price per square foot** | `price / sqft` | Normalizes price differences between large and small houses. |
| **Room-to-bathroom ratio** | `num_rooms / num_bathrooms` | Helps capture the usability of space. |
| **House Age** | `Current Year - Year Built` | Older homes may have lower values. |
| **Renovation Status** | `1 if last_renovation > 10 years ago, else 0` | Highlights recently updated homes. |

**Lesson:** By **engineering new features**, we often **improve model performance more than just adding more raw data.**

#### **Common Feature Creation Techniques**

‚úÖ **Mathematical Transformations** ‚Äì Log transformations, exponentiation, ratios.\
‚úÖ **Aggregations** ‚Äì Mean, sum, count over groups (e.g., total purchases per customer).\
‚úÖ **Interaction Features** ‚Äì Multiplying or dividing two variables (e.g., room-to-bathroom ratio).\
‚úÖ **Domain-Specific Features** ‚Äì Industry-based insights (e.g., calculating BMI from height & weight).

### **2Ô∏è‚É£ Feature Selection: Keeping Only the Best Ingredients**

Feature selection is the process of **choosing the most relevant features** while removing redundant or irrelevant ones.

#### **Why Feature Selection Matters**

-   Too many features **increase model complexity** and **risk overfitting**.

-   Some features may be **correlated** and **not provide new information**.

-   Unimportant features can **slow down model training** without improving accuracy.

#### **üìå Example: Predicting Employee Churn**

We have the following features in our dataset:

-   **Employee Age**

-   **Years at Company**

-   **Department**

-   **Salary**

-   **Coffee Consumption (cups per day)**

#### **Applying Feature Selection**

-   **Step 1: Check Correlation**

    -   If **Years at Company** and **Employee Age** are highly correlated, we may drop one.

-   **Step 2: Test Feature Importance**

    -   If **Coffee Consumption** has no relationship with churn, it‚Äôs removed.

-   **Step 3: Model-Based Selection**

    -   Use techniques like **SHAP values, Lasso Regression, or Decision Trees** to rank feature importance.

#### **Methods for Feature Selection**

‚úÖ **Filter Methods:**

-   **Correlation Analysis:** Drop highly correlated features.

-   **Chi-Square Test:** Checks relationships between categorical variables.\
    ‚úÖ **Wrapper Methods:**

-   **Recursive Feature Elimination (RFE):** Iteratively removes the weakest features.

-   **Forward/Backward Selection:** Adds or removes features based on model performance.\
    ‚úÖ **Embedded Methods:**

-   **Lasso (L1) Regression:** Shrinks coefficients of weak predictors to zero.

-   **Random Forest Feature Importance:** Uses decision trees to rank feature usefulness.

### **3Ô∏è‚É£ Feature Extraction: Transforming Raw Data into New Representations**

Feature extraction **reduces dimensionality** while retaining meaningful patterns. Instead of **creating** new features, we **compress or transform** existing ones.

#### **üìå Example: Text Data in Sentiment Analysis**

A raw dataset contains customer reviews like:

-   **‚ÄúThe product was excellent and I will buy again!‚Äù**

-   **‚ÄúTerrible experience. I regret this purchase.‚Äù**

Instead of passing raw text into a model, we extract features such as:

| **Extracted Feature** | **Example** | **Why It‚Äôs Useful?** |
|----|----|----|
| **TF-IDF Score** | `0.85` (for ‚Äúexcellent‚Äù) | Identifies important words. |
| **Sentiment Score** | `Positive (0.9)` | Captures the review's mood. |
| **Word Count** | `7` | Short vs. long reviews may differ. |

#### **üìå Example: Principal Component Analysis (PCA) for Reducing Dimensions**

Imagine a dataset with **100 highly correlated numerical features**.

-   Instead of keeping **all 100**, PCA reduces it to **10 principal components** while preserving most of the information.

-   This improves **model performance and speed** without losing predictive power.

#### **Common Feature Extraction Techniques**

‚úÖ **Text Features:** Bag-of-Words, TF-IDF, Word Embeddings (Word2Vec).\
‚úÖ **Image Features:** Convolutional Neural Networks (CNNs) extract patterns.\
‚úÖ **Dimensionality Reduction:** PCA, t-SNE, UMAP.\
‚úÖ **Frequency-Based Features:** Fourier Transforms for time series data.

### **Comparing the Three Approaches**

| **Method** | **Goal** | **Example** |
|------------------------|------------------------|------------------------|
| **Feature Creation** | Generate new insights | Creating "price per sqft" from "price" and "sqft". |
| **Feature Selection** | Keep the most relevant features | Removing redundant columns like "age" and "years at company". |
| **Feature Extraction** | Compress data into a lower-dimensional space | Using PCA to reduce 100 features into 10. |

### **Choosing the Right Approach**

-   **Feature Creation** is **often the most valuable** but requires **domain knowledge**.

-   **Feature Selection** prevents overfitting and reduces **unnecessary complexity**.

-   **Feature Extraction** is useful for **high-dimensional data** (e.g., images, text).

## **Tools for Feature Engineering in R and Python**

Now that we understand **feature creation, selection, and extraction**, let‚Äôs explore the **tools** that make feature engineering more efficient in **R and Python**.

### **Why Use Specialized Feature Engineering Tools?**

‚úîÔ∏è **Efficiency:** Automates repetitive transformations.\
‚úîÔ∏è **Reproducibility:** Standardized workflows make models easier to maintain.\
‚úîÔ∏è **Scalability:** Works well for both small and large datasets.\
‚úîÔ∏è **Error Reduction:** Prevents common mistakes like data leakage.

### **1Ô∏è‚É£ Feature Engineering in R: The `recipes` Package (tidymodels)**

The **`recipes` package** (part of **tidymodels**) provides a structured way to **preprocess and transform** data for modeling.

#### **Key Features of `recipes`:**

‚úîÔ∏è **Handles missing values, scaling, encoding, and feature creation**\
‚úîÔ∏è **Integrates directly into machine learning pipelines**\
‚úîÔ∏è **Uses a stepwise, declarative approach**

#### **Example: Creating a Feature Engineering Recipe**

We will use the **built-in `mtcars` dataset** to create new features and preprocess the data.

``` R
library(tidymodels)

mtcars = mtcars %>% mutate(cyl = as.factor(cyl), gear = as.factor(gear))

# Create a recipe for preprocessing
car_recipe <- recipe(mpg ~ ., data = mtcars) %>%
  step_log(hp, base = 10) %>%   # Log transform horsepower
  step_normalize(disp, wt) %>%  # Scale displacement and weight
  step_dummy(cyl, gear) %>%     # One-hot encode categorical variables
  step_interact(terms = ~ hp:wt) # Create an interaction term

# Print recipe
car_recipe

# # ‚îÄ‚îÄ Recipe ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 
# ‚îÄ‚îÄ Inputs 
# Number of variables by role
# outcome:    1
# predictor: 10
# 
# ‚îÄ‚îÄ Operations 
# ‚Ä¢ Log transformation on: hp
# ‚Ä¢ Centering and scaling for: disp and wt
# ‚Ä¢ Dummy variables from: cyl and gear
# ‚Ä¢ Interactions with: hp:wt

# Prepare the recipe and apply it to the data
prepped_recipe <- prep(car_recipe, training = mtcars)
transformed_data <- bake(prepped_recipe, new_data = mtcars)

head(transformed_data)

# # A tibble: 6 √ó 14
#      disp    hp  drat       wt  qsec    vs    am  carb   mpg cyl_X6 cyl_X8 gear_X4 gear_X5  hp_x_wt
#     <dbl> <dbl> <dbl>    <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>  <dbl>  <dbl>   <dbl>   <dbl>    <dbl>
# 1 -0.571   2.04  3.9  -0.610    16.5     0     1     4  21        1      0       1       0 -1.25   
# 2 -0.571   2.04  3.9  -0.350    17.0     0     1     4  21        1      0       1       0 -0.714  
# 3 -0.990   1.97  3.85 -0.917    18.6     1     1     1  22.8      0      0       1       0 -1.81   
# 4  0.220   2.04  3.08 -0.00230  19.4     1     0     1  21.4      1      0       0       0 -0.00469
# 5  1.04    2.24  3.15  0.228    17.0     0     0     2  18.7      0      1       0       0  0.511  
# 6 -0.0462  2.02  2.76  0.248    20.2     1     0     1  18.1      1      0       0       0  0.501  
```

This **automatically applies transformations** to new data without manual intervention.

### **2Ô∏è‚É£ Feature Engineering in Python: `pandas` and `scikit-learn` Pipelines**

Python provides **`pandas` for data manipulation** and **`scikit-learn` for preprocessing** within machine learning workflows.

#### **Key Libraries for Feature Engineering:**

‚úîÔ∏è `pandas` ‚Äì Basic transformations (scaling, encoding, missing value handling).\
‚úîÔ∏è `scikit-learn.preprocessing` ‚Äì Standardized transformations.\
‚úîÔ∏è `featuretools` ‚Äì Automated feature engineering.

#### **Example: Feature Engineering with `pandas` and `scikit-learn`**

Using the **classic `iris` dataset**, we apply **scaling, encoding, and feature interactions**.

``` python
import pandas as pd
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer

# Load dataset
df = pd.read_csv("https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv")

# Define numeric and categorical features
num_features = ['sepal_length', 'sepal_width']
cat_features = ['species']

# Define transformations
num_transformer = StandardScaler()
cat_transformer = OneHotEncoder()

# Combine transformations in a pipeline
preprocessor = ColumnTransformer([
    ('num', num_transformer, num_features),
    ('cat', cat_transformer, cat_features)
])

# Apply transformations
transformed_data = preprocessor.fit_transform(df)

print(transformed_data[:5])  # Show transformed data

# [[-0.90068117  1.01900435  1.          0.          0.        ]
#  [-1.14301691 -0.13197948  1.          0.          0.        ]
#  [-1.38535265  0.32841405  1.          0.          0.        ]
#  [-1.50652052  0.09821729  1.          0.          0.        ]
#  [-1.02184904  1.24920112  1.          0.          0.        ]]
```

**Explanation:**\
‚úÖ `StandardScaler()` ‚Äì Standardizes numerical features.\
‚úÖ `OneHotEncoder()` ‚Äì Converts categorical features into dummy variables.\
‚úÖ `ColumnTransformer()` ‚Äì Combines multiple transformations into a single step.

### **3Ô∏è‚É£ Feature Engineering with Automated Tools**

For **automating feature creation**, we can use **`featuretools` (Python) and `vtreat` (R)**.

#### **üìå Auto Feature Engineering in R: `vtreat`**

The **`vtreat` package** automates common transformations and handles messy data.

``` R
library(vtreat)

# Prepare treatment plan
plan <- designTreatmentsN(mtcars, varlist = c("hp", "wt"), outcomename = "mpg")

# [1] "vtreat 1.6.5 inspecting inputs Tue Feb 11 22:03:49 2025"
# [1] "designing treatments Tue Feb 11 22:03:49 2025"
# [1] " have initial level statistics Tue Feb 11 22:03:49 2025"
# [1] " scoring treatments Tue Feb 11 22:03:49 2025"
# [1] "have treatment plan Tue Feb 11 22:03:49 2025"


# Apply transformations
treated_data <- prepare(plan, mtcars)

head(treated_data)

#    hp    wt  mpg
# 1 110 2.620 21.0
# 2 110 2.875 21.0
# 3  93 2.320 22.8
# 4 110 3.215 21.4
# 5 175 3.440 18.7
# 6 105 3.460 18.1
```

‚úÖ **Automatically encodes variables**\
‚úÖ **Handles missing values and interactions**\
‚úÖ **Useful for quick feature engineering**\

#### **üìå Auto Feature Engineering in Python: `featuretools`**

The **`featuretools` package** automatically generates new features from relational datasets.

``` python
import featuretools as ft

# Load dataset
df = ft.demo.load_mock_customer()["customers"]

# Define an entity set
es = ft.EntitySet(id="customers")
es = es.add_dataframe(dataframe_name="customers", dataframe=df, index="customer_id")

# Automatically create features
features, feature_defs = ft.dfs(entityset=es, target_dataframe_name="customers")

print(features.head())

#              zip_code DAY(birthday) DAY(join_date) MONTH(birthday) MONTH(join_date) WEEKDAY(birthday) WEEKDAY(join_date) YEAR(birthday) YEAR(join_date)
# customer_id
# 1               60091            18             17               7                4                 0                  6           1994            2011
# 2               13244            18             15               8                4                 0                  6           1986            2012
# 3               13244            21             13              11                8                 4                  5           2003            2011
# 4               60091            15              8               8                4                 1                  4           2006            2011
# 5               60091            28             17               7                7                 5                  5           1984            2010
```

‚úÖ **Generates new time-based and aggregated features**\
‚úÖ **Reduces manual effort in feature creation**\
‚úÖ **Works well for relational datasets (e.g., customer transactions)**

### **4Ô∏è‚É£ Feature Selection Tools in R and Python**

Once we create features, we **need to select the most important ones**.

#### **üìå Feature Selection in R (`vip`, `glmnet`)**

``` r
library(vip)
library(glmnet)

# Train a Lasso model
x <- model.matrix(mpg ~ ., mtcars)[, -1]
y <- mtcars$mpg
lasso_model <- cv.glmnet(x, y, alpha = 1)

# Plot feature importance
vip(lasso_model)
```

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSXC

#### üìå Feature Selection in Python (`shap`, `SelectKBest`)

``` python
import shap
from sklearn.ensemble import RandomForestRegressor
import pandas as pd

# Load dataset
df = pd.read_csv("https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv")

# Define numeric and categorical features
num_features = ['sepal_length', 'sepal_width']
cat_features = ['species']

# Train a random forest model
model = RandomForestRegressor()
model.fit(df[num_features], df["petal_length"])

# Calculate SHAP values
explainer = shap.Explainer(model)
shap_values = explainer(df[num_features])

# Visualize feature importance
shap.summary_plot(shap_values, df[num_features])
```

![](ds/posts/images/SHAP.jpeg)

### **Comparison: R vs. Python for Feature Engineering**

| **Task** | **R (`recipes`, `vtreat`)** | **Python (`pandas`, `featuretools`)** |
|----|----|----|
| **Preprocessing** | `step_*()` in `recipes` | `ColumnTransformer()` |
| **Encoding** | `step_dummy()` | `OneHotEncoder()` |
| **Feature Creation** | `mutate()`, `step_interact()` | `featuretools.dfs()` |
| **Feature Selection** | `vip::vi()`, `glmnet` | `SelectKBest`, `shap` |

### **üìå Summary: Choosing the Right Tool**

üîπ Use **`recipes` (R) and `pandas` (Python)** for manual feature engineering.\
üîπ Use **`vtreat` (R) and `featuretools` (Python)** for **automated feature creation**.\
üîπ Use **`vip` (R) and `shap` (Python)** for **feature selection**.

## **Best Practices & Common Pitfalls in Feature Engineering**

Now that we have explored **feature engineering techniques**, let‚Äôs focus on **best practices** and the **mistakes to avoid**. Feature engineering **can make or break a model**, and following good principles helps ensure **better performance and generalizability**.

### **1Ô∏è‚É£ Best Practices for Feature Engineering**

#### **‚úÖ 1. Use Domain Knowledge to Guide Feature Creation**

üîπ The best features **often come from understanding the problem**, not just applying transformations.\
üîπ **Ask:** What real-world factors influence the target variable?

**üìå Example:**\
**Predicting customer churn**

-   Instead of just using `number_of_logins`, create **"days since last login"** to better capture customer behavior.

#### **‚úÖ 2. Keep It Simple: Avoid Overcomplicating Features**

üîπ More features **do not always mean better models**‚Äîavoid unnecessary transformations.\
üîπ **Focus on interpretability:** If a feature is too complex to explain, it may introduce **unwanted noise**.

**üìå Example:**\
Instead of adding **5-degree polynomial features**, a simple log transformation may be enough.

#### **‚úÖ 3. Check for Redundant or Highly Correlated Features**

üîπ Features that **contain the same information** may **confuse models** and **increase computation time**.\
üîπ **How to detect?**\
‚úî **Correlation matrix** (Pearson/Spearman).\
‚úî **Variance Inflation Factor (VIF)** (R).\
‚úî **SHAP values or feature importance plots** (Python).

**üìå Example:**

-   `Total number of rooms` and `house size in square feet` may be **highly correlated**‚Äîdropping one avoids redundancy.

#### **‚úÖ 4. Normalize or Standardize When Needed**

üîπ Some models (e.g., KNN, linear regression, neural networks) **perform poorly with unscaled data**.\
üîπ Scaling ensures **each feature contributes equally** to model training.

**üìå Example:**

-   `House price` ranges from **\$50,000 to \$2,000,000**.

-   `Bedrooms count` ranges from **1 to 5**.\
    **Solution:** Standardizing prices helps prevent models from giving too much weight to large numerical values.

#### **‚úÖ 5. Use Cross-Validation When Testing Features**

üîπ A feature may improve accuracy **on the training set** but **not generalize** to new data.\
üîπ **Use k-fold cross-validation** to check how new features impact performance.

**üìå Example:**\
A feature like **‚Äúcustomer average spending in the last month‚Äù** may work well during training but fail on customers who **just signed up**.

### **2Ô∏è‚É£ Common Pitfalls to Avoid**

#### **‚ùå 1. Feature Leakage (Using Future Information)**

üîπ **Using data that wouldn‚Äôt be available at prediction time** leads to unrealistically high accuracy.\
üîπ Always ensure **feature values are only from past events**.

**üìå Example of Feature Leakage:**\
**Predicting loan default**

-   A feature like **"loan repayment status"** is not valid, because it‚Äôs already telling the model if the loan was paid or not.

#### **‚ùå 2. Overfitting to Training Data**

üîπ Over-engineering features may **memorize patterns** in the training data instead of finding **general trends**.\
üîπ Avoid **too many polynomial features or irrelevant categorical encodings**.

**üìå Example:**\
A model that includes **zip code** as a feature may perform well **only on seen locations** but fail when predicting in new areas.

#### **‚ùå 3. Encoding High-Cardinality Categorical Variables Poorly**

üîπ **One-hot encoding thousands of categories** results in **huge feature matrices**.\
üîπ Instead, use:\
‚úî **Target encoding** (replacing category with its mean target value).\
‚úî **Embedding layers** (deep learning).

**üìå Example:**\
A dataset with **thousands of unique products**‚Äîusing one-hot encoding **explodes feature space**.

#### **‚ùå 4. Ignoring Outliers Before Engineering Features**

üîπ Some transformations (e.g., **log scaling**) **fail** when outliers exist.\
üîπ **Solution:** Winsorize or use **robust scaling** to handle extreme values.

**üìå Example:**

-   Income dataset where 95% of people earn **\$50,000-\$100,000**, but one person earns **\$10M**.

-   Without handling, the model **may become biased** towards extreme values.

### **üìå Summary: Best Practices & Pitfalls**

| ‚úÖ Best Practices | ‚ùå Pitfalls to Avoid |
|----|----|
| Use **domain knowledge** for better feature creation | **Feature leakage**‚Äîusing future data |
| **Keep it simple**‚Äîavoid over-engineering | **Overfitting**‚Äîtoo many complex transformations |
| **Check correlations**‚Äîremove redundant features | Poor encoding of **high-cardinality categorical variables** |
| **Scale features** when needed | **Ignoring outliers** before transformations |
| **Validate with cross-validation** | Using **irrelevant or noisy features** |

‚úÖ **By following these principles, we ensure our models are trained on reliable, useful features**‚Äîleading to better performance in real-world scenarios.

# **Best Practices & Common Pitfalls in Feature Engineering**

Now that we have explored **feature engineering techniques**, let‚Äôs focus on **best practices** and the **mistakes to avoid**. Feature engineering **can make or break a model**, and following good principles helps ensure **better performance and generalizability**.

## **1Ô∏è‚É£ Best Practices for Feature Engineering**

### **‚úÖ 1. Use Domain Knowledge to Guide Feature Creation**

üîπ The best features **often come from understanding the problem**, not just applying transformations.\
üîπ **Ask:** What real-world factors influence the target variable?

**üìå Example:**\
**Predicting customer churn**

-   Instead of just using `number_of_logins`, create **"days since last login"** to better capture customer behavior.

### **‚úÖ 2. Keep It Simple: Avoid Overcomplicating Features**

üîπ More features **do not always mean better models**‚Äîavoid unnecessary transformations.\
üîπ **Focus on interpretability:** If a feature is too complex to explain, it may introduce **unwanted noise**.

**üìå Example:**\
Instead of adding **5-degree polynomial features**, a simple log transformation may be enough.

### **‚úÖ 3. Check for Redundant or Highly Correlated Features**

üîπ Features that **contain the same information** may **confuse models** and **increase computation time**.\
üîπ **How to detect?**\
‚úî **Correlation matrix** (Pearson/Spearman).\
‚úî **Variance Inflation Factor (VIF)** (R).\
‚úî **SHAP values or feature importance plots** (Python).

**üìå Example:**

-   `Total number of rooms` and `house size in square feet` may be **highly correlated**‚Äîdropping one avoids redundancy.

### **‚úÖ 4. Normalize or Standardize When Needed**

üîπ Some models (e.g., KNN, linear regression, neural networks) **perform poorly with unscaled data**.\
üîπ Scaling ensures **each feature contributes equally** to model training.

**üìå Example:**

-   `House price` ranges from **\$50,000 to \$2,000,000**.

-   `Bedrooms count` ranges from **1 to 5**.\
    **Solution:** Standardizing prices helps prevent models from giving too much weight to large numerical values.

### **‚úÖ 5. Use Cross-Validation When Testing Features**

üîπ A feature may improve accuracy **on the training set** but **not generalize** to new data.\
üîπ **Use k-fold cross-validation** to check how new features impact performance.

**üìå Example:**\
A feature like **‚Äúcustomer average spending in the last month‚Äù** may work well during training but fail on customers who **just signed up**.

## **2Ô∏è‚É£ Common Pitfalls to Avoid**

### **‚ùå 1. Feature Leakage (Using Future Information)**

üîπ **Using data that wouldn‚Äôt be available at prediction time** leads to unrealistically high accuracy.\
üîπ Always ensure **feature values are only from past events**.

**üìå Example of Feature Leakage:**\
**Predicting loan default**

-   A feature like **"loan repayment status"** is not valid, because it‚Äôs already telling the model if the loan was paid or not.

### **‚ùå 2. Overfitting to Training Data**

üîπ Over-engineering features may **memorize patterns** in the training data instead of finding **general trends**.\
üîπ Avoid **too many polynomial features or irrelevant categorical encodings**.

**üìå Example:**\
A model that includes **zip code** as a feature may perform well **only on seen locations** but fail when predicting in new areas.

### **‚ùå 3. Encoding High-Cardinality Categorical Variables Poorly**

üîπ **One-hot encoding thousands of categories** results in **huge feature matrices**.\
üîπ Instead, use:\
‚úî **Target encoding** (replacing category with its mean target value).\
‚úî **Embedding layers** (deep learning).

**üìå Example:**\
A dataset with **thousands of unique products**‚Äîusing one-hot encoding **explodes feature space**.

### **‚ùå 4. Ignoring Outliers Before Engineering Features**

üîπ Some transformations (e.g., **log scaling**) **fail** when outliers exist.\
üîπ **Solution:** Winsorize or use **robust scaling** to handle extreme values.

**üìå Example:**

-   Income dataset where 95% of people earn **\$50,000-\$100,000**, but one person earns **\$10M**.

-   Without handling, the model **may become biased** towards extreme values.

## **üìå Summary: Best Practices & Pitfalls**

| ‚úÖ Best Practices | ‚ùå Pitfalls to Avoid |
|----|----|
| Use **domain knowledge** for better feature creation | **Feature leakage**‚Äîusing future data |
| **Keep it simple**‚Äîavoid over-engineering | **Overfitting**‚Äîtoo many complex transformations |
| **Check correlations**‚Äîremove redundant features | Poor encoding of **high-cardinality categorical variables** |
| **Scale features** when needed | **Ignoring outliers** before transformations |
| **Validate with cross-validation** | Using **irrelevant or noisy features** |

‚úÖ **By following these principles, we ensure our models are trained on reliable, useful features**‚Äîleading to better performance in real-world scenarios.

# **Conclusion & Next Steps**

Feature engineering is **both an art and a science**‚Äîit bridges the gap between raw data and meaningful insights, ultimately determining whether a model performs **well or fails to generalize**. Throughout this article, we‚Äôve explored the **foundations of feature engineering**, covering the essentials of **feature creation, selection, and extraction**, along with best practices to avoid common pitfalls.

### **üîπ Key Takeaways**

‚úÖ **Good features matter more than complex models**‚ÄîA well-engineered dataset can outperform advanced algorithms trained on poorly processed data.\
‚úÖ **Preprocessing is a critical first step**‚ÄîHandling missing values, scaling numerical features, and encoding categorical variables set the stage for effective transformations.\
‚úÖ **Feature engineering techniques vary**‚ÄîFrom polynomial transformations and interaction terms to log scaling and binning, different problems require different approaches.\
‚úÖ **Domain knowledge is key**‚ÄîUnderstanding the context behind the data leads to more **meaningful, interpretable** features.\
‚úÖ **Avoid feature leakage and overfitting**‚ÄîEnsuring that features reflect only past information and generalize well to unseen data is crucial for real-world applications.

### **üöÄ What‚Äôs Next?**

This article is just the **beginning** of a larger **Feature Engineering Series**! In the upcoming articles, we‚Äôll take **deep dives into specific techniques**, such as:\
üìå **Transforming Categorical Variables**‚ÄîBeyond one-hot encoding, exploring techniques like target encoding and embeddings.\
üìå **Feature Engineering for Time Series Data**‚ÄîCreating lag features, moving averages, and handling seasonality.\
üìå **Text-Based Feature Engineering**‚ÄîExtracting insights using TF-IDF, word embeddings, and sentiment scores.\
üìå **Automated Feature Engineering**‚ÄîUsing tools like `featuretools` (Python) and `vtreat` (R) to generate features at scale.

By applying these methods, you‚Äôll gain the **practical knowledge** needed to craft **better features, build stronger models, and make data-driven decisions more effectively**.

üîπ **Next up: "Mastering the Spice Rack ‚Äì Transforming Categorical Features"**‚Äîwhere we explore the **best techniques for handling categorical data** to maximize model performance!

üí° **Let‚Äôs discuss!** What‚Äôs your go-to feature engineering trick? Drop your thoughts below! ‚¨á‚¨á‚¨á
