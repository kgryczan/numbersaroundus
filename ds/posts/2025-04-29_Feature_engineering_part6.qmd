---
title: "The Final Cut â€“ Choosing the Right Features for the Job"
author: "Numbers around us"
date: "2025-04-29"
format: html
---

![](images/FE_7.jpg)

Youâ€™ve created dozensâ€”maybe hundredsâ€”of new features: cleaned, transformed, encoded, engineered.\
But now comes a critical decision:

ğŸ”ª **Which features stay, and which must go?**

Choosing the right features is about **maximizing model performance while minimizing complexity**. Good feature selection can:\
âœ… Improve model generalization\
âœ… Reduce overfitting\
âœ… Speed up training and inference\
âœ… Enhance model interpretability

This final article in the Feature Engineering Series will walk through:\
âœ… **Why feature selection matters**\
âœ… **Simple filtering techniques (correlation, variance)**\
âœ… **Model-driven feature importance (tree models, permutation)**\
âœ… **Selection methods (filter, wrapper, embedded)**\
âœ… **Best practices for pruning without hurting performance**

## **Why Choosing Features Matters**

More features â‰  better models.\
In fact, **too many irrelevant or redundant features** can actively hurt your model by:\
âŒ Increasing noise and variance\
âŒ Leading to overfitting\
âŒ Slowing down training and prediction\
âŒ Making models harder to explain

ğŸ“Œ **Feature selection is about keeping what mattersâ€”and cutting what doesn't.**\
Itâ€™s a mix of **art and science**, where we balance **domain knowledge**, **data-driven decisions**, and **model feedback**.

### **ğŸ“Œ Simple Thought Exercise**

Imagine you're predicting house prices.\
You have features like:

-   `square_footage`

-   `number_of_bathrooms`

-   `zip_code`

-   `price_of_dog_food_in_region`

ğŸ›‘ Even if "dog food price" technically varies across regions, itâ€™s noise. It **adds confusion**, **increases data collection cost**, and **risks spurious correlations**. You need **only the features that directly contribute to your target**.

âœ… Good feature selection leads to:

-   **Simpler, faster models**

-   **Better generalization to new data**

-   **Clearer understanding of what drives predictions**

**Feature engineering without feature selection is like cooking without tasting.**\
You need to trim the dish before serving it.

# **Filter Methods â€“ Quick Wins for Cutting Features**

**Filter methods** are your **first line of defense** in feature selection.\
They evaluate each feature **independently of any machine learning model**, based purely on statistics.\
âœ… Fast\
âœ… Intuitive\
âœ… Great for initial cleanup before deeper analysis

In this chapter, weâ€™ll cover:\
âœ… Removing features with low variance\
âœ… Removing highly correlated features\
âœ… Univariate statistical tests (optional advanced filtering)

## **1ï¸âƒ£ Removing Low-Variance Features**

If a feature barely changes across your dataset, itâ€™s unlikely to help your model.\
Examples:

-   A product code that's always the same

-   A "country" field that's 99% "USA"

ğŸ”¹ **Low-variance features are boring. Models can safely ignore themâ€”or better yet, you should remove them.**

### ğŸ“Œ R: Using `recipes::step_nzv()`

``` r

recipe <- recipe(target ~ ., data = df) %>%
  step_nzv(all_predictors())  # Near zero variance
```

### ğŸ“Œ Python: Using `VarianceThreshold`

``` python
from sklearn.feature_selection import VarianceThreshold

selector = VarianceThreshold(threshold=0.01)  # Remove features with variance < 0.01
X_reduced = selector.fit_transform(X)
```

âœ… Simple, no model required, and **often removes dozens of useless columns in wide datasets**.

## **2ï¸âƒ£ Removing Highly Correlated Features**

Having two features that are **nearly identical** doesn't add informationâ€”it **adds redundancy**.\
High correlation (e.g., \> 0.9) between two features suggests **keeping only one**.

ğŸ“Œ **Why?**

-   Reduces multicollinearity (important for linear models)

-   Speeds up training

-   Improves model stability

### ğŸ“Œ R: Correlation Matrix with `corrr` + Manual Cutoff

``` r
library(corrr)

cor_matrix <- correlate(df)
# Find feature pairs with absolute correlation > 0.9
high_corr_pairs <- stretch(cor_matrix) %>%
  filter(abs(r) > 0.9, x != y)
```

### ğŸ“Œ Python: Correlation Matrix + Manual Cutoff

``` python
import pandas as pd

corr_matrix = df.corr().abs()
upper = corr_matrix.where(pd.np.triu(pd.np.ones(corr_matrix.shape), k=1).astype(bool))

# Find columns to drop
to_drop = [column for column in upper.columns if any(upper[column] > 0.9)]
df_reduced = df.drop(columns=to_drop)
```

âœ… Helps models generalize better, especially **linear, logistic regression, and LASSO models**.

## **3ï¸âƒ£ (Optional) Statistical Tests for Feature Relevance**

Sometimes you want a **quick univariate check** for how strongly each feature relates to the target.

-   For classification â†’ Chi-squared test, ANOVA F-test

-   For regression â†’ Pearson/Spearman correlation

These are **rough guides**, not gospel.

### ğŸ“Œ Python: Select K Best Features

``` python
from sklearn.feature_selection import SelectKBest, f_classif

selector = SelectKBest(f_classif, k=10)
X_new = selector.fit_transform(X, y)
```

âœ… **Statistical filters** can help shortlist candidates **before** deeper model-driven selection.

## **ğŸ“Œ Quick Recap: When to Use Filter Methods**

| Use Case                 | Filter to Apply                       |
|--------------------------|---------------------------------------|
| Too many static features | Remove low variance                   |
| Redundant twin features  | Remove highly correlated              |
| Very wide datasets       | Univariate feature ranking (optional) |

âœ… **Filter methods are fast, interpretable, and perfect for early-stage pruning before feeding data into heavier model-based feature selectors.**

# **Wrapper Methods â€“ Model-Driven Feature Selection**

While filter methods **rank features individually**, wrapper methods **evaluate subsets of features together** based on how well they actually perform in a model.\
âœ… More accurate\
âœ… Takes feature interactions into account\
âŒ Slower and more computationally expensive

In this chapter, weâ€™ll cover:\
âœ… What wrapper methods are\
âœ… Recursive Feature Elimination (RFE)\
âœ… Stepwise Selection\
âœ… When and why to use them

## **1ï¸âƒ£ What Are Wrapper Methods?**

Instead of looking at each feature in isolation, wrapper methods:\
ğŸ”¹ Train a model on different combinations of features\
ğŸ”¹ Evaluate model performance (accuracy, RMSE, AUC, etc.)\
ğŸ”¹ Keep the combinations that **improve the metric**

ğŸ“Œ **Think of it as: "Train â†’ Evaluate â†’ Keep the best."**

## **2ï¸âƒ£ Recursive Feature Elimination (RFE)**

RFE **starts with all features**, trains a model, **removes the least important feature**, retrains, and **repeats** until only the strongest features remain.

âœ… Good for:

-   Logistic regression, linear models

-   Small- to mid-sized feature sets

-   Cases where you want **compact, high-quality feature subsets**

### ğŸ“Œ Python: RFE Example with `sklearn`

``` python
from sklearn.feature_selection import RFE
from sklearn.ensemble import RandomForestClassifier

rfe = RFE(estimator=RandomForestClassifier(), n_features_to_select=10)
X_rfe = rfe.fit_transform(X, y)
```

### ğŸ“Œ R: RFE Example with `caret`

``` r
library(caret)

control <- rfeControl(functions = rfFuncs, method = "cv", number = 5)
rfe_result <- rfe(df[, predictors], df$target, sizes = c(5, 10, 15), rfeControl = control)
```

âœ… RFE **optimizes based on model performance**â€”not just correlation or variance.

## **3ï¸âƒ£ Stepwise Feature Selection**

**Stepwise selection** builds a model **by adding or removing features one at a time**, depending on how much they improve model performance.

-   **Forward selection** starts with no features and adds the most useful one at each step.

-   **Backward elimination** starts with all features and removes the least useful one at each step.

-   **Bidirectional selection** combines both approaches.

âœ… Good for:

-   Smaller feature spaces

-   Quick-and-dirty baselines for interpretable models

### ğŸ“Œ R: Stepwise Selection with `MASS::stepAIC`

``` r
library(MASS)

model_full <- lm(target ~ ., data = df)
stepwise_model <- stepAIC(model_full, direction = "both")
```

âœ… Stepwise works well when **interpretability** is critical (e.g., regression reports).

## **4ï¸âƒ£ Pros and Cons of Wrapper Methods**

| Pros                           | Cons                                   |
|--------------------------------|----------------------------------------|
| Produces strong feature sets   | Slow (lots of model training)          |
| Considers feature interactions | Can overfit if not cross-validated     |
| Model-specific (customized)    | Not always scalable for very wide data |

âœ… Use wrapper methods when **accuracy matters more than speed**â€”especially in smaller datasets or final feature polishing.

## **ğŸ“Œ Summary: When to Use Wrapper Methods**

| Situation                                  | Method                        |
|------------------------------------------|------------------------------|
| Need very compact, strong feature set      | RFE                           |
| Quick model explainability (linear models) | Stepwise                      |
| Feature interactions matter                | RFE or bidirectional stepwise |

âœ… **Wrapper methods are your scalpelâ€”precise, careful, and best used after initial filtering.**

# **Embedded Methods â€“ Feature Selection Built Into Models**

Embedded methods are **the smartest feature selectors**:\
they **integrate feature selection into the model training process itself**.\
Instead of preprocessing or wrapper-based evaluation, the model **naturally identifies and reduces less important features while learning**.

This chapter covers:\
âœ… What embedded methods are\
âœ… Feature selection via regularization (LASSO)\
âœ… Feature importance from tree-based models\
âœ… When and how to use them efficiently

## **1ï¸âƒ£ What Are Embedded Methods?**

Unlike filters or wrappers that work outside of the model, embedded methods:\
ğŸ”¹ Train the model\
ğŸ”¹ Penalize or reward features automatically\
ğŸ”¹ Give **built-in signals** about which features matter

ğŸ“Œ **Feature selection happens during model fittingâ€”not as a separate step.**

## **2ï¸âƒ£ Regularization: LASSO for Shrinking Features**

**LASSO (Least Absolute Shrinkage and Selection Operator)** is a type of linear regression that **shrinks** less important feature coefficients **toward zero**.

-   Coefficients that become exactly **zero** are effectively **removed** from the model.

âœ… Great for:

-   High-dimensional data (many features, few samples)

-   Finding compact linear models

### ğŸ“Œ R: LASSO with `glmnet`

``` r
library(glmnet)

X <- model.matrix(target ~ ., df)[, -1]
y <- df$target

lasso_model <- cv.glmnet(X, y, alpha = 1)
coef(lasso_model, s = "lambda.min")
```

### ğŸ“Œ Python: LASSO with `sklearn`

``` python
from sklearn.linear_model import LassoCV

lasso = LassoCV(cv=5).fit(X, y)
selected_features = X.columns[(lasso.coef_ != 0)]
```

âœ… **LASSO shrinks weak features out of the model, automatically cleaning your feature set.**

## **3ï¸âƒ£ Tree-Based Models: Natural Feature Importance**

Decision trees, random forests, and gradient boosting models naturally **rank feature importance** based on **how much each feature improves splits**.

âœ… Good for:

-   Nonlinear feature interactions

-   Handling messy, mixed-type data without heavy preprocessing

-   Quick feature importance ranking

### ğŸ“Œ R: Feature Importance from `ranger`

``` r
library(ranger)

model <- ranger(target ~ ., data = df, importance = "impurity")
importance(model)
```

### ğŸ“Œ Python: Feature Importance from `RandomForestClassifier`

``` python
from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier().fit(X, y)
importances = model.feature_importances_
```

âœ… **Models like XGBoost, LightGBM, and CatBoost also offer similar importance scores.**

## **4ï¸âƒ£ Pros and Cons of Embedded Methods**

| Pros | Cons |
|------------------------------------|------------------------------------|
| Efficient: selection happens while training | Model-specific |
| Handles large feature sets well | Might discard features useful for other models |
| Captures nonlinear interactions (trees) | Can overfit without tuning (especially small data) |

âœ… Embedded methods are **fast, smart, and directly tied to model objectives**.

## **ğŸ“Œ Summary: When to Use Embedded Feature Selection**

| Situation | Embedded Method |
|------------------------------------|------------------------------------|
| High-dimensional linear models | LASSO |
| Nonlinear or tree-based models | Random Forest / XGBoost feature importance |
| Need automatic selection without extra loops | Any embedded method |

âœ… **Embedded methods are your autopilotâ€”they work while you train.**

# **Best Practices & Common Mistakes in Feature Selection**

Feature selection sounds simple: **keep the good, throw away the bad.**\
But in practice, itâ€™s very easy to **introduce biases, overfit, or accidentally destroy predictive power** if youâ€™re not careful.

This chapter covers:\
âœ… The most common mistakes made during feature selection\
âœ… Best practices to keep models honest\
âœ… A simple checklist to build your final feature set

## **1ï¸âƒ£ Common Mistakes to Avoid**

| Mistake | Why It's Dangerous |
|------------------------------------|------------------------------------|
| ğŸ” Selecting features on the full dataset | **Leaks test set info** â†’ fake performance boost |
| ğŸ“¦ Blindly trusting feature importance | Importance â‰  necessity (some features help in combos) |
| ğŸ›  Overfitting during wrapper methods | Small datasets + too many selections = fragile models |
| ğŸ§½ Over-cleaning (dropping weak but synergistic features) | Sometimes features interact in ways you can't see alone |

### **ğŸ“Œ Real-World Example of Leakage**

If you compute feature importance, correlation, or do stepwise selection **before splitting train/test**,\
youâ€™re giving your model information it wouldn't have at prediction time.

âœ… **Always split your data first** â†’ only perform selection based on the training set.

## **2ï¸âƒ£ Best Practices for Robust Feature Selection**

âœ” **Always split first** â€” Train/test split (or CV folds) before any feature importance evaluation.\
âœ” **Use multiple methods** â€” Combine filter (variance/correlation), wrapper (RFE), and embedded (tree importance) techniques.\
âœ” **Cross-validate selection** â€” Donâ€™t just trust one random split; validate across folds if possible.\
âœ” **Prioritize simplicity** â€” Favor smaller, interpretable feature sets if predictive power remains stable.\
âœ” **Use domain knowledge wisely** â€” Donâ€™t blindly delete a variable just because itâ€™s weak statistically; it might still matter logically.\
âœ” **Donâ€™t panic about slight drops** â€” Sometimes, dropping noisy features *slightly* worsens training accuracy but **improves generalization**.

## **3ï¸âƒ£ Practical Checklist: How to Choose Features Like a Pro**

| Step | Action |
|------------------------------------|------------------------------------|
| 1ï¸âƒ£ | Train/test split (or CV setup) |
| 2ï¸âƒ£ | Filter out near-zero variance and highly correlated features |
| 3ï¸âƒ£ | Run simple univariate filters (optional) |
| 4ï¸âƒ£ | Apply wrapper method (RFE / stepwise) if time allows |
| 5ï¸âƒ£ | Validate against model-specific embedded importance |
| 6ï¸âƒ£ | Build multiple models (full vs selected) and compare metrics |
| 7ï¸âƒ£ | Choose the **smallest feature set** with acceptable or better validation performance |
| 8ï¸âƒ£ | Document feature selection steps for reproducibility |

âœ… **Systematic selection \> Gut feeling**â€”but combining both often works best!

## **ğŸ“Œ Quick Summary: Mastering the Final Cut**

| Do | Avoid |
|------------------------------------|------------------------------------|
| Split data first | Feature selection on the full dataset |
| Cross-validate selections | Overfitting on a lucky split |
| Favor smaller, simpler models | Keeping everything just in case |
| Combine filter, wrapper, and embedded signals | Trusting only one method |

âœ… **Feature selection isnâ€™t just cleaningâ€”it's strategic model crafting.**
