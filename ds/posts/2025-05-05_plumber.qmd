---
title: "How plumber Turns Your R Scripts into On-Demand Data Faucets"
author: "Numbers around us"
date: "2025-05-06"
format: html
---

![](images/plumber.png)

# ğŸ’§ Introduction

Imagine if your data could flow like water â€” available when needed, filtered to your taste, and ready to serve. As R users, we often write powerful analyses that live inside scripts or notebooks, waiting to be rerun or reknit. But what if someone else â€” a dashboard, a spreadsheet, even a non-technical colleague â€” could *turn a faucet* and get just the data or insight they need, right when they need it?

Enter [`plumber`](https://www.rplumber.io/): the package that transforms your R code into web-accessible endpoints. With just a few annotations, you can create lightweight, on-demand services â€” faucets â€” that deliver filtered data, summaries, forecasts, and even dynamic plots.

In this tutorial, weâ€™ll build a trio of â€œdata faucetsâ€ using the classic `gapminder` dataset. Each one will be activated via a simple HTTP GET request, controlled through query parameters â€” just like adjusting the knobs on a sink. No dashboards, no shiny, no server frameworks. Just clean, functional API plumbing in R.

# ğŸ”§ What is plumber (and what is a data faucet)?

`plumber` is an R package that exposes R functions as web API endpoints. That means you can call your R code over HTTP â€” from a browser, a spreadsheet, or another app â€” and get results back as JSON.

In this metaphor, weâ€™ll treat your code like a **faucet**. A faucet doesnâ€™t boil water or change its chemistry â€” it just controls when, where, and how water flows. Similarly, a plumber endpoint gives users access to a very specific piece of logic youâ€™ve defined in R: filter this data, summarize that column, draw a plot. They donâ€™t need to understand the pipework underneath â€” just turn the handle.

A typical plumber endpoint looks like this:

```{r}
#| eval: false
#| include: true
#| echo: true
#| collapse: false


#* @get /hello
function(name = "World") {
  list(message = paste("Hello,", name))
}
```

Visiting `http://localhost:8000/hello?name=Alice` would return:

``` json
{
  "message": "Hello, Alice"
}
```

In the rest of this article, weâ€™ll build more sophisticated faucets â€” not to say hello, but to **summarize**, **forecast**, and **visualize** data from `gapminder`.

# ğŸŒ Our dataset â€” a classic reservoir

Before you can open the tap, you need a water supply. In our case, thatâ€™s the `gapminder` dataset â€” a tidy, time-series snapshot of global development indicators. Itâ€™s compact, well-structured, and ideal for building API examples without external dependencies.

Letâ€™s load the dataset and take a quick look:

```{r}
#| eval: false
#| include: true

library(gapminder)
library(dplyr)

data <- gapminder::gapminder
glimpse(data)

```

Youâ€™ll see six columns:

-   `country`: Name of the country

-   `continent`: Continent the country belongs to

-   `year`: Observation year (1952â€“2007 in 5-year steps)

-   `lifeExp`: Life expectancy

-   `pop`: Population

-   `gdpPercap`: GDP per capita

Think of this dataset as a **water tank** â€” structured, pressurized, and ready to be tapped. Every API call weâ€™ll build will draw from this tank, apply some filtering or transformation, and return a clean stream of results.

Hereâ€™s a simple example using `dplyr` to filter the data manually â€” weâ€™ll soon automate this behind an endpoint:

```{r}
#| eval: false
#| include: true

data %>%
  filter(country == "Poland", year >= 1972, year <= 2002) %>%
  select(year, lifeExp)
```

This snippet answers a narrow question: *How has life expectancy changed in Poland between 1972 and 2002?*\

Now letâ€™s build a faucet that does this â€” on demand.

# Faucet 1 â€” Data summary on request

Our first faucet delivers **summary statistics** for any country and development metric in the dataset. Think of it as a cold-water tap: you specify the country and the metric, and it returns a quick, refreshing snapshot â€” no modeling, no visualization, just facts.

ğŸ’¡ Example request:

```         
GET /summary?country=Poland&metric=lifeExp
```

Expected response:

```         
{
  "country": "Poland",
  "metric": "lifeExp",
  "min": 61.0,
  "max": 75.5,
  "mean": 70.34,
  "median": 71.0,
  "latest": 75.5
}
```

## ğŸ§  Behind the faucet: the R function

Letâ€™s write an R function that accepts a country and a metric, filters the dataset accordingly, and returns a named list of basic statistics:

```{r}
#| eval: false
#| include: true

summary_stats <- function(data, country, metric) {
  if (!(metric %in% names(data))) {
    stop("Metric not found in dataset")
  }

  df <- data %>% 
    filter(country == !!country) %>%
    select(year, value = all_of(metric)) %>%
    arrange(year)

  if (nrow(df) == 0) {
    stop("No data found for selected country")
  }

  list(
    country = country,
    metric = metric,
    min = min(df$value, na.rm = TRUE),
    max = max(df$value, na.rm = TRUE),
    mean = mean(df$value, na.rm = TRUE),
    median = median(df$value, na.rm = TRUE),
    latest = tail(df$value, 1)
  )
}

```

## ğŸ”Œ Exposing the faucet with plumber

Now we add this function to a plumber route. Hereâ€™s how to define a `/summary` endpoint that accepts query parameters for `country` and `metric`:

```{r}
#| eval: false
#| include: true

# plumber.R
#* @param country The name of the country (e.g. "Poland")
#* @param metric The metric to summarize (e.g. "lifeExp", "gdpPercap")
#* @get /summary
function(country = "", metric = "lifeExp") {
  tryCatch({
    summary_stats(data, country, metric)
  }, error = function(e) {
    list(error = e$message)
  })
}

```

Save this file and launch the API with:

```{r}
#| eval: false
#| include: true
pr <- plumber::plumb("plumber.R")
pr$run(port = 8000)
```

Now try visiting:

```         
http://localhost:8000/summary?country=Poland&metric=lifeExp
```

ğŸ’¡ *Tip: if you get an error, check spelling â€” plumber doesnâ€™t do autocomplete!*

Youâ€™ve now built your first live data faucet: users can request a country and a metric, and get real-time stats â€” no dashboards, no spreadsheets.

# ğŸ”¥ Faucet 2 â€” Forecasting future flow

Our next faucet doesnâ€™t just report the past â€” it **predicts the future**. Given a country, a metric (like life expectancy), and a time range, it builds a **linear trend model** and forecasts future values for a specified horizon.

Think of this as a **hot water tap**: it uses energy (a model) to project where things are going â€” not just where theyâ€™ve been.

ğŸ’¡ Example request:

```         
GET /forecast?country=Poland&metric=lifeExp&from=1972&to=2002&horizon=10
```

Expected response (truncated):

```         
[
  { "year": 1972, "value": 70.1 },
  ...
  { "year": 2002, "value": 75.5 },
  { "year": 2007, "value": 76.4 },
  { "year": 2012, "value": 77.3 }
]
```

#### ğŸ”§ Modeling the future with `lm()`

Letâ€™s write a function that:

1.  Filters the dataset by country and year range

2.  Fits a linear regression model (`lm(value ~ year)`)

3.  Forecasts `horizon` years into the future (at 5-year intervals)Â 

```{r}
#| eval: false
#| include: true
forecast_lm <- function(data, country, metric, from, to, horizon = 10) {
  df <- data %>%
    filter(country == !!country, year >= from, year <= to) %>%
    select(year, value = all_of(metric)) %>%
    arrange(year)

  if (nrow(df) < 3) stop("Not enough data points to build a model")

  model <- lm(value ~ year, data = df)

  last_year <- max(df$year)
  future_years <- tibble(year = seq(last_year + 5, last_year + horizon, by = 5))
  preds <- predict(model, newdata = future_years)

  forecast <- bind_rows(
    df,
    tibble(year = future_years$year, value = preds)
  )

  forecast
}
```

ğŸ§© Creating the `/forecast` endpoint

```{r}
#| eval: false
#| include: true

# plumber.R
#* @param country Country name
#* @param metric Metric to forecast
#* @param from Start year
#* @param to End year
#* @param horizon Years to forecast beyond `to`
#* @get /forecast
function(country = "", metric = "lifeExp", from = 1962, to = 2007, horizon = 10) {
  tryCatch({
    from <- as.numeric(from)
    to <- as.numeric(to)
    horizon <- as.numeric(horizon)

    result <- forecast_lm(data, country, metric, from, to, horizon)
    result
  }, error = function(e) {
    list(error = e$message)
  })
}

```

Start or reload your plumber API, then try:

```         
http://localhost:8000/forecast?country=Poland&metric=lifeExp&from=1972&to=2002&horizon=10
```

You now have a faucet that **models and delivers future insights** on the fly.

# ğŸ–¼ï¸ Faucet 3 â€” Line chart with a twist

So far, your API can serve stats and forecasts â€” but sometimes numbers just donâ€™t flow as well as **pictures**. This final faucet will return a **line chart** showing the historical and forecasted values as a single visual stream.

This is the *designer faucet* â€” it doesnâ€™t just give you water, it does it with flair.

ğŸ’¡ Example request:

```         
GET /forecast?country=Poland&metric=lifeExp&from=1972&to=2002&horizon=10&plot=true
```

Expected response:

```         
{
  "plot": "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAA..."
}
```

This base64-encoded string can be rendered directly in an HTML `img` tag, embedded in dashboards, or downloaded as a PNG.

### ğŸ–Œ Generating the plot

Weâ€™ll use `ggplot2` to draw the chart and `base64enc` to encode the image for API delivery.

```{r}
#| eval: false
#| include: true
library(ggplot2)
library(base64enc)

plot_forecast <- function(df, country, metric) {
  p <- ggplot(df, aes(x = year, y = value)) +
    geom_line(color = "steelblue", size = 1) +
    geom_point(size = 2) +
    theme_minimal() +
    labs(
      title = paste("Forecast for", country),
      y = metric, x = "Year"
    )

  tf <- tempfile(fileext = ".png")
  ggsave(tf, plot = p, width = 6, height = 4, dpi = 150)
  dataURI(file = tf, mime = "image/png")
}
```

### ğŸ§© Updating the `/forecast` endpoint with `plot=TRUE`

Letâ€™s update the existing endpoint to return either raw data or a plot, based on a `plot` parameter:

```{r}
#| eval: false
#| include: true
#* @serializer contentType list(type='image/png')
#* @get /forecast
function(country = "", metric = "lifeExp", from = 1962, to = 2007, horizon = 10, plot = "false") {
  tryCatch({
    from <- as.numeric(from)
    to <- as.numeric(to)
    horizon <- as.numeric(horizon)
    plot <- tolower(plot) == "true"

    result <- forecast_lm(data, country, metric, from, to, horizon)

    if (plot) {
      p <- ggplot(result, aes(x = year, y = value)) +
        geom_line(color = "steelblue", size = 1) +
        geom_point(size = 2) +
        theme_minimal() +
        labs(
          title = paste("Forecast for", country),
          y = metric, x = "Year"
        )

      tf <- tempfile(fileext = ".png")
      ggsave(tf, plot = p, width = 6, height = 4, dpi = 150)

      # Return file connection (plumber will serve it directly as image/png)
      readBin(tf, "raw", n = file.info(tf)$size)
    } else {
      result
    }
  }, error = function(e) {
    list(error = e$message)
  })
}

```

### ğŸ§ª Test the PNG response

Visit this in your browser:

```         
http://localhost:8000/forecast?country=Poland&metric=lifeExp&from=1972&to=2002&horizon=10&plot=true
```

![](images/clipboard-2719250189.png){width="524"}

# ğŸ› ï¸ Putting it all together

At this point, youâ€™ve built a small but mighty collection of **data faucets**:

-   `/summary` delivers quick statistics based on country and metric

-   `/forecast` returns a time-extended series â€” either as JSON or as a **real PNG** chart

-   Youâ€™ve used plumber to wire up these faucets into accessible, browser-friendly endpoints

Now letâ€™s look at how to organize and run this whole system.

#### ğŸ”§ Final `plumber.R` file (minimal, modular version)

```{r}
#| eval: false
#| include: true
library(plumber)
library(dplyr)
library(gapminder)
library(ggplot2)

data <- gapminder::gapminder

# Summary function
summary_stats <- function(data, country, metric) {
  if (!(metric %in% names(data))) stop("Metric not found")
  df <- data %>%
    filter(country == !!country) %>%
    select(year, value = all_of(metric)) %>%
    arrange(year)
  if (nrow(df) == 0) stop("No data found")

  list(
    country = country,
    metric = metric,
    min = min(df$value, na.rm = TRUE),
    max = max(df$value, na.rm = TRUE),
    mean = mean(df$value, na.rm = TRUE),
    median = median(df$value, na.rm = TRUE),
    latest = tail(df$value, 1)
  )
}

# Forecast function
forecast_lm <- function(data, country, metric, from, to, horizon) {
  df <- data %>%
    filter(country == !!country, year >= from, year <= to) %>%
    select(year, value = all_of(metric)) %>%
    arrange(year)
  if (nrow(df) < 3) stop("Not enough data")

  model <- lm(value ~ year, data = df)
  future_years <- tibble(year = seq(max(df$year) + 5, max(df$year) + horizon, by = 5))
  preds <- predict(model, newdata = future_years)

  bind_rows(df, tibble(year = future_years$year, value = preds))
}

#* @get /summary
function(country = "", metric = "lifeExp") {
  tryCatch({
    summary_stats(data, country, metric)
  }, error = function(e) list(error = e$message))
}

#* @serializer contentType list(type='image/png')
#* @get /forecast
function(country = "", metric = "lifeExp", from = 1962, to = 2007, horizon = 10, plot = "false") {
  tryCatch({
    from <- as.numeric(from)
    to <- as.numeric(to)
    horizon <- as.numeric(horizon)
    plot <- tolower(plot) == "true"

    result <- forecast_lm(data, country, metric, from, to, horizon)

    if (plot) {
      p <- ggplot(result, aes(x = year, y = value)) +
        geom_line(color = "steelblue", size = 1) +
        geom_point(size = 2) +
        theme_minimal() +
        labs(title = paste("Forecast for", country), y = metric, x = "Year")

      tf <- tempfile(fileext = ".png")
      ggsave(tf, plot = p, width = 6, height = 4, dpi = 150)
      readBin(tf, "raw", n = file.info(tf)$size)
    } else {
      result
    }
  }, error = function(e) list(error = e$message))
}

```

#### â–¶ï¸ Running the API

In a script or console:

```{r}
#| eval: false
#| include: true
pr <- plumber::plumb("plumber.R")
pr$run(port = 8000)
```

You now have a live REST API with:

-   `GET /summary?country=Sweden&metric=lifeExp`

-   `GET /forecast?country=Poland&from=1972&to=2002&horizon=10`

-   `GET /forecast?country=Poland&plot=true` â†’ returns image

This can be accessed from:

-   **Web browsers**

-   **R, Python, JS clients**

-   **Excel or Power BI as a Web data source**

-   **Scheduled scripts or cron jobs**

# ğŸ§© Real-world use cases

Building an API in R isnâ€™t just a neat exercise â€” it opens doors to automation, collaboration, and smarter data workflows. Hereâ€™s how you can put your new set of **data faucets** to work in the wild.

#### ğŸ’¼ 1. **Business dashboards (without Shiny)**

Have a team that needs to monitor key indicators â€” like GDP, life expectancy, or population trends â€” but doesn't need interactive Shiny apps?

-   Serve your forecasts or summaries as **JSON** to tools like **Power BI**, **Google Sheets**, or **Grafana**.

-   Point a GET request at your `/summary` or `/forecast` endpoint.

-   Bonus: use query parameters to adjust country, metric, or time range dynamically.

#### ğŸ“ˆ 2. **Report automation with Quarto**

Want a PDF or HTML report that updates automatically every week?

-   In your Quarto document, call the API using `httr::GET()` or `jsonlite::fromJSON()`.

-   Pull in fresh forecasts or visuals just before knitting.

-   Schedule the report generation via cron, GitHub Actions, or RStudio Connect.

Example code block in R Markdown:

```{r}
#| eval: false
#| include: true
response <- jsonlite::fromJSON("http://localhost:8000/summary?country=Kenya&metric=lifeExp")
```

#### ğŸ¤ 3. **Cross-language collaboration**

Have Python developers or JavaScript apps that need data from your R analysis?

-   No need to share scripts or environments â€” just provide API access.

-   Python client example:

```{python}
#| eval: false
#| include: true
import requests
response = requests.get("http://localhost:8000/summary", params={"country": "Norway"})
print(response.json())

```

-   Works with any language that can make HTTP requests.

#### ğŸ“¤ 4. **Batch forecasting or scheduled predictions**

Want to auto-refresh 5-year projections for 100 countries?

-   Write an R script that loops through countries, hits `/forecast`, and saves the results.

-   Useful for building datasets, dashboards, or feeding a downstream model.

#### ğŸŒ 5. **Public-facing endpoints**

If your data is safe to share, expose endpoints externally (e.g. via ShinyApps.io, Posit Connect, or a lightweight cloud server). Youâ€™ll have:

-   Educational tools for students and journalists

-   Interactive demos for workshops

-   Lightweight alternatives to heavy web apps

Just be sure to handle request limits, logging, and basic input validation.

Even without a frontend or database, your plumber API becomes a flexible, modular **backbone** for all kinds of data delivery â€” internal or external, on-demand or automated.

# ğŸ§¾ Conclusion: small faucets, big flow

You didnâ€™t build a dashboard.\
You didnâ€™t deploy a full app.\
And yet, with just a few lines of R and `plumber`, you created something powerful: **an accessible, reusable, on-demand interface to your data**.

By treating your analysis like plumbing â€” with controlled flow, modular design, and clean output â€” youâ€™ve turned raw R scripts into **living services**. Whether itâ€™s a quick summary, a forward-looking forecast, or a real-time chart, your data is now:

-   Self-serve

-   Language-agnostic

-   Reusable

-   Ready for production (or a very elegant hack)

And best of all: **youâ€™re still just writing R**.

### ğŸ›  Whatâ€™s next?

Here are a few ways you can evolve this system:

-   ğŸ§® **Swap in more powerful models**: Try `prophet`, `ARIMA`, or `fable` instead of `lm()`

-   ğŸŒ **Add geo endpoints**: Return top countries by growth or metrics per continent

-   ğŸ“¦ **Connect to real data**: Read from a live database or API instead of `gapminder`

-   ğŸ§ª **Add tests**: Use `testthat` to validate endpoints and logic

-   ğŸš€ **Deploy it**: Host it via Docker, Posit Connect, or a simple VPS with `systemd`

The key takeaway?\
**APIs donâ€™t need to be heavy, complex, or coupled to ML.**\
Sometimes, a clean faucet with exactly the data you want is more than enough.
