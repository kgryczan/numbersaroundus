---
title: "The Hidden Missing: What Your Data Isnâ€™t Telling You"
author: "Numbers around us"
date: "2025-06-09"
format: html
---

![](images/ds_missing_values.png)

You run a summary on your dataset. No `NA`s, no `NULL`s, no errors in sight. Everything looks pristine â€” a data scientistâ€™s dream. And yet, your analysis leads to odd conclusions: strange averages, inconsistent totals, suspicious gaps in time. Somethingâ€™s off.

That â€œsomethingâ€ is *missing data* â€” just not the kind you were taught to expect.

In most data workflows, weâ€™re trained to hunt for the obvious culprits: the `NA`s, the blanks, the `NaN`s. But not all missing data is so polite as to announce itself. Sometimes it hides behind a zero, an empty string, a vague â€œunknown,â€ or worse â€” it disappears entirely, leaving no trace in your rows.

This article explores these **hidden forms of missingness**. Weâ€™ll break down the different types, show you how to spot them, and give you practical tools to clean them up. Because in data analysis, the gaps you canâ€™t see are often the ones that do the most damage.

## Redefining â€œMissing Dataâ€

When most people think of missing data, they picture something literal: a blank cell in Excel, an `NA` in R, a `NULL` in SQL. These are **explicit missing values** â€” easy to find, easy to filter, and usually easy to fix.

But real-world data is messier than that. Often, whatâ€™s *functionally* missing doesnâ€™t look empty at all.

Imagine a column filled with `"unknown"` or `"â€”"` or just `"."`. Technically, those are values. But they tell you nothing. Theyâ€™re **implicit missing values** â€” not labeled as missing, but carrying no usable information. Worse yet, they can fly under the radar during cleaning and quietly sabotage your summaries, visualizations, or model performance.

So hereâ€™s the key insight:

> **Missing data isnâ€™t just about absence â€” itâ€™s about absence of meaning.**

To analyze data well, we need to stop thinking only in terms of `NA`s and start considering missingness as a broader concept. That means re-evaluating:

-   What counts as a â€œrealâ€ value?

-   Which values carry meaningful information?

-   And what gaps in structure might be hiding the truth?

Up next, weâ€™ll walk through common types of hidden missing data â€” the kind that doesnâ€™t show up as `NA`, but still distorts your analysis.

## The Many Faces of Hidden Missing Data

Not all missing data shows up in your `is.na()` check. In fact, many of the most insidious gaps are hidden in plain sight. Here are the most common ways missing data sneaks into clean-looking datasets:

### 1. **Placeholders in Disguise**

Sometimes missing values are â€œfilled inâ€ with strings that sound like answers but mean nothing.

**Common culprits**:

-   `"unknown"`, `"n/a"`, `"missing"`, `"not applicable"`

-   Symbols like `"-"`, `"."`, `"?"`

These placeholders often appear during manual data entry, survey exports, or Excel-based inputs. Theyâ€™re especially dangerous when automatically converted to strings or factors â€” suddenly, your `"unknown"`s are treated like meaningful categories.

### 2. **Zeros That Lie**

A `0` can mean many things:

-   Zero sales (an actual value),

-   No entry (a missing event),

-   Or default filler when data wasn't recorded.

For example, a row with `0` revenue could mean "no sale occurred" â€” or that no one entered the data yet. In time series or transactional data, this nuance is crucial.

**Tip**: Always ask, â€œIs 0 a true measurement, or a stand-in for nothing?â€

### 3. **Blank Strings and Invisible Blanks**

An empty string (`""`) isnâ€™t technically `NA`, but itâ€™s often just as meaningless.

This happens when:

-   Text fields are left blank in Excel.

-   White space or tabs are entered by mistake.

-   `read_csv()` or other import functions donâ€™t automatically treat blanks as `NA`.

These silent gaps pass `is.na()` but break summaries, joins, and visualizations.

### 4. **Implicitly Missing Rows or Combinations**

Sometimes missingness doesnâ€™t exist *in* the data â€” itâ€™s missing *from* the data.

Examples:

-   A product has no sales in February, so no row is recorded at all.

-   A region is completely missing from a cross-tab.

These structural gaps are common in transactional datasets and make it seem like something â€œdidnâ€™t happenâ€ when in fact it just wasnâ€™t logged.

**Tool to use**: `tidyr::complete()` helps surface these hidden absences by expanding expected combinations.

### 5. **Inconsistent Categories**

Data can be â€œpresentâ€ but still broken.

**Example**:\
A gender column with: `"Male"`, `"male"`, `"M"`, `"man"`, `""`\
Are they the same? Yes.\
Will they be treated the same? Definitely not.

This is a form of *semantic missingness*: when inconsistency masks true values, leading to undercounts and poor grouping.

**Fix with**: `stringr::str_to_lower()`, `forcats::fct_collapse()`, or domain-specific mappings.

## Detecting the Invisible

The worst kind of missing data is the kind you donâ€™t notice. Fortunately, you can train yourself â€” and your code â€” to spot it. Here are some tidy strategies to make the invisible visible:

### ğŸ” **Scan for Common Placeholder Patterns**

Use `dplyr` and `stringr` to find likely stand-ins for missing values:

``` r
library(dplyr)
library(stringr)

df %>%
  summarise(across(everything(), ~sum(str_detect(.x, "^(unknown|n/?a|\\-|\\.)$"), na.rm = TRUE)))
```

This quickly tells you how often placeholder values show up across your dataset.

### ğŸ“Š **Check Category Frequency and Odd Levels**

Suspiciously high numbers of `"other"` or `"none"`? Run a quick frequency check:

``` r
df %>%
  count(gender, sort = TRUE)
```

Youâ€™ll spot typos, inconsistent spellings, and hidden blanks (`""`) just by looking at the top counts.

### ğŸ“ **Visualize Missingness**

Use packages like `naniar` or `visdat` to create heatmaps and summaries:

``` r
library(naniar)
vis_miss(df)

miss_var_summary(df)
```

These tools help surface missing values *and* patterns â€” for example, fields that are always missing together.

### ğŸ“¦ **Look for Missing Combinations**

Are you missing entire rows â€” not just values? `tidyr::complete()` is your friend:

``` r
library(tidyr)

df %>%
  complete(month = 1:12, region)
```

This expands the dataset to include all expected combinations, revealing gaps in grouped data.

### ğŸš© **Anti-Joins as Absence Detectors**

Want to know which expected IDs or codes are missing from another table?

``` r
anti_join(reference_table, raw_data, by = "id")
```

This technique is simple and powerful for checking completeness.

### ğŸ§¼ **Check for Structural Blanks and Whitespace**

Strings that *look* empty can slip through if they contain space or tab characters:

``` r
df %>%
  summarise(across(everything(), ~sum(str_trim(.x) == "", na.rm = TRUE)))
```

Cleaning tools like `stringr::str_trim()` and `janitor::remove_empty()` help catch these.

By combining textual pattern detection, visual inspection, and structure-aware tools like `complete()` and `anti_join()`, you create a robust toolkit for uncovering hidden missingness before it hurts your analysis.

## Not Just NA: When Numbers Misbehave

Some missingness hides not behind strings or empty rows, but behind **weird numeric values** that most analysts overlook. These arenâ€™t *technically* `NA`, but theyâ€™re not usable either â€” and if left untreated, they can break models, inflate summaries, or simply vanish in charts without explanation.

Letâ€™s unpack the main offenders:

### ğŸ§® **`NaN` â€” Not a Number**

`NaN` (Not a Number) typically shows up after invalid operations, like:

``` r
0 / 0
sqrt(-1)
```

While `NA` indicates a missing value, `NaN` means â€œthis computation failed.â€ In modeling or plotting, these can cause unexpected skips or warnings â€” and `NaN`s are often silently dropped.

**Detect with**:

``` r
is.nan(x)
```

### ğŸ” **`Inf` and `-Inf` â€” Infinite Values**

These often result from divisions like:

``` r
1 / 0  # Inf
-1 / 0 # -Inf
```

Theyâ€™re technically numeric but **canâ€™t be visualized meaningfully** or used in summaries. For instance, calculating the mean of a vector with `Inf` will return `Inf`, breaking downstream logic.

**Detect with**:

``` r
is.infinite(x)
```

**Clean with**::

``` r
x[is.infinite(x)] <- NA
```

### ğŸ§± **`NULL` â€” The Absence of an Object**

Less common in tidy workflows, but relevant in programming contexts (`list`, `purrr`, API responses). Unlike `NA`, which is a placeholder, `NULL` means â€œnothing here at allâ€ â€” no memory allocation, no structure.

In data frames, `NULL`s typically donâ€™t show up, but they *do* appear in nested lists or when working with APIs, R6, or reactive Shiny components.

**Use with caution** â€” donâ€™t assume `length(NULL) == 1`.

### ğŸ‘€ **Why It Matters**

While these arenâ€™t hidden the same way as `"unknown"` or `""`, they *are* often **forgotten** in early cleaning stages. Worse, they:

-   Donâ€™t always trigger missing value warnings

-   Can silently distort statistical functions

-   Often sneak past `is.na()`

âœ… **Quick Check Template**

``` r
df %>%
  summarise(across(where(is.numeric), list(
    na = ~sum(is.na(.)),
    nan = ~sum(is.nan(.)),
    inf = ~sum(is.infinite(.))
  ))
```

Consider this your cleanup checklist for the â€œnon-missingâ€ missing values â€” the mathematically broken pieces of your dataset that need just as much attention as `NA`s.

## Fixing the Problem

Once youâ€™ve uncovered hidden missing values, the next step is to clean them â€” systematically and reproducibly. Hereâ€™s how to do it using tidyverse tools.

### ğŸ§½ **Standardize Known Placeholders to NA**

Start by replacing known placeholder strings with real `NA`s:

``` r
df_clean <- df %>%
  mutate(across(
    where(is.character),
    ~na_if(.x, "unknown")
  )) %>%
  mutate(across(
    where(is.character),
    ~na_if(.x, "n/a")
  ))
```

Or wrap it up in a single step using `case_when()` or a custom function:

``` r
replace_missing <- function(x) {
  na_if(trimws(tolower(x)), "unknown") %>%
    na_if("n/a") %>%
    na_if("-") %>%
    na_if(".")
}

df_clean <- df %>%
  mutate(across(where(is.character), replace_missing))
```

ğŸ› **Clean Blank and Whitespace-Only Strings**

``` r
df_clean <- df_clean %>%
  mutate(across(where(is.character), ~na_if(str_trim(.x), "")))
```

This removes invisible blanks that otherwise pass unnoticed.

### ğŸ”¢ **Differentiate Real Zeros from Structural Zeros**

Use domain knowledge or logic to reinterpret misleading zeros:

``` r
df_clean <- df_clean %>%
  mutate(revenue = if_else(event_logged == FALSE, NA_real_, revenue))
```

Or flag suspect zeroes for manual review:

``` r
df_clean <- df_clean %>%
  mutate(zero_suspect = revenue == 0 & event_logged == FALSE)
```

### ğŸ§± **Rebuild the Structure with `complete()`**

To ensure all expected combinations are represented:

``` r
df_complete <- df_clean %>%
  complete(product, month = 1:12, fill = list(sales = 0))
```

This is especially useful for grouped time series or faceted charts that expect consistent row structures.

### ğŸ”  **Unify Inconsistent Categories**

Clean categorical variables using `stringr` and `forcats`:

``` r
df_clean <- df_clean %>%
  mutate(gender = str_to_lower(gender)) %>%
  mutate(gender = fct_collapse(gender,
                               male = c("male", "m", "man"),
                               female = c("female", "f", "woman")))
```

This avoids category fragmentation during grouping or modeling.

### ğŸ” **Automate with Modular Cleaning Functions**

Wrap these steps into reusable helpers. Example:

``` r
clean_column <- function(x) {
  x %>%
    str_trim() %>%
    str_to_lower() %>%
    na_if("") %>%
    na_if("unknown") %>%
    na_if("n/a")
}

df_clean <- df %>%
  mutate(across(where(is.character), clean_column))
```

These small changes drastically improve the accuracy of your summaries, aggregations, joins, and models. Clean data isnâ€™t just free of `NA`s â€” itâ€™s free of ambiguity.

## Conclusion

The most dangerous kind of missing data is the one you donâ€™t know is missing.

Youâ€™ve now seen that missingness isnâ€™t just about `NA`s. It hides behind placeholder strings, lurks in empty text fields and unexpected zeros, and sometimes vanishes entirely in the form of unrecorded combinations. It also disguises itself as **numeric edge cases** â€” `NaN`, `Inf`, or `NULL` â€” that silently break your calculations or skip rows in charts.

These values donâ€™t always raise red flags. But they distort your summaries, pollute your models, and make your data look more complete than it really is.

The fix? Donâ€™t just clean your data â€” interrogate it.

Make it a habit to:

-   Look beyond `is.na()`

-   Normalize suspicious placeholders and broken numeric values

-   Rebuild expected structures with `complete()`

-   Visualize not only what exists â€” but also whatâ€™s missing

Clean data isnâ€™t just `NA`-free â€” itâ€™s ambiguity-free.

So next time youâ€™re handed a dataset that looks â€œclean,â€ ask yourself:

> *â€œWhatâ€™s missing that I canâ€™t see?â€*

Because often, the story your data isnâ€™t telling is the one that matters most.
