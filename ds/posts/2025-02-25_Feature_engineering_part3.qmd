---
title: "Cooking Temperatures Matter â€“ Scaling & Transforming Numerical Data"
author: "Numbers around us"
date: "2025-02-25"
format: html
---

![](images/FE_3.jpg)

After mastering categorical feature engineering, itâ€™s time to focus on **numerical data transformations**â€”a crucial step that can **make or break** model performance. Many machine learning algorithms **assume numerical features follow a specific distribution**, and incorrect handling can lead to **biased models, slow training, and poor generalization**.

In this article, weâ€™ll explore:\
âœ… **Why scaling matters and when to use it** (and when to avoid it).\
âœ… **Different types of transformationsâ€”log, power, and polynomial features**.\
âœ… **Standardization vs. normalizationâ€”how they impact different models.**\
âœ… **Practical R & Python examples to apply numerical feature transformations effectively.**

# **ğŸ“Œ Chapter 1: Why Scaling & Transformation Matter**

## **1ï¸âƒ£ The Importance of Scaling Numerical Features**

Machine learning models rely on **mathematical operations** that are sensitive to the scale of features. Features with different ranges can **negatively impact model performance** in several ways:

ğŸ“Œ **Unbalanced feature influence** â†’ Some models (e.g., linear regression, SVMs) give **higher importance to large-magnitude features**.\
ğŸ“Œ **Slow convergence in gradient-based models** â†’ Features with large values dominate updates, making training **slower and less stable**.\
ğŸ“Œ **Distance-based models struggle** â†’ Algorithms like KNN and K-Means depend on **distance calculations**, and unscaled data skews these distances.

**ğŸ”¹ Example: Predicting House Prices**\
Imagine we have two numerical features:

-   `square_footage` (ranging from 500 to 5000)

-   `num_bedrooms` (ranging from 1 to 5)

Since `square_footage` has a much larger range, some models might **assign it more weight than necessary**, even if `num_bedrooms` is just as important. **Scaling ensures all features contribute fairly.**

## **2ï¸âƒ£ When Scaling Is (and Isnâ€™t) Necessary**

**âœ… Models that Require Scaling:**\
âœ” **Linear Regression, Logistic Regression** (weights are sensitive to magnitude).\
âœ” **KNN, K-Means, PCA** (distance-based models).\
âœ” **SVMs, Neural Networks** (gradient-based optimizers).

**âŒ Models That Handle Scaling Automatically:**\
âœ– **Tree-based models (Random Forest, XGBoost, LightGBM)**â€”Trees split on feature values, so scaling **does not affect performance**.\
âœ– **Naive Bayes**â€”Uses probabilities, not distance-based measures.\

# **ğŸ“Œ Chapter 2: Standardization vs. Normalization**

Scaling numerical data is an essential preprocessing step, but **not all scaling methods are created equal**. Two of the most common techniquesâ€”**standardization and normalization**â€”serve different purposes and should be applied depending on the dataset and model.

But before we dive in, letâ€™s talk about something **unexpectedly funny** in Râ€™s `recipes` package:

-   **"Normalization" (`step_normalize()`) actually means standardization (Z-score transformation).**

-   **"Range Scaling" (`step_range()`) actually performs normalization (Min-Max scaling).**

-   So, if you thought these names were flipped, **you're not alone!** ğŸ­

Letâ€™s break it all down.

## **1ï¸âƒ£ Standardization (Z-Score Scaling) â†’ `step_normalize()`**

ğŸ”¹ **How It Works**\
Standardization **centers the data around 0** and scales it to have a **standard deviation of 1**. The formula is:

$X_{scaled} = \frac{X - \mu}{\sigma}$

Where:

-   $X$ = original feature value

-   $\mu$ = mean of the feature

-   $\sigma$ = standard deviation

### **ğŸ“Œ Example: Standardizing "House Size"**

| House Size (sqft) | Standardized Value |
|-------------------|--------------------|
| 1500              | -0.8               |
| 2000              | 0.0                |
| 2500              | 0.8                |

After standardization, the values are **centered around 0**, which prevents features with larger magnitudes from dominating smaller ones.

âœ… **When to Use Standardization**\
âœ” **Linear models (e.g., logistic regression, linear regression)** â†’ Keeps coefficients balanced.\
âœ” **PCA (Principal Component Analysis)** â†’ Reduces variance bias from large-magnitude variables.\
âœ” **SVMs, Neural Networks** â†’ Ensures faster and more stable training.

âŒ **When Not to Use Standardization**

-   **When features have a non-normal distribution**â€”since standardization assumes a bell-curve shape.

-   **For tree-based models (Random Forest, XGBoost, LightGBM)**â€”scaling does **not** impact tree-based splits.

### ğŸ“Œ R: Standardizing Features Using `recipes` (Even Though It Says "Normalize")

``` r
library(tidymodels)

recipe_standardized <- recipe(~ sqft, data = df) %>%
  step_normalize(sqft)  # Standardization (Z-score)

prepped_data <- prep(recipe_standardized) %>% bake(new_data = df)
```

### ğŸ“Œ Python: Standardizing with `StandardScaler` in `scikit-learn`

``` python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
df['sqft_standardized'] = scaler.fit_transform(df[['sqft']])
```

## **2ï¸âƒ£ Normalization (Min-Max Scaling) â†’ `step_range()`**

ğŸ”¹ **How It Works**\
Normalization **rescales feature values between a fixed range** (usually 0 to 1). The formula is:

$X_{scaled} = \frac{X - X_{\text{min}}}{X_{\text{max}} - X_{\text{min}}}$

Where:

-   $X$ = original feature value

-   $X_{min}$= minimum value in the feature

-   $X_{max}$â€‹ = maximum value in the feature

### **ğŸ“Œ Example: Normalizing "Income"**

| Income (\$) | Normalized Value |
|-------------|------------------|
| 30,000      | 0.0              |
| 50,000      | 0.5              |
| 100,000     | 1.0              |

After normalization, all values fit within the **0-1 range**, making comparisons between variables easier.

âœ… **When to Use Normalization**\
âœ” **KNN, K-Means, Neural Networks** â†’ Models relying on distance-based calculations benefit from a uniform scale.\
âœ” **Data with extreme outliers** â†’ Prevents large values from dominating calculations.

âŒ **When Not to Use Normalization**

-   **If the dataset contains meaningful negative values**â€”scaling to \[0,1\] may distort relationships.

-   **If a normal distribution is required**â€”use standardization instead.

### ğŸ“Œ R: Normalizing Features Using `recipes` (Even Though It Says "Range")

``` r
recipe_normalized <- recipe(~ Income, data = df) %>%
  step_range(Income, min = 0, max = 1)  # Min-Max Scaling (Normalization)

prepped_data <- prep(recipe_normalized) %>% bake(new_data = df)
```

### ğŸ“Œ Python: Normalizing with `MinMaxScaler` in `scikit-learn`

``` python
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
df['income_normalized'] = scaler.fit_transform(df[['Income']])
```

## **3ï¸âƒ£ When to Use Standardization vs. Normalization**

| **Scenario** | **Use Standardization (`step_normalize()`)** | **Use Normalization (`step_range()`)** |
|------------------------|------------------------|------------------------|
| **Linear Models (Regression, SVMs, PCA)** | âœ… Recommended | âŒ Not ideal |
| **Distance-Based Models (KNN, K-Means, Neural Networks)** | âŒ Not ideal | âœ… Recommended |
| **Feature values contain extreme outliers** | âœ… Helps handle outliers | âŒ Not robust to outliers |
| **Features have different units (e.g., weight in kg, height in cm)** | âœ… Ensures balanced impact | âŒ Can distort relationships |
| **Tree-Based Models (Random Forest, XGBoost)** | âŒ Not needed | âŒ Not needed |

âœ… **By choosing the right scaling method, we ensure models perform optimally without unnecessary transformations.**

### **ğŸ“Œ Funny Naming Recap (So You Donâ€™t Get Confused Again!)**

| **What We Expect** | **What `recipes` Actually Calls It** | **What It Actually Does** |
|------------------------|------------------------|------------------------|
| **Standardization (Z-score)** | `step_normalize()` | Centers data (mean = 0, std = 1) |
| **Min-Max Scaling (Normalization)** | `step_range()` | Rescales values between `[0,1]` |

Yes, itâ€™s **confusing at first**, but just remember:

-   `step_normalize()` **normalizes the distribution** (standardization).

-   `step_range()` **scales into a fixed range** (normalization).

At the end of the day, **names donâ€™t matterâ€”choosing the right technique does!** ğŸ­

# **ğŸ“Œ Chapter 3: Transforming Skewed Data**

Scaling is **not always enough** to prepare numerical data for machine learning. Many real-world datasets contain **skewed distributions**, where values are concentrated in a small range and **a few extreme values (outliers) dominate the feature**.

If we donâ€™t handle skewed data properly, it can lead to:\
âŒ **Poor model performance** (linear models assume normally distributed features).\
âŒ **Ineffective scaling** (min-max scaling doesnâ€™t fix skewness).\
âŒ **Reduced interpretability** (exponential relationships may be misrepresented).

In this chapter, weâ€™ll explore:\
âœ… **How to detect skewed data.**\
âœ… **Log, Box-Cox, and Yeo-Johnson transformations.**\
âœ… **Feature binning (discretization) to improve model interpretability.**

## **1ï¸âƒ£ Detecting Skewed Data**

Before applying transformations, we need to **check whether a feature is skewed**.

ğŸ”¹ **Right-skewed (positive skew)** â†’ Long tail on the **right** (e.g., income distribution).\
ğŸ”¹ **Left-skewed (negative skew)** â†’ Long tail on the **left** (e.g., age at retirement).

### ğŸ“Œ R: Checking Skewness in a Feature

``` r
library(e1071)

# Calculate skewness
skewness(df$income)
```

### ğŸ“Œ Python: Checking Skewness with `scipy`

``` python
from scipy.stats import skew

print(skew(df['income']))  # Positive = right-skewed, Negative = left-skewed
```

âœ… **A skewness value above Â±1 indicates a highly skewed feature that may need transformation.**

## **2ï¸âƒ£ Log Transformation**

**ğŸ”¹ How It Works**\
Log transformation **reduces right skewness** by compressing large values while keeping small values distinct:

$X' = \log(X+1)$

(The +1 prevents issues with zero values.)

ğŸ”¹ **Best for:** Right-skewed data (e.g., income, sales, house prices).\
ğŸ”¹ **Not useful if data contains negative or zero values.**

### **ğŸ“Œ R: Applying Log Transformation**

``` r
df$income_log <- log(df$income + 1)  # Log-transform income
```

### ğŸ“Œ Python: Applying Log Transformation

``` python
import numpy as np

df['income_log'] = np.log1p(df['income'])  # log(1 + x)
```

âœ… **Log transformation is a simple way to make right-skewed features more normal.**\
âŒ **Avoid using it on features with negative values!**

## **3ï¸âƒ£ Box-Cox Transformation (Only for Positive Data)**

ğŸ”¹ **How It Works**\
Box-Cox transformation adjusts skewed data **dynamically** using a parameter Î»\lambdaÎ»:

\begin{cases} \frac{X^\lambda - 1}{\lambda}, & \lambda \neq 0 \\ \log(X), & \lambda = 0 \end{cases}

ğŸ”¹ **Best for:** Data that is **not strictly right-skewed** and requires a flexible transformation.\
ğŸ”¹ **Only works for positive values!**

### **ğŸ“Œ R: Applying Box-Cox Transformation**

``` r
library(MASS)

df$income_boxcox <- boxcox(df$income + 1, lambda = seq(-2, 2, by = 0.1))  # Find best lambda
```

### ğŸ“Œ Python: Applying Box-Cox Transformation

``` python
from scipy.stats import boxcox

df['income_boxcox'], lambda_opt = boxcox(df['income'] + 1)  # Apply transformation
```

âœ… **Box-Cox adapts to different types of skewness dynamically.**\
âŒ **Only works on strictly positive values!**

## **4ï¸âƒ£ Yeo-Johnson Transformation (Handles Negative & Zero Values)**

ğŸ”¹ **How It Works**\
Yeo-Johnson is similar to Box-Cox but works **on both positive and negative values**, making it more flexible.

\begin{cases} \frac{(X + 1)^\lambda - 1}{\lambda}, & X \geq 0, \lambda \neq 0 \\ \frac{-(|X| + 1)^{2 - \lambda} - 1}{2 - \lambda}, & X < 0, \lambda \neq 2 \end{cases}

ğŸ”¹ **Best for:** Right- or left-skewed data, including negative values.\
ğŸ”¹ **More robust than Box-Cox, since it handles negatives!**

### **ğŸ“Œ R: Applying Yeo-Johnson Transformation**

``` r
df$income_yeojohnson <- bestNormalize::yeojohnson(df$income)$x.t
```

### ğŸ“Œ Python: Applying Yeo-Johnson Transformation

``` python
from sklearn.preprocessing import PowerTransformer

yeojohnson = PowerTransformer(method='yeo-johnson')
df['income_yeojohnson'] = yeojohnson.fit_transform(df[['income']])
```

âœ… **Yeo-Johnson is the best choice when data contains negative values.**\
âŒ **Slightly more computationally expensive than log or Box-Cox transformations.**

## **5ï¸âƒ£ Feature Binning (Discretization)**

Sometimes, instead of transforming continuous data, **itâ€™s better to convert it into categories** (bins).

ğŸ”¹ **Best for:**\
âœ” Features that **donâ€™t need fine-grained numeric precision**.\
âœ” Making data **more interpretable** (e.g., income brackets).

### ğŸ“Œ R: Binning Income into Categories

``` r
df$income_bin <- cut(df$income, breaks = c(0, 30000, 70000, 150000), labels = c("Low", "Medium", "High"))
```

### ğŸ“Œ Python: Binning Income Using `pd.cut()`

``` python
df['income_bin'] = pd.cut(df['income'], bins=[0, 30000, 70000, 150000], labels=["Low", "Medium", "High"])
```

âœ… **Binning can improve model interpretability, especially in decision trees.**\
âŒ **May lose fine-grained numeric detail.**

## **ğŸ“Œ Summary: Choosing the Right Transformation**

| **Transformation** | **Fixes** | **Handles Negative Values?** | **Best For** |
|------------------|------------------|------------------|------------------|
| **Log Transform** | Right-skewed data | âŒ No | Income, sales, house prices |
| **Box-Cox** | Flexible skew correction | âŒ No | Normally distributed features that need transformation |
| **Yeo-Johnson** | Both right & left skew | âœ… Yes | Datasets with negatives & zeros |
| **Feature Binning** | Converts numeric to categories | âœ… Yes | Making features more interpretable |

âœ… **By selecting the right transformation, we can make skewed data more usable for models while preserving interpretability.**

# **ğŸ“Œ Chapter 4: Best Practices & Common Pitfalls in Numerical Transformations**

Weâ€™ve explored **scaling and transforming numerical features**, but applying these techniques without a strategy can lead to **poor model performance, data leakage, or unnecessary complexity**.

This chapter will focus on:\
âœ… **When to apply transformations before vs. after scaling.**\
âœ… **How to prevent data leakage when using transformations.**\
âœ… **Avoiding overfitting when using feature binning.**

## **1ï¸âƒ£ Should You Transform Before or After Scaling?**

One common question in preprocessing is: **"Should I apply log transformations or Box-Cox before or after standardization/normalization?"**

ğŸ”¹ **Always transform first, then scale.**

Why?\
ğŸ“Œ **Log, Box-Cox, and Yeo-Johnson are meant to fix skewness**â€”scaling first would distort their effect.\
ğŸ“Œ **Scaling should be the last step** before feeding data into the model, ensuring all features are on the same scale.

### **ğŸ“Œ Correct Order of Preprocessing Steps**

| **Step** | **What It Does** | **Example Transformation** |
|----|----|----|
| 1ï¸âƒ£ Handle missing values | Avoids issues in transformation | Mean/median imputation |
| 2ï¸âƒ£ Apply transformations | Fixes skewed distributions | Log, Box-Cox, Yeo-Johnson |
| 3ï¸âƒ£ Scale features | Ensures uniform range | Standardization (Z-score) or Min-Max Scaling |

âœ… **This ensures transformations work correctly before values are scaled.**

### ğŸ“Œ R: Correct Order in a `recipes` Pipeline

``` r
recipe_pipeline <- recipe(~ income, data = df) %>%
  step_impute_median(income) %>%  # Handle missing values
  step_log(income, base = 10) %>%  # Fix skewness
  step_normalize(income)  # Standardize feature (mean 0, std 1)

prepped_data <- prep(recipe_pipeline) %>% bake(new_data = df)
```

### ğŸ“Œ Python: Correct Order in `scikit-learn` Pipeline

``` python
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, FunctionTransformer
import numpy as np

pipeline = Pipeline([
    ('log_transform', FunctionTransformer(np.log1p)),  # Apply log transformation
    ('scaler', StandardScaler())  # Standardize feature
])

df['income_transformed'] = pipeline.fit_transform(df[['income']])
```

âœ… **Applying transformations before scaling ensures proper feature engineering.**

## **2ï¸âƒ£ Preventing Data Leakage in Feature Transformations**

Feature transformations should **only be learned from the training set**â€”if applied to the full dataset before splitting, they can **leak information** into the test set.

ğŸš¨ **Common leakage risks:**\
âŒ **Fitting scalers or transformers on the full dataset** (instead of just training data).\
âŒ **Binning continuous values using test set information** (test data should only be transformed using bins learned from training).

### ğŸ“Œ R: Preventing Leakage in `recipes`

``` r
recipe_pipeline <- recipe(~ income, data = df_train) %>%
  step_log(income, base = 10) %>%
  step_normalize(income)

prepped_train <- prep(recipe_pipeline) %>% bake(new_data = df_train)
prepped_test <- bake(recipe_pipeline, new_data = df_test)  # Apply transformations to test set
```

### ğŸ“Œ Python: Preventing Leakage in `scikit-learn`

``` python
from sklearn.model_selection import train_test_split

# Split data
df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)

# Fit transformer on training set only
pipeline.fit(df_train[['income']])

# Apply transformation separately to test set
df_train['income_transformed'] = pipeline.transform(df_train[['income']])
df_test['income_transformed'] = pipeline.transform(df_test[['income']])
```

âœ… **Always fit scalers and transformers on the training set and apply them separately to the test set.**

## **3ï¸âƒ£ Avoiding Overfitting in Feature Binning**

Binning (discretization) can improve model interpretability, but if **bins are too small**, models may overfit.

ğŸš¨ **Common mistakes in binning:**\
âŒ **Using too many bins** â†’ Models memorize categories instead of learning patterns.\
âŒ **Defining bins based on test data** â†’ This introduces **data leakage**.\
âŒ **Creating bins without checking distribution** â†’ May result in **uneven data distribution**.

### **ğŸ“Œ Best Practices for Feature Binning**

âœ” Use **quantile-based binning** instead of equal-width bins.\
âœ” Ensure bins **capture meaningful groups** (e.g., income brackets).\
âœ” **Test different bin sizes** to prevent overfitting.

### **ğŸ“Œ R: Creating Quantile-Based Bins**

``` r
df$income_bins <- cut_number(df$income, n = 4)  # Creates 4 equal-sized bins
```

### ğŸ“Œ Python: Creating Quantile-Based Bins

``` python
df['income_bins'] = pd.qcut(df['income'], q=4, labels=["Low", "Medium", "High", "Very High"])
```

## **ğŸ“Œ Summary: Best Practices & Pitfalls in Numerical Feature Transformations**

| âœ… Best Practices | âŒ Pitfalls to Avoid |
|----|----|
| **Transform before scaling** | Scaling before transformation can distort effects. |
| **Fit transformers on the training set only** | Applying to full dataset causes data leakage. |
| **Use quantile-based binning instead of equal-width bins** | Too many bins lead to overfitting. |
| **Choose the right transformation for the data distribution** | Using log transformations on negative values breaks models. |

âœ… **By following these principles, numerical transformations can significantly improve model performance without introducing bias.**

# **ğŸ“Œ Chapter 5: Conclusion & Next Steps**

Numerical feature transformations **play a critical role** in ensuring machine learning models are trained on data that is **well-scaled, properly distributed, and interpretable**. Throughout this article, weâ€™ve explored the **best techniques to transform numerical features**, prevent issues like skewness and overfitting, and **apply these techniques effectively in R and Python**.

## **1ï¸âƒ£ Key Takeaways**

âœ” **Scaling matters for certain models**â€”linear models, distance-based models, and neural networks benefit from standardization or normalization.\
âœ” **Not all scaling methods are the same**â€”Standardization (`step_normalize()` in R, `StandardScaler()` in Python) is different from Min-Max Normalization (`step_range()` in R, `MinMaxScaler()` in Python).\
âœ” **Skewed features should be transformed before scaling**â€”using log, Box-Cox, or Yeo-Johnson transformations.\
âœ” **Prevent data leakage when applying transformations**â€”always fit transformations on the training set and apply them separately to the test set.\
âœ” **Binning can improve interpretability but must be done carefully**â€”too many bins can lead to **overfitting** and poor generalization.

âœ… **By selecting the right transformation technique, we ensure that numerical data is properly prepared for machine learning models, improving performance and interpretability.**

## **2ï¸âƒ£ Whatâ€™s Next?**

This article is part of the **Feature Engineering Series**, where we explore how to create **better predictive models by transforming raw data**.

ğŸš€ **Next up: "Timing is Everything â€“ Feature Engineering for Time-Series Data"**\
In the next article, weâ€™ll cover:\
ğŸ“Œ **Time-based aggregations** (rolling averages, cumulative sums).\
ğŸ“Œ **Extracting date-based features** (day of the week, seasonality indicators).\
ğŸ“Œ **Lag and lead variables** for forecasting.\
ğŸ“Œ **Handling missing time-series data effectively.**

ğŸ”¹ **Want to stay updated?** Keep an eye out for the next post in the series!

ğŸ’¡ **Whatâ€™s your go-to method for transforming numerical features? Drop your thoughts below!** â¬‡â¬‡â¬‡
