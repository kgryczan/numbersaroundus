<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.37">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Numbers around us">
<meta name="dcterms.date" content="2025-02-11">

<title>Feature Engineering 101: Prepping Your Ingredients for Success – Numbers Around Us</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-697306ee647f3aecb60be57249203282.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-3fe3df12cb322cd60d4f50ab5ce79ec8.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-71a1c0b2fece7d4d1d4daa5e23e0c0ad.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark-00ab377d17f9c7e1b98bb5b1ff2b2f0e.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Numbers Around Us</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../last-articles.html"> 
<span class="menu-text">Newest articles</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../challenges/index.html"> 
<span class="menu-text">Challenges</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../bi/index.html"> 
<span class="menu-text">Business Intelligence</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../ds/index.html"> 
<span class="menu-text">Data Science</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../dp/index.html"> 
<span class="menu-text">Data Philosophy</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
    <a href="https://github.com" title="Github" class="quarto-navigation-tool px-1" aria-label="Github"><i class="bi bi-github"></i></a>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Feature Engineering 101: Prepping Your Ingredients for Success</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Numbers around us </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">February 11, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<p><img src="images/FE_1.jpg" class="img-fluid"></p>
<section id="what-is-feature-engineering" class="level3">
<h3 class="anchored" data-anchor-id="what-is-feature-engineering"><strong>What is Feature Engineering?</strong></h3>
<p>Feature engineering is the process of transforming raw data into <strong>meaningful, informative variables</strong> that improve the predictive power of models. While machine learning algorithms can recognize patterns in data, they rely on well-prepared inputs to make accurate predictions.</p>
<p>Just like a chef carefully preps ingredients before cooking, a data scientist must refine raw data <strong>before</strong> passing it to a model. <strong>Messy, irrelevant, or unoptimized data can lead to poor performance, even with the best algorithms.</strong></p>
<p>Think about cooking:</p>
<ul>
<li><p>If you use high-quality, well-prepared ingredients, the final dish will be <strong>delicious and well-balanced</strong>.</p></li>
<li><p>If you throw in <strong>random, untested elements</strong>, your dish (model) may <strong>taste awful or even fail</strong>.</p></li>
</ul>
<p>Similarly, the difference between an <strong>amateur and a professional chef</strong> is knowing:<br>
✔️ What ingredients (features) work best together.<br>
✔️ Which ones to remove to avoid ruining the dish (model).<br>
✔️ How to adjust flavors (transformations) for the best final result.</p>
</section>
<section id="why-feature-engineering-is-crucial-in-data-science" class="level3">
<h3 class="anchored" data-anchor-id="why-feature-engineering-is-crucial-in-data-science"><strong>Why Feature Engineering is Crucial in Data Science</strong></h3>
<p>Even the most <strong>advanced machine learning models rely on quality data</strong>. No amount of hyperparameter tuning can compensate for <strong>bad feature selection or poor transformations</strong>.</p>
<section id="good-feature-engineering-leads-to" class="level4">
<h4 class="anchored" data-anchor-id="good-feature-engineering-leads-to"><strong>Good Feature Engineering Leads to:</strong></h4>
<p>✅ <strong>Higher Predictive Accuracy:</strong> Well-chosen features improve a model’s ability to generalize.<br>
✅ <strong>Faster Model Training:</strong> Reducing unnecessary features speeds up training.<br>
✅ <strong>Better Interpretability:</strong> Features that make sense allow for easier debugging and analysis.<br>
✅ <strong>Reduced Overfitting:</strong> Eliminating redundant or misleading features helps prevent the model from memorizing noise.</p>
<p>Without feature engineering, you may be using <strong>irrelevant, noisy, or redundant data</strong>, which can lead to <strong>worse results, longer training times, and overfitting</strong>.</p>
</section>
</section>
<section id="how-features-impact-model-performance-a-real-world-example" class="level3">
<h3 class="anchored" data-anchor-id="how-features-impact-model-performance-a-real-world-example"><strong>How Features Impact Model Performance: A Real-World Example</strong></h3>
<p>Let’s say we are building a machine learning model to predict <strong>house prices</strong> based on available data. Our dataset contains:</p>
<ul>
<li><p><strong>Square footage</strong> of the house</p></li>
<li><p><strong>Number of bedrooms</strong></p></li>
<li><p><strong>Number of bathrooms</strong></p></li>
<li><p><strong>Location</strong></p></li>
<li><p><strong>Age of the house</strong></p></li>
</ul>
<p>This is a <strong>decent</strong> starting point, but it may not be enough for a high-quality model.</p>
<section id="how-feature-engineering-improves-this-dataset" class="level4">
<h4 class="anchored" data-anchor-id="how-feature-engineering-improves-this-dataset"><strong>How Feature Engineering Improves This Dataset:</strong></h4>
<table class="caption-top table">
<colgroup>
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th>Feature</th>
<th>Raw Form</th>
<th>Engineered Version</th>
<th>Why It’s Useful</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Square Footage</td>
<td>2500</td>
<td>Price per square foot = price / sqft</td>
<td>Normalizes pricing across house sizes</td>
</tr>
<tr class="even">
<td>Location</td>
<td>“Downtown”</td>
<td>Distance to city center (km)</td>
<td>Captures geographic price variation</td>
</tr>
<tr class="odd">
<td>Bedrooms &amp; Bathrooms</td>
<td>3 beds, 2 baths</td>
<td>Room-to-bathroom ratio = beds/baths</td>
<td>Reflects usability of space</td>
</tr>
<tr class="even">
<td>House Age</td>
<td>15 years</td>
<td>Has Renovation (1/0)</td>
<td>Highlights recent renovations</td>
</tr>
<tr class="odd">
<td>Lot Size</td>
<td>5000 sqft</td>
<td>Lot Size to House Ratio = lot sqft / house sqft</td>
<td>Identifies whether land size impacts pricing</td>
</tr>
</tbody>
</table>
<p>Each of these <strong>engineered features</strong> can add valuable insights, making the model <strong>more robust and predictive</strong>.</p>
</section>
</section>
<section id="when-do-you-need-feature-engineering" class="level3">
<h3 class="anchored" data-anchor-id="when-do-you-need-feature-engineering"><strong>When Do You Need Feature Engineering?</strong></h3>
<p>Not every dataset requires extensive feature engineering, but <strong>certain situations</strong> make it absolutely necessary:</p>
<p>✅ <strong>When your raw data lacks meaningful predictors:</strong></p>
<ul>
<li>Example: Predicting user churn in an app based only on <strong>login frequency</strong>. A better approach might involve <strong>time-based features</strong> (e.g., “days since last login,” “average session length”).</li>
</ul>
<p>✅ <strong>When raw variables have nonlinear relationships with the target:</strong></p>
<ul>
<li>Example: The relationship between <strong>income and spending habits</strong> may not be linear. <strong>Log transformations or percentiles</strong> can help normalize these relationships.</li>
</ul>
<p>✅ <strong>When there are many categorical variables:</strong></p>
<ul>
<li>Example: A dataset with <strong>country names</strong> will be more useful if transformed into <strong>continent-based grouping</strong> or encoded into <strong>clustering-friendly formats</strong>.</li>
</ul>
<p>✅ <strong>When working with time-series data:</strong></p>
<ul>
<li>Example: Instead of using <strong>raw timestamps</strong>, extracting <strong>day of the week, month, quarter, and seasonality patterns</strong> can reveal meaningful trends.</li>
</ul>
</section>
<section id="common-challenges-in-feature-engineering" class="level3">
<h3 class="anchored" data-anchor-id="common-challenges-in-feature-engineering"><strong>Common Challenges in Feature Engineering</strong></h3>
<p>Feature engineering is <strong>powerful</strong>, but it comes with its own set of <strong>challenges</strong>:</p>
<p>❌ <strong>Overfitting to Training Data</strong> → Adding too many engineered features <strong>may create spurious correlations</strong> that don’t generalize well.<br>
❌ <strong>High-Dimensional Data</strong> → If you add <strong>too many features</strong>, it can increase complexity without meaningful improvement.<br>
❌ <strong>Correlation Between Features</strong> → Some features may be <strong>redundant or highly correlated</strong>, reducing the benefit of adding them.<br>
❌ <strong>Computational Cost</strong> → Complex transformations (e.g., <strong>Fourier features for time-series data</strong>) may be <strong>too expensive for real-time applications</strong>.</p>
</section>
<section id="summary-why-feature-engineering-is-like-cooking" class="level3">
<h3 class="anchored" data-anchor-id="summary-why-feature-engineering-is-like-cooking"><strong>Summary: Why Feature Engineering is Like Cooking</strong></h3>
<ul>
<li><p>Just like a great meal depends on <strong>carefully prepped ingredients</strong>, a great model depends on <strong>carefully chosen features</strong>.</p></li>
<li><p>Some raw ingredients (features) need to be <strong>cleaned, processed, and refined</strong> to be useful.</p></li>
<li><p>Too many ingredients (features) can <strong>overwhelm</strong> the dish (model), making it <strong>confusing and ineffective</strong>.</p></li>
<li><p><strong>The right balance</strong> leads to the <strong>best results</strong>—for both cooking and machine learning.</p></li>
</ul>
</section>
<section id="what-makes-a-good-feature" class="level2">
<h2 class="anchored" data-anchor-id="what-makes-a-good-feature"><strong>What Makes a Good Feature?</strong></h2>
<section id="defining-a-good-feature" class="level3">
<h3 class="anchored" data-anchor-id="defining-a-good-feature"><strong>Defining a “Good” Feature</strong></h3>
<p>Not all features are created equal. Some <strong>enhance</strong> a model’s predictive power, while others <strong>add noise or redundancy</strong>. The difference between a good and bad feature is similar to choosing the right <strong>ingredients for a recipe</strong>—some elements <strong>elevate the dish</strong>, while others <strong>clash or dilute the flavors</strong>.</p>
<section id="a-good-feature-should-be" class="level4">
<h4 class="anchored" data-anchor-id="a-good-feature-should-be"><strong>A Good Feature Should Be:</strong></h4>
<p>✅ <strong>Relevant</strong> – It has a meaningful relationship with the target variable.<br>
✅ <strong>Predictive</strong> – It improves the model’s ability to make accurate predictions.<br>
✅ <strong>Independent</strong> – It adds new information rather than repeating existing data.<br>
✅ <strong>Interpretable</strong> – It makes logical sense and can be explained to stakeholders.<br>
✅ <strong>Efficient</strong> – It balances performance improvement with computational cost.</p>
</section>
</section>
<section id="relevance-does-the-feature-matter-for-prediction" class="level3">
<h3 class="anchored" data-anchor-id="relevance-does-the-feature-matter-for-prediction"><strong>1️⃣ Relevance: Does the Feature Matter for Prediction?</strong></h3>
<p>A feature is <strong>relevant</strong> if it has a meaningful <strong>correlation</strong> with the outcome we’re trying to predict.</p>
<section id="example-predicting-car-prices" class="level4">
<h4 class="anchored" data-anchor-id="example-predicting-car-prices"><strong>Example: Predicting Car Prices</strong></h4>
<p>If we are building a model to predict <strong>car prices</strong>, which of the following features are relevant?</p>
<table class="caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th>Feature</th>
<th>Relevant?</th>
<th>Why?</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Engine Size</td>
<td>✅ Yes</td>
<td>Larger engines generally increase car price.</td>
</tr>
<tr class="even">
<td>Number of Cup Holders</td>
<td>❌ No</td>
<td>This has little to no impact on price.</td>
</tr>
<tr class="odd">
<td>Brand Name</td>
<td>✅ Yes</td>
<td>Premium brands usually have higher prices.</td>
</tr>
<tr class="even">
<td>Car Color</td>
<td>❓ Maybe</td>
<td>If certain colors have higher resale value, it may be relevant.</td>
</tr>
</tbody>
</table>
<p>👉 <strong>Lesson:</strong> Just because a variable is available in the dataset doesn’t mean it’s useful.</p>
</section>
<section id="how-to-test-feature-relevance" class="level4">
<h4 class="anchored" data-anchor-id="how-to-test-feature-relevance"><strong>How to Test Feature Relevance?</strong></h4>
<ul>
<li><p><strong>Correlation (Pearson/Spearman):</strong> Measures how strongly a numerical feature is related to the target variable.</p></li>
<li><p><strong>ANOVA/F-test:</strong> Tests whether categorical variables significantly impact the target.</p></li>
<li><p><strong>Mutual Information:</strong> Measures <strong>nonlinear</strong> relationships between features and the target.</p></li>
</ul>
</section>
</section>
<section id="predictiveness-does-the-feature-improve-model-accuracy" class="level3">
<h3 class="anchored" data-anchor-id="predictiveness-does-the-feature-improve-model-accuracy"><strong>2️⃣ Predictiveness: Does the Feature Improve Model Accuracy?</strong></h3>
<p>Even if a feature is <strong>somewhat relevant</strong>, it may not meaningfully improve predictions. <strong>Features with low predictive power add noise</strong> rather than useful signals.</p>
<section id="example-predicting-employee-turnover" class="level4">
<h4 class="anchored" data-anchor-id="example-predicting-employee-turnover"><strong>Example: Predicting Employee Turnover</strong></h4>
<p>We want to predict whether an employee will leave a company. Consider these two features:</p>
<ol type="1">
<li><p><strong>Salary Increase in the Last Year (%)</strong></p></li>
<li><p><strong>Employee’s ID Number</strong></p></li>
</ol>
<p>✔️ The <strong>salary increase</strong> may <strong>predict retention</strong> (employees with high raises may stay).<br>
❌ The <strong>employee ID number</strong> has no meaningful impact on turnover.</p>
</section>
<section id="how-to-measure-predictiveness" class="level4">
<h4 class="anchored" data-anchor-id="how-to-measure-predictiveness"><strong>How to Measure Predictiveness?</strong></h4>
<ul>
<li><p><strong>Train a simple model</strong> using just one feature at a time.</p></li>
<li><p><strong>Check model accuracy:</strong> If removing the feature <strong>doesn’t change accuracy</strong>, it’s probably <strong>useless</strong>.</p></li>
<li><p><strong>Feature importance scores</strong> from decision trees or SHAP values.</p></li>
</ul>
</section>
</section>
<section id="independence-does-the-feature-add-unique-information" class="level3">
<h3 class="anchored" data-anchor-id="independence-does-the-feature-add-unique-information"><strong>3️⃣ Independence: Does the Feature Add Unique Information?</strong></h3>
<p>A good feature should provide <strong>new insights</strong> rather than <strong>duplicate existing data</strong>.</p>
</section>
<section id="example-redundant-features-in-a-dataset" class="level3">
<h3 class="anchored" data-anchor-id="example-redundant-features-in-a-dataset"><strong>Example: Redundant Features in a Dataset</strong></h3>
<p>We are predicting <strong>house prices</strong> and have these features:</p>
<ul>
<li><p><strong>House Size (sqft)</strong></p></li>
<li><p><strong>Number of Rooms</strong></p></li>
<li><p><strong>Number of Bedrooms</strong></p></li>
</ul>
<p>The <strong>number of rooms</strong> is already captured by <strong>house size</strong>—it <strong>doesn’t add much new information</strong>.</p>
<section id="how-to-detect-redundancy" class="level4">
<h4 class="anchored" data-anchor-id="how-to-detect-redundancy"><strong>How to Detect Redundancy?</strong></h4>
<ul>
<li><p><strong>Correlation Matrix:</strong> If two features have a correlation &gt; 0.9, one can often be removed.</p></li>
<li><p><strong>Variance Inflation Factor (VIF):</strong> High values indicate redundant predictors.</p></li>
<li><p><strong>PCA (Principal Component Analysis):</strong> Reduces redundancy by creating <strong>new composite features</strong>.</p></li>
</ul>
</section>
</section>
<section id="interpretability-can-you-explain-it" class="level3">
<h3 class="anchored" data-anchor-id="interpretability-can-you-explain-it"><strong>4️⃣ Interpretability: Can You Explain It?</strong></h3>
<p>Features should make logical sense to <strong>humans</strong>, not just models.</p>
</section>
<section id="example-black-box-vs.-explainable-features" class="level3">
<h3 class="anchored" data-anchor-id="example-black-box-vs.-explainable-features"><strong>Example: Black-Box vs.&nbsp;Explainable Features</strong></h3>
<p>A deep learning model may create <strong>complex interactions</strong> between features that work well but are hard to interpret.</p>
<p>💡 If stakeholders need to understand <strong>why</strong> a model is making decisions (e.g., in finance or healthcare), <strong>simple, well-explained features</strong> are preferable.</p>
<section id="how-to-improve-interpretability" class="level4">
<h4 class="anchored" data-anchor-id="how-to-improve-interpretability"><strong>How to Improve Interpretability?</strong></h4>
<ul>
<li><p>Use <strong>domain knowledge</strong> to select meaningful features.</p></li>
<li><p>Avoid overly <strong>complex transformations</strong> (e.g., high-degree polynomial features).</p></li>
<li><p>Use <strong>SHAP values</strong> to explain feature impact.</p></li>
</ul>
</section>
</section>
<section id="efficiency-is-the-feature-computationally-feasible" class="level3">
<h3 class="anchored" data-anchor-id="efficiency-is-the-feature-computationally-feasible"><strong>5️⃣ Efficiency: Is the Feature Computationally Feasible?</strong></h3>
<p>Some features require <strong>significant processing power</strong>. If a feature takes too long to compute, it may <strong>not be worth the cost</strong>.</p>
<section id="example-predicting-customer-churn-with-clickstream-data" class="level4">
<h4 class="anchored" data-anchor-id="example-predicting-customer-churn-with-clickstream-data"><strong>Example: Predicting Customer Churn with Clickstream Data</strong></h4>
<ul>
<li><p><strong>Basic Features (Efficient)</strong></p>
<ul>
<li><p>Number of logins in the last month</p></li>
<li><p>Average session duration</p></li>
</ul></li>
<li><p><strong>Complex Features (Expensive)</strong></p>
<ul>
<li><p>NLP-based topic modeling on user chat logs</p></li>
<li><p>Image recognition of uploaded user photos</p></li>
</ul></li>
</ul>
<p>While complex features <strong>may</strong> add predictive power, they can <strong>increase training time, require large datasets, and be difficult to scale</strong>.</p>
</section>
<section id="when-to-keep-expensive-features" class="level4">
<h4 class="anchored" data-anchor-id="when-to-keep-expensive-features"><strong>When to Keep Expensive Features?</strong></h4>
<p>✅ If they <strong>significantly boost accuracy</strong>.<br>
✅ If model <strong>interpretability isn’t a major concern</strong>.<br>
✅ If computing resources <strong>aren’t a bottleneck</strong>.</p>
</section>
</section>
<section id="putting-it-all-together-a-feature-engineering-checklist" class="level3">
<h3 class="anchored" data-anchor-id="putting-it-all-together-a-feature-engineering-checklist"><strong>Putting It All Together: A Feature Engineering Checklist</strong></h3>
<table class="caption-top table">
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<thead>
<tr class="header">
<th>Criteria</th>
<th>Questions to Ask</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Relevance</td>
<td>Does the feature have a logical connection to the target variable?</td>
</tr>
<tr class="even">
<td>Predictiveness</td>
<td>Does including this feature improve model accuracy?</td>
</tr>
<tr class="odd">
<td>Independence</td>
<td>Does this feature add <strong>new</strong> information rather than duplicate existing features?</td>
</tr>
<tr class="even">
<td>Interpretability</td>
<td>Can you explain why this feature matters?</td>
</tr>
<tr class="odd">
<td>Efficiency</td>
<td>Is it computationally feasible to calculate this feature?</td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="feature-creation-vs.-feature-selection-vs.-feature-extraction" class="level2">
<h2 class="anchored" data-anchor-id="feature-creation-vs.-feature-selection-vs.-feature-extraction"><strong>Feature Creation vs.&nbsp;Feature Selection vs.&nbsp;Feature Extraction</strong></h2>
<p>Now that we understand what makes a <strong>good feature</strong>, it’s time to explore the <strong>three core processes</strong> of feature engineering:</p>
<p>1️⃣ <strong>Feature Creation</strong> – Designing new features from existing data.<br>
2️⃣ <strong>Feature Selection</strong> – Choosing only the most valuable features for a model.<br>
3️⃣ <strong>Feature Extraction</strong> – Transforming raw features into a <strong>more useful representation</strong>.</p>
<p>Each of these steps plays a crucial role in <strong>improving model performance</strong> while balancing accuracy, interpretability, and efficiency.</p>
<section id="feature-creation-generating-new-insights-from-data" class="level3">
<h3 class="anchored" data-anchor-id="feature-creation-generating-new-insights-from-data"><strong>1️⃣ Feature Creation: Generating New Insights from Data</strong></h3>
<p>Feature creation is about <strong>deriving new, meaningful variables</strong> from raw data. This step is <strong>often the key to unlocking hidden patterns</strong> that improve model performance.</p>
<section id="example-predicting-house-prices" class="level4">
<h4 class="anchored" data-anchor-id="example-predicting-house-prices"><strong>📌 Example: Predicting House Prices</strong></h4>
<p>Consider a dataset with the following raw variables:</p>
<ul>
<li><p><strong>Total square footage</strong></p></li>
<li><p><strong>Number of rooms</strong></p></li>
<li><p><strong>Year built</strong></p></li>
</ul>
<p>While these raw variables provide <strong>some insights</strong>, we can <strong>create new features</strong> that add even more value:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th><strong>New Feature</strong></th>
<th><strong>Formula / Definition</strong></th>
<th><strong>Why It’s Useful?</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Price per square foot</strong></td>
<td><code>price / sqft</code></td>
<td>Normalizes price differences between large and small houses.</td>
</tr>
<tr class="even">
<td><strong>Room-to-bathroom ratio</strong></td>
<td><code>num_rooms / num_bathrooms</code></td>
<td>Helps capture the usability of space.</td>
</tr>
<tr class="odd">
<td><strong>House Age</strong></td>
<td><code>Current Year - Year Built</code></td>
<td>Older homes may have lower values.</td>
</tr>
<tr class="even">
<td><strong>Renovation Status</strong></td>
<td><code>1 if last_renovation &gt; 10 years ago, else 0</code></td>
<td>Highlights recently updated homes.</td>
</tr>
</tbody>
</table>
<p><strong>Lesson:</strong> By <strong>engineering new features</strong>, we often <strong>improve model performance more than just adding more raw data.</strong></p>
</section>
<section id="common-feature-creation-techniques" class="level4">
<h4 class="anchored" data-anchor-id="common-feature-creation-techniques"><strong>Common Feature Creation Techniques</strong></h4>
<p>✅ <strong>Mathematical Transformations</strong> – Log transformations, exponentiation, ratios.<br>
✅ <strong>Aggregations</strong> – Mean, sum, count over groups (e.g., total purchases per customer).<br>
✅ <strong>Interaction Features</strong> – Multiplying or dividing two variables (e.g., room-to-bathroom ratio).<br>
✅ <strong>Domain-Specific Features</strong> – Industry-based insights (e.g., calculating BMI from height &amp; weight).</p>
</section>
</section>
<section id="feature-selection-keeping-only-the-best-ingredients" class="level3">
<h3 class="anchored" data-anchor-id="feature-selection-keeping-only-the-best-ingredients"><strong>2️⃣ Feature Selection: Keeping Only the Best Ingredients</strong></h3>
<p>Feature selection is the process of <strong>choosing the most relevant features</strong> while removing redundant or irrelevant ones.</p>
<section id="why-feature-selection-matters" class="level4">
<h4 class="anchored" data-anchor-id="why-feature-selection-matters"><strong>Why Feature Selection Matters</strong></h4>
<ul>
<li><p>Too many features <strong>increase model complexity</strong> and <strong>risk overfitting</strong>.</p></li>
<li><p>Some features may be <strong>correlated</strong> and <strong>not provide new information</strong>.</p></li>
<li><p>Unimportant features can <strong>slow down model training</strong> without improving accuracy.</p></li>
</ul>
</section>
<section id="example-predicting-employee-churn" class="level4">
<h4 class="anchored" data-anchor-id="example-predicting-employee-churn"><strong>📌 Example: Predicting Employee Churn</strong></h4>
<p>We have the following features in our dataset:</p>
<ul>
<li><p><strong>Employee Age</strong></p></li>
<li><p><strong>Years at Company</strong></p></li>
<li><p><strong>Department</strong></p></li>
<li><p><strong>Salary</strong></p></li>
<li><p><strong>Coffee Consumption (cups per day)</strong></p></li>
</ul>
</section>
<section id="applying-feature-selection" class="level4">
<h4 class="anchored" data-anchor-id="applying-feature-selection"><strong>Applying Feature Selection</strong></h4>
<ul>
<li><p><strong>Step 1: Check Correlation</strong></p>
<ul>
<li>If <strong>Years at Company</strong> and <strong>Employee Age</strong> are highly correlated, we may drop one.</li>
</ul></li>
<li><p><strong>Step 2: Test Feature Importance</strong></p>
<ul>
<li>If <strong>Coffee Consumption</strong> has no relationship with churn, it’s removed.</li>
</ul></li>
<li><p><strong>Step 3: Model-Based Selection</strong></p>
<ul>
<li>Use techniques like <strong>SHAP values, Lasso Regression, or Decision Trees</strong> to rank feature importance.</li>
</ul></li>
</ul>
</section>
<section id="methods-for-feature-selection" class="level4">
<h4 class="anchored" data-anchor-id="methods-for-feature-selection"><strong>Methods for Feature Selection</strong></h4>
<p>✅ <strong>Filter Methods:</strong></p>
<ul>
<li><p><strong>Correlation Analysis:</strong> Drop highly correlated features.</p></li>
<li><p><strong>Chi-Square Test:</strong> Checks relationships between categorical variables.<br>
✅ <strong>Wrapper Methods:</strong></p></li>
<li><p><strong>Recursive Feature Elimination (RFE):</strong> Iteratively removes the weakest features.</p></li>
<li><p><strong>Forward/Backward Selection:</strong> Adds or removes features based on model performance.<br>
✅ <strong>Embedded Methods:</strong></p></li>
<li><p><strong>Lasso (L1) Regression:</strong> Shrinks coefficients of weak predictors to zero.</p></li>
<li><p><strong>Random Forest Feature Importance:</strong> Uses decision trees to rank feature usefulness.</p></li>
</ul>
</section>
</section>
<section id="feature-extraction-transforming-raw-data-into-new-representations" class="level3">
<h3 class="anchored" data-anchor-id="feature-extraction-transforming-raw-data-into-new-representations"><strong>3️⃣ Feature Extraction: Transforming Raw Data into New Representations</strong></h3>
<p>Feature extraction <strong>reduces dimensionality</strong> while retaining meaningful patterns. Instead of <strong>creating</strong> new features, we <strong>compress or transform</strong> existing ones.</p>
<section id="example-text-data-in-sentiment-analysis" class="level4">
<h4 class="anchored" data-anchor-id="example-text-data-in-sentiment-analysis"><strong>📌 Example: Text Data in Sentiment Analysis</strong></h4>
<p>A raw dataset contains customer reviews like:</p>
<ul>
<li><p><strong>“The product was excellent and I will buy again!”</strong></p></li>
<li><p><strong>“Terrible experience. I regret this purchase.”</strong></p></li>
</ul>
<p>Instead of passing raw text into a model, we extract features such as:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th><strong>Extracted Feature</strong></th>
<th><strong>Example</strong></th>
<th><strong>Why It’s Useful?</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>TF-IDF Score</strong></td>
<td><code>0.85</code> (for “excellent”)</td>
<td>Identifies important words.</td>
</tr>
<tr class="even">
<td><strong>Sentiment Score</strong></td>
<td><code>Positive (0.9)</code></td>
<td>Captures the review’s mood.</td>
</tr>
<tr class="odd">
<td><strong>Word Count</strong></td>
<td><code>7</code></td>
<td>Short vs.&nbsp;long reviews may differ.</td>
</tr>
</tbody>
</table>
</section>
<section id="example-principal-component-analysis-pca-for-reducing-dimensions" class="level4">
<h4 class="anchored" data-anchor-id="example-principal-component-analysis-pca-for-reducing-dimensions"><strong>📌 Example: Principal Component Analysis (PCA) for Reducing Dimensions</strong></h4>
<p>Imagine a dataset with <strong>100 highly correlated numerical features</strong>.</p>
<ul>
<li><p>Instead of keeping <strong>all 100</strong>, PCA reduces it to <strong>10 principal components</strong> while preserving most of the information.</p></li>
<li><p>This improves <strong>model performance and speed</strong> without losing predictive power.</p></li>
</ul>
</section>
<section id="common-feature-extraction-techniques" class="level4">
<h4 class="anchored" data-anchor-id="common-feature-extraction-techniques"><strong>Common Feature Extraction Techniques</strong></h4>
<p>✅ <strong>Text Features:</strong> Bag-of-Words, TF-IDF, Word Embeddings (Word2Vec).<br>
✅ <strong>Image Features:</strong> Convolutional Neural Networks (CNNs) extract patterns.<br>
✅ <strong>Dimensionality Reduction:</strong> PCA, t-SNE, UMAP.<br>
✅ <strong>Frequency-Based Features:</strong> Fourier Transforms for time series data.</p>
</section>
</section>
<section id="comparing-the-three-approaches" class="level3">
<h3 class="anchored" data-anchor-id="comparing-the-three-approaches"><strong>Comparing the Three Approaches</strong></h3>
<table class="caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th><strong>Method</strong></th>
<th><strong>Goal</strong></th>
<th><strong>Example</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Feature Creation</strong></td>
<td>Generate new insights</td>
<td>Creating “price per sqft” from “price” and “sqft”.</td>
</tr>
<tr class="even">
<td><strong>Feature Selection</strong></td>
<td>Keep the most relevant features</td>
<td>Removing redundant columns like “age” and “years at company”.</td>
</tr>
<tr class="odd">
<td><strong>Feature Extraction</strong></td>
<td>Compress data into a lower-dimensional space</td>
<td>Using PCA to reduce 100 features into 10.</td>
</tr>
</tbody>
</table>
</section>
<section id="choosing-the-right-approach" class="level3">
<h3 class="anchored" data-anchor-id="choosing-the-right-approach"><strong>Choosing the Right Approach</strong></h3>
<ul>
<li><p><strong>Feature Creation</strong> is <strong>often the most valuable</strong> but requires <strong>domain knowledge</strong>.</p></li>
<li><p><strong>Feature Selection</strong> prevents overfitting and reduces <strong>unnecessary complexity</strong>.</p></li>
<li><p><strong>Feature Extraction</strong> is useful for <strong>high-dimensional data</strong> (e.g., images, text).</p></li>
</ul>
</section>
</section>
<section id="tools-for-feature-engineering-in-r-and-python" class="level2">
<h2 class="anchored" data-anchor-id="tools-for-feature-engineering-in-r-and-python"><strong>Tools for Feature Engineering in R and Python</strong></h2>
<p>Now that we understand <strong>feature creation, selection, and extraction</strong>, let’s explore the <strong>tools</strong> that make feature engineering more efficient in <strong>R and Python</strong>.</p>
<section id="why-use-specialized-feature-engineering-tools" class="level3">
<h3 class="anchored" data-anchor-id="why-use-specialized-feature-engineering-tools"><strong>Why Use Specialized Feature Engineering Tools?</strong></h3>
<p>✔️ <strong>Efficiency:</strong> Automates repetitive transformations.<br>
✔️ <strong>Reproducibility:</strong> Standardized workflows make models easier to maintain.<br>
✔️ <strong>Scalability:</strong> Works well for both small and large datasets.<br>
✔️ <strong>Error Reduction:</strong> Prevents common mistakes like data leakage.</p>
</section>
<section id="feature-engineering-in-r-the-recipes-package-tidymodels" class="level3">
<h3 class="anchored" data-anchor-id="feature-engineering-in-r-the-recipes-package-tidymodels"><strong>1️⃣ Feature Engineering in R: The <code>recipes</code> Package (tidymodels)</strong></h3>
<p>The <strong><code>recipes</code> package</strong> (part of <strong>tidymodels</strong>) provides a structured way to <strong>preprocess and transform</strong> data for modeling.</p>
<section id="key-features-of-recipes" class="level4">
<h4 class="anchored" data-anchor-id="key-features-of-recipes"><strong>Key Features of <code>recipes</code>:</strong></h4>
<p>✔️ <strong>Handles missing values, scaling, encoding, and feature creation</strong><br>
✔️ <strong>Integrates directly into machine learning pipelines</strong><br>
✔️ <strong>Uses a stepwise, declarative approach</strong></p>
</section>
<section id="example-creating-a-feature-engineering-recipe" class="level4">
<h4 class="anchored" data-anchor-id="example-creating-a-feature-engineering-recipe"><strong>Example: Creating a Feature Engineering Recipe</strong></h4>
<p>We will use the <strong>built-in <code>mtcars</code> dataset</strong> to create new features and preprocess the data.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidymodels)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>mtcars <span class="ot">=</span> mtcars <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">cyl =</span> <span class="fu">as.factor</span>(cyl), <span class="at">gear =</span> <span class="fu">as.factor</span>(gear))</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a recipe for preprocessing</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>car_recipe <span class="ot">&lt;-</span> <span class="fu">recipe</span>(mpg <span class="sc">~</span> ., <span class="at">data =</span> mtcars) <span class="sc">%&gt;%</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_log</span>(hp, <span class="at">base =</span> <span class="dv">10</span>) <span class="sc">%&gt;%</span>   <span class="co"># Log transform horsepower</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_normalize</span>(disp, wt) <span class="sc">%&gt;%</span>  <span class="co"># Scale displacement and weight</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_dummy</span>(cyl, gear) <span class="sc">%&gt;%</span>     <span class="co"># One-hot encode categorical variables</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_interact</span>(<span class="at">terms =</span> <span class="sc">~</span> hp<span class="sc">:</span>wt) <span class="co"># Create an interaction term</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Print recipe</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>car_recipe</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co"># # ── Recipe ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="co"># </span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="co"># ── Inputs </span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Number of variables by role</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="co"># outcome:    1</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="co"># predictor: 10</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="co"># </span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="co"># ── Operations </span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="co"># • Log transformation on: hp</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="co"># • Centering and scaling for: disp and wt</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="co"># • Dummy variables from: cyl and gear</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="co"># • Interactions with: hp:wt</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Prepare the recipe and apply it to the data</span></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>prepped_recipe <span class="ot">&lt;-</span> <span class="fu">prep</span>(car_recipe, <span class="at">training =</span> mtcars)</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>transformed_data <span class="ot">&lt;-</span> <span class="fu">bake</span>(prepped_recipe, <span class="at">new_data =</span> mtcars)</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(transformed_data)</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a><span class="co"># # A tibble: 6 × 14</span></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a><span class="co">#      disp    hp  drat       wt  qsec    vs    am  carb   mpg cyl_X6 cyl_X8 gear_X4 gear_X5  hp_x_wt</span></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a><span class="co">#     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;</span></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a><span class="co"># 1 -0.571   2.04  3.9  -0.610    16.5     0     1     4  21        1      0       1       0 -1.25   </span></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a><span class="co"># 2 -0.571   2.04  3.9  -0.350    17.0     0     1     4  21        1      0       1       0 -0.714  </span></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a><span class="co"># 3 -0.990   1.97  3.85 -0.917    18.6     1     1     1  22.8      0      0       1       0 -1.81   </span></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a><span class="co"># 4  0.220   2.04  3.08 -0.00230  19.4     1     0     1  21.4      1      0       0       0 -0.00469</span></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a><span class="co"># 5  1.04    2.24  3.15  0.228    17.0     0     0     2  18.7      0      1       0       0  0.511  </span></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a><span class="co"># 6 -0.0462  2.02  2.76  0.248    20.2     1     0     1  18.1      1      0       0       0  0.501  </span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This <strong>automatically applies transformations</strong> to new data without manual intervention.</p>
</section>
</section>
<section id="feature-engineering-in-python-pandas-and-scikit-learn-pipelines" class="level3">
<h3 class="anchored" data-anchor-id="feature-engineering-in-python-pandas-and-scikit-learn-pipelines"><strong>2️⃣ Feature Engineering in Python: <code>pandas</code> and <code>scikit-learn</code> Pipelines</strong></h3>
<p>Python provides <strong><code>pandas</code> for data manipulation</strong> and <strong><code>scikit-learn</code> for preprocessing</strong> within machine learning workflows.</p>
<section id="key-libraries-for-feature-engineering" class="level4">
<h4 class="anchored" data-anchor-id="key-libraries-for-feature-engineering"><strong>Key Libraries for Feature Engineering:</strong></h4>
<p>✔️ <code>pandas</code> – Basic transformations (scaling, encoding, missing value handling).<br>
✔️ <code>scikit-learn.preprocessing</code> – Standardized transformations.<br>
✔️ <code>featuretools</code> – Automated feature engineering.</p>
</section>
<section id="example-feature-engineering-with-pandas-and-scikit-learn" class="level4">
<h4 class="anchored" data-anchor-id="example-feature-engineering-with-pandas-and-scikit-learn"><strong>Example: Feature Engineering with <code>pandas</code> and <code>scikit-learn</code></strong></h4>
<p>Using the <strong>classic <code>iris</code> dataset</strong>, we apply <strong>scaling, encoding, and feature interactions</strong>.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler, OneHotEncoder</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.pipeline <span class="im">import</span> Pipeline</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.compose <span class="im">import</span> ColumnTransformer</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Load dataset</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv"</span>)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Define numeric and categorical features</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>num_features <span class="op">=</span> [<span class="st">'sepal_length'</span>, <span class="st">'sepal_width'</span>]</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>cat_features <span class="op">=</span> [<span class="st">'species'</span>]</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Define transformations</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>num_transformer <span class="op">=</span> StandardScaler()</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>cat_transformer <span class="op">=</span> OneHotEncoder()</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Combine transformations in a pipeline</span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>preprocessor <span class="op">=</span> ColumnTransformer([</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>    (<span class="st">'num'</span>, num_transformer, num_features),</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>    (<span class="st">'cat'</span>, cat_transformer, cat_features)</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply transformations</span></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>transformed_data <span class="op">=</span> preprocessor.fit_transform(df)</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(transformed_data[:<span class="dv">5</span>])  <span class="co"># Show transformed data</span></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a><span class="co"># [[-0.90068117  1.01900435  1.          0.          0.        ]</span></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a><span class="co">#  [-1.14301691 -0.13197948  1.          0.          0.        ]</span></span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a><span class="co">#  [-1.38535265  0.32841405  1.          0.          0.        ]</span></span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a><span class="co">#  [-1.50652052  0.09821729  1.          0.          0.        ]</span></span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a><span class="co">#  [-1.02184904  1.24920112  1.          0.          0.        ]]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Explanation:</strong><br>
✅ <code>StandardScaler()</code> – Standardizes numerical features.<br>
✅ <code>OneHotEncoder()</code> – Converts categorical features into dummy variables.<br>
✅ <code>ColumnTransformer()</code> – Combines multiple transformations into a single step.</p>
</section>
</section>
<section id="feature-engineering-with-automated-tools" class="level3">
<h3 class="anchored" data-anchor-id="feature-engineering-with-automated-tools"><strong>3️⃣ Feature Engineering with Automated Tools</strong></h3>
<p>For <strong>automating feature creation</strong>, we can use <strong><code>featuretools</code> (Python) and <code>vtreat</code> (R)</strong>.</p>
<section id="auto-feature-engineering-in-r-vtreat" class="level4">
<h4 class="anchored" data-anchor-id="auto-feature-engineering-in-r-vtreat"><strong>📌 Auto Feature Engineering in R: <code>vtreat</code></strong></h4>
<p>The <strong><code>vtreat</code> package</strong> automates common transformations and handles messy data.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(vtreat)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Prepare treatment plan</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>plan <span class="ot">&lt;-</span> <span class="fu">designTreatmentsN</span>(mtcars, <span class="at">varlist =</span> <span class="fu">c</span>(<span class="st">"hp"</span>, <span class="st">"wt"</span>), <span class="at">outcomename =</span> <span class="st">"mpg"</span>)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co"># [1] "vtreat 1.6.5 inspecting inputs Tue Feb 11 22:03:49 2025"</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="co"># [1] "designing treatments Tue Feb 11 22:03:49 2025"</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="co"># [1] " have initial level statistics Tue Feb 11 22:03:49 2025"</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="co"># [1] " scoring treatments Tue Feb 11 22:03:49 2025"</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="co"># [1] "have treatment plan Tue Feb 11 22:03:49 2025"</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply transformations</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>treated_data <span class="ot">&lt;-</span> <span class="fu">prepare</span>(plan, mtcars)</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(treated_data)</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a><span class="co">#    hp    wt  mpg</span></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a><span class="co"># 1 110 2.620 21.0</span></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a><span class="co"># 2 110 2.875 21.0</span></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a><span class="co"># 3  93 2.320 22.8</span></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a><span class="co"># 4 110 3.215 21.4</span></span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a><span class="co"># 5 175 3.440 18.7</span></span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a><span class="co"># 6 105 3.460 18.1</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>✅ <strong>Automatically encodes variables</strong><br>
✅ <strong>Handles missing values and interactions</strong><br>
✅ <strong>Useful for quick feature engineering</strong><br>
</p>
</section>
<section id="auto-feature-engineering-in-python-featuretools" class="level4">
<h4 class="anchored" data-anchor-id="auto-feature-engineering-in-python-featuretools"><strong>📌 Auto Feature Engineering in Python: <code>featuretools</code></strong></h4>
<p>The <strong><code>featuretools</code> package</strong> automatically generates new features from relational datasets.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> featuretools <span class="im">as</span> ft</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Load dataset</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> ft.demo.load_mock_customer()[<span class="st">"customers"</span>]</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Define an entity set</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>es <span class="op">=</span> ft.EntitySet(<span class="bu">id</span><span class="op">=</span><span class="st">"customers"</span>)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>es <span class="op">=</span> es.add_dataframe(dataframe_name<span class="op">=</span><span class="st">"customers"</span>, dataframe<span class="op">=</span>df, index<span class="op">=</span><span class="st">"customer_id"</span>)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Automatically create features</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>features, feature_defs <span class="op">=</span> ft.dfs(entityset<span class="op">=</span>es, target_dataframe_name<span class="op">=</span><span class="st">"customers"</span>)</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(features.head())</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="co">#              zip_code DAY(birthday) DAY(join_date) MONTH(birthday) MONTH(join_date) WEEKDAY(birthday) WEEKDAY(join_date) YEAR(birthday) YEAR(join_date)</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a><span class="co"># customer_id</span></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a><span class="co"># 1               60091            18             17               7                4                 0                  6           1994            2011</span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a><span class="co"># 2               13244            18             15               8                4                 0                  6           1986            2012</span></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a><span class="co"># 3               13244            21             13              11                8                 4                  5           2003            2011</span></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a><span class="co"># 4               60091            15              8               8                4                 1                  4           2006            2011</span></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a><span class="co"># 5               60091            28             17               7                7                 5                  5           1984            2010</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>✅ <strong>Generates new time-based and aggregated features</strong><br>
✅ <strong>Reduces manual effort in feature creation</strong><br>
✅ <strong>Works well for relational datasets (e.g., customer transactions)</strong></p>
</section>
</section>
<section id="feature-selection-tools-in-r-and-python" class="level3">
<h3 class="anchored" data-anchor-id="feature-selection-tools-in-r-and-python"><strong>4️⃣ Feature Selection Tools in R and Python</strong></h3>
<p>Once we create features, we <strong>need to select the most important ones</strong>.</p>
<section id="feature-selection-in-r-vip-glmnet" class="level4">
<h4 class="anchored" data-anchor-id="feature-selection-in-r-vip-glmnet"><strong>📌 Feature Selection in R (<code>vip</code>, <code>glmnet</code>)</strong></h4>
<div class="sourceCode" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(vip)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(glmnet)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Train a Lasso model</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(mpg <span class="sc">~</span> ., mtcars)[, <span class="sc">-</span><span class="dv">1</span>]</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> mtcars<span class="sc">$</span>mpg</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>lasso_model <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(x, y, <span class="at">alpha =</span> <span class="dv">1</span>)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot feature importance</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="fu">vip</span>(lasso_model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSXC</p>
</section>
<section id="feature-selection-in-python-shap-selectkbest" class="level4">
<h4 class="anchored" data-anchor-id="feature-selection-in-python-shap-selectkbest">📌 Feature Selection in Python (<code>shap</code>, <code>SelectKBest</code>)</h4>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> shap</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestRegressor</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Load dataset</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv"</span>)</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Define numeric and categorical features</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>num_features <span class="op">=</span> [<span class="st">'sepal_length'</span>, <span class="st">'sepal_width'</span>]</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>cat_features <span class="op">=</span> [<span class="st">'species'</span>]</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Train a random forest model</span></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> RandomForestRegressor()</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>model.fit(df[num_features], df[<span class="st">"petal_length"</span>])</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate SHAP values</span></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>explainer <span class="op">=</span> shap.Explainer(model)</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>shap_values <span class="op">=</span> explainer(df[num_features])</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize feature importance</span></span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>shap.summary_plot(shap_values, df[num_features])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><img src="ds/posts/images/SHAP.jpeg" class="img-fluid"></p>
</section>
</section>
<section id="comparison-r-vs.-python-for-feature-engineering" class="level3">
<h3 class="anchored" data-anchor-id="comparison-r-vs.-python-for-feature-engineering"><strong>Comparison: R vs.&nbsp;Python for Feature Engineering</strong></h3>
<table class="caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th><strong>Task</strong></th>
<th><strong>R (<code>recipes</code>, <code>vtreat</code>)</strong></th>
<th><strong>Python (<code>pandas</code>, <code>featuretools</code>)</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Preprocessing</strong></td>
<td><code>step_*()</code> in <code>recipes</code></td>
<td><code>ColumnTransformer()</code></td>
</tr>
<tr class="even">
<td><strong>Encoding</strong></td>
<td><code>step_dummy()</code></td>
<td><code>OneHotEncoder()</code></td>
</tr>
<tr class="odd">
<td><strong>Feature Creation</strong></td>
<td><code>mutate()</code>, <code>step_interact()</code></td>
<td><code>featuretools.dfs()</code></td>
</tr>
<tr class="even">
<td><strong>Feature Selection</strong></td>
<td><code>vip::vi()</code>, <code>glmnet</code></td>
<td><code>SelectKBest</code>, <code>shap</code></td>
</tr>
</tbody>
</table>
</section>
<section id="summary-choosing-the-right-tool" class="level3">
<h3 class="anchored" data-anchor-id="summary-choosing-the-right-tool"><strong>📌 Summary: Choosing the Right Tool</strong></h3>
<p>🔹 Use <strong><code>recipes</code> (R) and <code>pandas</code> (Python)</strong> for manual feature engineering.<br>
🔹 Use <strong><code>vtreat</code> (R) and <code>featuretools</code> (Python)</strong> for <strong>automated feature creation</strong>.<br>
🔹 Use <strong><code>vip</code> (R) and <code>shap</code> (Python)</strong> for <strong>feature selection</strong>.</p>
</section>
</section>
<section id="best-practices-common-pitfalls-in-feature-engineering" class="level2">
<h2 class="anchored" data-anchor-id="best-practices-common-pitfalls-in-feature-engineering"><strong>Best Practices &amp; Common Pitfalls in Feature Engineering</strong></h2>
<p>Now that we have explored <strong>feature engineering techniques</strong>, let’s focus on <strong>best practices</strong> and the <strong>mistakes to avoid</strong>. Feature engineering <strong>can make or break a model</strong>, and following good principles helps ensure <strong>better performance and generalizability</strong>.</p>
<section id="best-practices-for-feature-engineering" class="level3">
<h3 class="anchored" data-anchor-id="best-practices-for-feature-engineering"><strong>1️⃣ Best Practices for Feature Engineering</strong></h3>
<section id="use-domain-knowledge-to-guide-feature-creation" class="level4">
<h4 class="anchored" data-anchor-id="use-domain-knowledge-to-guide-feature-creation"><strong>✅ 1. Use Domain Knowledge to Guide Feature Creation</strong></h4>
<p>🔹 The best features <strong>often come from understanding the problem</strong>, not just applying transformations.<br>
🔹 <strong>Ask:</strong> What real-world factors influence the target variable?</p>
<p><strong>📌 Example:</strong><br>
<strong>Predicting customer churn</strong></p>
<ul>
<li>Instead of just using <code>number_of_logins</code>, create <strong>“days since last login”</strong> to better capture customer behavior.</li>
</ul>
</section>
<section id="keep-it-simple-avoid-overcomplicating-features" class="level4">
<h4 class="anchored" data-anchor-id="keep-it-simple-avoid-overcomplicating-features"><strong>✅ 2. Keep It Simple: Avoid Overcomplicating Features</strong></h4>
<p>🔹 More features <strong>do not always mean better models</strong>—avoid unnecessary transformations.<br>
🔹 <strong>Focus on interpretability:</strong> If a feature is too complex to explain, it may introduce <strong>unwanted noise</strong>.</p>
<p><strong>📌 Example:</strong><br>
Instead of adding <strong>5-degree polynomial features</strong>, a simple log transformation may be enough.</p>
</section>
<section id="check-for-redundant-or-highly-correlated-features" class="level4">
<h4 class="anchored" data-anchor-id="check-for-redundant-or-highly-correlated-features"><strong>✅ 3. Check for Redundant or Highly Correlated Features</strong></h4>
<p>🔹 Features that <strong>contain the same information</strong> may <strong>confuse models</strong> and <strong>increase computation time</strong>.<br>
🔹 <strong>How to detect?</strong><br>
✔ <strong>Correlation matrix</strong> (Pearson/Spearman).<br>
✔ <strong>Variance Inflation Factor (VIF)</strong> (R).<br>
✔ <strong>SHAP values or feature importance plots</strong> (Python).</p>
<p><strong>📌 Example:</strong></p>
<ul>
<li><code>Total number of rooms</code> and <code>house size in square feet</code> may be <strong>highly correlated</strong>—dropping one avoids redundancy.</li>
</ul>
</section>
<section id="normalize-or-standardize-when-needed" class="level4">
<h4 class="anchored" data-anchor-id="normalize-or-standardize-when-needed"><strong>✅ 4. Normalize or Standardize When Needed</strong></h4>
<p>🔹 Some models (e.g., KNN, linear regression, neural networks) <strong>perform poorly with unscaled data</strong>.<br>
🔹 Scaling ensures <strong>each feature contributes equally</strong> to model training.</p>
<p><strong>📌 Example:</strong></p>
<ul>
<li><p><code>House price</code> ranges from <strong>$50,000 to $2,000,000</strong>.</p></li>
<li><p><code>Bedrooms count</code> ranges from <strong>1 to 5</strong>.<br>
<strong>Solution:</strong> Standardizing prices helps prevent models from giving too much weight to large numerical values.</p></li>
</ul>
</section>
<section id="use-cross-validation-when-testing-features" class="level4">
<h4 class="anchored" data-anchor-id="use-cross-validation-when-testing-features"><strong>✅ 5. Use Cross-Validation When Testing Features</strong></h4>
<p>🔹 A feature may improve accuracy <strong>on the training set</strong> but <strong>not generalize</strong> to new data.<br>
🔹 <strong>Use k-fold cross-validation</strong> to check how new features impact performance.</p>
<p><strong>📌 Example:</strong><br>
A feature like <strong>“customer average spending in the last month”</strong> may work well during training but fail on customers who <strong>just signed up</strong>.</p>
</section>
</section>
<section id="common-pitfalls-to-avoid" class="level3">
<h3 class="anchored" data-anchor-id="common-pitfalls-to-avoid"><strong>2️⃣ Common Pitfalls to Avoid</strong></h3>
<section id="feature-leakage-using-future-information" class="level4">
<h4 class="anchored" data-anchor-id="feature-leakage-using-future-information"><strong>❌ 1. Feature Leakage (Using Future Information)</strong></h4>
<p>🔹 <strong>Using data that wouldn’t be available at prediction time</strong> leads to unrealistically high accuracy.<br>
🔹 Always ensure <strong>feature values are only from past events</strong>.</p>
<p><strong>📌 Example of Feature Leakage:</strong><br>
<strong>Predicting loan default</strong></p>
<ul>
<li>A feature like <strong>“loan repayment status”</strong> is not valid, because it’s already telling the model if the loan was paid or not.</li>
</ul>
</section>
<section id="overfitting-to-training-data" class="level4">
<h4 class="anchored" data-anchor-id="overfitting-to-training-data"><strong>❌ 2. Overfitting to Training Data</strong></h4>
<p>🔹 Over-engineering features may <strong>memorize patterns</strong> in the training data instead of finding <strong>general trends</strong>.<br>
🔹 Avoid <strong>too many polynomial features or irrelevant categorical encodings</strong>.</p>
<p><strong>📌 Example:</strong><br>
A model that includes <strong>zip code</strong> as a feature may perform well <strong>only on seen locations</strong> but fail when predicting in new areas.</p>
</section>
<section id="encoding-high-cardinality-categorical-variables-poorly" class="level4">
<h4 class="anchored" data-anchor-id="encoding-high-cardinality-categorical-variables-poorly"><strong>❌ 3. Encoding High-Cardinality Categorical Variables Poorly</strong></h4>
<p>🔹 <strong>One-hot encoding thousands of categories</strong> results in <strong>huge feature matrices</strong>.<br>
🔹 Instead, use:<br>
✔ <strong>Target encoding</strong> (replacing category with its mean target value).<br>
✔ <strong>Embedding layers</strong> (deep learning).</p>
<p><strong>📌 Example:</strong><br>
A dataset with <strong>thousands of unique products</strong>—using one-hot encoding <strong>explodes feature space</strong>.</p>
</section>
<section id="ignoring-outliers-before-engineering-features" class="level4">
<h4 class="anchored" data-anchor-id="ignoring-outliers-before-engineering-features"><strong>❌ 4. Ignoring Outliers Before Engineering Features</strong></h4>
<p>🔹 Some transformations (e.g., <strong>log scaling</strong>) <strong>fail</strong> when outliers exist.<br>
🔹 <strong>Solution:</strong> Winsorize or use <strong>robust scaling</strong> to handle extreme values.</p>
<p><strong>📌 Example:</strong></p>
<ul>
<li><p>Income dataset where 95% of people earn <strong>$50,000-$100,000</strong>, but one person earns <strong>$10M</strong>.</p></li>
<li><p>Without handling, the model <strong>may become biased</strong> towards extreme values.</p></li>
</ul>
</section>
</section>
<section id="summary-best-practices-pitfalls" class="level3">
<h3 class="anchored" data-anchor-id="summary-best-practices-pitfalls"><strong>📌 Summary: Best Practices &amp; Pitfalls</strong></h3>
<table class="caption-top table">
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<thead>
<tr class="header">
<th>✅ Best Practices</th>
<th>❌ Pitfalls to Avoid</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Use <strong>domain knowledge</strong> for better feature creation</td>
<td><strong>Feature leakage</strong>—using future data</td>
</tr>
<tr class="even">
<td><strong>Keep it simple</strong>—avoid over-engineering</td>
<td><strong>Overfitting</strong>—too many complex transformations</td>
</tr>
<tr class="odd">
<td><strong>Check correlations</strong>—remove redundant features</td>
<td>Poor encoding of <strong>high-cardinality categorical variables</strong></td>
</tr>
<tr class="even">
<td><strong>Scale features</strong> when needed</td>
<td><strong>Ignoring outliers</strong> before transformations</td>
</tr>
<tr class="odd">
<td><strong>Validate with cross-validation</strong></td>
<td>Using <strong>irrelevant or noisy features</strong></td>
</tr>
</tbody>
</table>
<p>✅ <strong>By following these principles, we ensure our models are trained on reliable, useful features</strong>—leading to better performance in real-world scenarios.</p>
</section>
</section>
<section id="best-practices-common-pitfalls-in-feature-engineering-1" class="level1">
<h1><strong>Best Practices &amp; Common Pitfalls in Feature Engineering</strong></h1>
<p>Now that we have explored <strong>feature engineering techniques</strong>, let’s focus on <strong>best practices</strong> and the <strong>mistakes to avoid</strong>. Feature engineering <strong>can make or break a model</strong>, and following good principles helps ensure <strong>better performance and generalizability</strong>.</p>
<section id="best-practices-for-feature-engineering-1" class="level2">
<h2 class="anchored" data-anchor-id="best-practices-for-feature-engineering-1"><strong>1️⃣ Best Practices for Feature Engineering</strong></h2>
<section id="use-domain-knowledge-to-guide-feature-creation-1" class="level3">
<h3 class="anchored" data-anchor-id="use-domain-knowledge-to-guide-feature-creation-1"><strong>✅ 1. Use Domain Knowledge to Guide Feature Creation</strong></h3>
<p>🔹 The best features <strong>often come from understanding the problem</strong>, not just applying transformations.<br>
🔹 <strong>Ask:</strong> What real-world factors influence the target variable?</p>
<p><strong>📌 Example:</strong><br>
<strong>Predicting customer churn</strong></p>
<ul>
<li>Instead of just using <code>number_of_logins</code>, create <strong>“days since last login”</strong> to better capture customer behavior.</li>
</ul>
</section>
<section id="keep-it-simple-avoid-overcomplicating-features-1" class="level3">
<h3 class="anchored" data-anchor-id="keep-it-simple-avoid-overcomplicating-features-1"><strong>✅ 2. Keep It Simple: Avoid Overcomplicating Features</strong></h3>
<p>🔹 More features <strong>do not always mean better models</strong>—avoid unnecessary transformations.<br>
🔹 <strong>Focus on interpretability:</strong> If a feature is too complex to explain, it may introduce <strong>unwanted noise</strong>.</p>
<p><strong>📌 Example:</strong><br>
Instead of adding <strong>5-degree polynomial features</strong>, a simple log transformation may be enough.</p>
</section>
<section id="check-for-redundant-or-highly-correlated-features-1" class="level3">
<h3 class="anchored" data-anchor-id="check-for-redundant-or-highly-correlated-features-1"><strong>✅ 3. Check for Redundant or Highly Correlated Features</strong></h3>
<p>🔹 Features that <strong>contain the same information</strong> may <strong>confuse models</strong> and <strong>increase computation time</strong>.<br>
🔹 <strong>How to detect?</strong><br>
✔ <strong>Correlation matrix</strong> (Pearson/Spearman).<br>
✔ <strong>Variance Inflation Factor (VIF)</strong> (R).<br>
✔ <strong>SHAP values or feature importance plots</strong> (Python).</p>
<p><strong>📌 Example:</strong></p>
<ul>
<li><code>Total number of rooms</code> and <code>house size in square feet</code> may be <strong>highly correlated</strong>—dropping one avoids redundancy.</li>
</ul>
</section>
<section id="normalize-or-standardize-when-needed-1" class="level3">
<h3 class="anchored" data-anchor-id="normalize-or-standardize-when-needed-1"><strong>✅ 4. Normalize or Standardize When Needed</strong></h3>
<p>🔹 Some models (e.g., KNN, linear regression, neural networks) <strong>perform poorly with unscaled data</strong>.<br>
🔹 Scaling ensures <strong>each feature contributes equally</strong> to model training.</p>
<p><strong>📌 Example:</strong></p>
<ul>
<li><p><code>House price</code> ranges from <strong>$50,000 to $2,000,000</strong>.</p></li>
<li><p><code>Bedrooms count</code> ranges from <strong>1 to 5</strong>.<br>
<strong>Solution:</strong> Standardizing prices helps prevent models from giving too much weight to large numerical values.</p></li>
</ul>
</section>
<section id="use-cross-validation-when-testing-features-1" class="level3">
<h3 class="anchored" data-anchor-id="use-cross-validation-when-testing-features-1"><strong>✅ 5. Use Cross-Validation When Testing Features</strong></h3>
<p>🔹 A feature may improve accuracy <strong>on the training set</strong> but <strong>not generalize</strong> to new data.<br>
🔹 <strong>Use k-fold cross-validation</strong> to check how new features impact performance.</p>
<p><strong>📌 Example:</strong><br>
A feature like <strong>“customer average spending in the last month”</strong> may work well during training but fail on customers who <strong>just signed up</strong>.</p>
</section>
</section>
<section id="common-pitfalls-to-avoid-1" class="level2">
<h2 class="anchored" data-anchor-id="common-pitfalls-to-avoid-1"><strong>2️⃣ Common Pitfalls to Avoid</strong></h2>
<section id="feature-leakage-using-future-information-1" class="level3">
<h3 class="anchored" data-anchor-id="feature-leakage-using-future-information-1"><strong>❌ 1. Feature Leakage (Using Future Information)</strong></h3>
<p>🔹 <strong>Using data that wouldn’t be available at prediction time</strong> leads to unrealistically high accuracy.<br>
🔹 Always ensure <strong>feature values are only from past events</strong>.</p>
<p><strong>📌 Example of Feature Leakage:</strong><br>
<strong>Predicting loan default</strong></p>
<ul>
<li>A feature like <strong>“loan repayment status”</strong> is not valid, because it’s already telling the model if the loan was paid or not.</li>
</ul>
</section>
<section id="overfitting-to-training-data-1" class="level3">
<h3 class="anchored" data-anchor-id="overfitting-to-training-data-1"><strong>❌ 2. Overfitting to Training Data</strong></h3>
<p>🔹 Over-engineering features may <strong>memorize patterns</strong> in the training data instead of finding <strong>general trends</strong>.<br>
🔹 Avoid <strong>too many polynomial features or irrelevant categorical encodings</strong>.</p>
<p><strong>📌 Example:</strong><br>
A model that includes <strong>zip code</strong> as a feature may perform well <strong>only on seen locations</strong> but fail when predicting in new areas.</p>
</section>
<section id="encoding-high-cardinality-categorical-variables-poorly-1" class="level3">
<h3 class="anchored" data-anchor-id="encoding-high-cardinality-categorical-variables-poorly-1"><strong>❌ 3. Encoding High-Cardinality Categorical Variables Poorly</strong></h3>
<p>🔹 <strong>One-hot encoding thousands of categories</strong> results in <strong>huge feature matrices</strong>.<br>
🔹 Instead, use:<br>
✔ <strong>Target encoding</strong> (replacing category with its mean target value).<br>
✔ <strong>Embedding layers</strong> (deep learning).</p>
<p><strong>📌 Example:</strong><br>
A dataset with <strong>thousands of unique products</strong>—using one-hot encoding <strong>explodes feature space</strong>.</p>
</section>
<section id="ignoring-outliers-before-engineering-features-1" class="level3">
<h3 class="anchored" data-anchor-id="ignoring-outliers-before-engineering-features-1"><strong>❌ 4. Ignoring Outliers Before Engineering Features</strong></h3>
<p>🔹 Some transformations (e.g., <strong>log scaling</strong>) <strong>fail</strong> when outliers exist.<br>
🔹 <strong>Solution:</strong> Winsorize or use <strong>robust scaling</strong> to handle extreme values.</p>
<p><strong>📌 Example:</strong></p>
<ul>
<li><p>Income dataset where 95% of people earn <strong>$50,000-$100,000</strong>, but one person earns <strong>$10M</strong>.</p></li>
<li><p>Without handling, the model <strong>may become biased</strong> towards extreme values.</p></li>
</ul>
</section>
</section>
<section id="summary-best-practices-pitfalls-1" class="level2">
<h2 class="anchored" data-anchor-id="summary-best-practices-pitfalls-1"><strong>📌 Summary: Best Practices &amp; Pitfalls</strong></h2>
<table class="caption-top table">
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<thead>
<tr class="header">
<th>✅ Best Practices</th>
<th>❌ Pitfalls to Avoid</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Use <strong>domain knowledge</strong> for better feature creation</td>
<td><strong>Feature leakage</strong>—using future data</td>
</tr>
<tr class="even">
<td><strong>Keep it simple</strong>—avoid over-engineering</td>
<td><strong>Overfitting</strong>—too many complex transformations</td>
</tr>
<tr class="odd">
<td><strong>Check correlations</strong>—remove redundant features</td>
<td>Poor encoding of <strong>high-cardinality categorical variables</strong></td>
</tr>
<tr class="even">
<td><strong>Scale features</strong> when needed</td>
<td><strong>Ignoring outliers</strong> before transformations</td>
</tr>
<tr class="odd">
<td><strong>Validate with cross-validation</strong></td>
<td>Using <strong>irrelevant or noisy features</strong></td>
</tr>
</tbody>
</table>
<p>✅ <strong>By following these principles, we ensure our models are trained on reliable, useful features</strong>—leading to better performance in real-world scenarios.</p>
</section>
</section>
<section id="conclusion-next-steps" class="level1">
<h1><strong>Conclusion &amp; Next Steps</strong></h1>
<p>Feature engineering is <strong>both an art and a science</strong>—it bridges the gap between raw data and meaningful insights, ultimately determining whether a model performs <strong>well or fails to generalize</strong>. Throughout this article, we’ve explored the <strong>foundations of feature engineering</strong>, covering the essentials of <strong>feature creation, selection, and extraction</strong>, along with best practices to avoid common pitfalls.</p>
<section id="key-takeaways" class="level3">
<h3 class="anchored" data-anchor-id="key-takeaways"><strong>🔹 Key Takeaways</strong></h3>
<p>✅ <strong>Good features matter more than complex models</strong>—A well-engineered dataset can outperform advanced algorithms trained on poorly processed data.<br>
✅ <strong>Preprocessing is a critical first step</strong>—Handling missing values, scaling numerical features, and encoding categorical variables set the stage for effective transformations.<br>
✅ <strong>Feature engineering techniques vary</strong>—From polynomial transformations and interaction terms to log scaling and binning, different problems require different approaches.<br>
✅ <strong>Domain knowledge is key</strong>—Understanding the context behind the data leads to more <strong>meaningful, interpretable</strong> features.<br>
✅ <strong>Avoid feature leakage and overfitting</strong>—Ensuring that features reflect only past information and generalize well to unseen data is crucial for real-world applications.</p>
</section>
<section id="whats-next" class="level3">
<h3 class="anchored" data-anchor-id="whats-next"><strong>🚀 What’s Next?</strong></h3>
<p>This article is just the <strong>beginning</strong> of a larger <strong>Feature Engineering Series</strong>! In the upcoming articles, we’ll take <strong>deep dives into specific techniques</strong>, such as:<br>
📌 <strong>Transforming Categorical Variables</strong>—Beyond one-hot encoding, exploring techniques like target encoding and embeddings.<br>
📌 <strong>Feature Engineering for Time Series Data</strong>—Creating lag features, moving averages, and handling seasonality.<br>
📌 <strong>Text-Based Feature Engineering</strong>—Extracting insights using TF-IDF, word embeddings, and sentiment scores.<br>
📌 <strong>Automated Feature Engineering</strong>—Using tools like <code>featuretools</code> (Python) and <code>vtreat</code> (R) to generate features at scale.</p>
<p>By applying these methods, you’ll gain the <strong>practical knowledge</strong> needed to craft <strong>better features, build stronger models, and make data-driven decisions more effectively</strong>.</p>
<p>🔹 <strong>Next up: “Mastering the Spice Rack – Transforming Categorical Features”</strong>—where we explore the <strong>best techniques for handling categorical data</strong> to maximize model performance!</p>
<p>💡 <strong>Let’s discuss!</strong> What’s your go-to feature engineering trick? Drop your thoughts below! ⬇⬇⬇</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = true;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>For everybody, not to be fooled by numbers</p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>