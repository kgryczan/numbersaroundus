[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Numbers Around Us",
    "section": "",
    "text": "Welcome to Numbers Around Us, your go-to resource for mastering analytics programming, business intelligence tools, and the art of data-driven thinking. Whether you’re diving into R, Python, or SQL, exploring Tableau and Power BI, or rethinking how you approach data projects, you’ll find practical insights, tools, and solutions here."
  },
  {
    "objectID": "index.html#what-you-will-find",
    "href": "index.html#what-you-will-find",
    "title": "Numbers Around Us",
    "section": "What You Will Find",
    "text": "What You Will Find\n\n1. Analytics Programming (R, SQL, Python)\nUnlock the potential of your data with step-by-step tutorials, advanced tips, and innovative solutions. Our resources cover:\n\nR: From data wrangling to advanced visualizations, learn how to harness the power of R.\nPython: Explore its versatility, from automation to machine learning.\nSQL: Master the language of databases for efficient querying and analysis.\n\n\n\n2. Business Intelligence (Tableau and Power BI)\nVisualize and communicate your insights effectively. Learn how to:\n\nCreate stunning dashboards and reports in Tableau.\nBuild dynamic, actionable visuals in Power BI.\nIntegrate BI tools into your analytics workflow.\n\n\n\n3. Data Philosophy\nAnalytics is more than tools—it’s a mindset. In this section, we explore:\n\nData management: Best practices for clean and reliable data.\nProject planning: Strategies for successful analytics projects.\nEthics and governance: Ensuring responsible use of data."
  },
  {
    "objectID": "index.html#solve-challenges-gain-insights",
    "href": "index.html#solve-challenges-gain-insights",
    "title": "Numbers Around Us",
    "section": "Solve Challenges, Gain Insights",
    "text": "Solve Challenges, Gain Insights\nOne of our standout features is our Challenge Solutions section. Here, we tackle real-world analytics challenges from LinkedIn, offering:\n\nDetailed solutions in R and Python.\nInsights into problem-solving techniques.\nTips for applying these skills to your own work."
  },
  {
    "objectID": "index.html#why-choose-numbers-around-us",
    "href": "index.html#why-choose-numbers-around-us",
    "title": "Numbers Around Us",
    "section": "Why Choose Numbers Around Us?",
    "text": "Why Choose Numbers Around Us?\nWe combine technical expertise with a passion for data-driven storytelling. Whether you’re a beginner looking for guidance or an experienced analyst refining your craft, our content is designed to inspire and empower you.\n\nStart Your Journey\nDive into our latest articles, explore the challenge solutions, or check out the Data Philosophy section to rethink how you work with data. Let’s build a smarter, more insightful data world—together."
  },
  {
    "objectID": "ds/posts/2024-11-03_Data-at-Your-Fingertips--Crafting-Interactive-Tables-in-R-b4ae5ca7a71d.html",
    "href": "ds/posts/2024-11-03_Data-at-Your-Fingertips--Crafting-Interactive-Tables-in-R-b4ae5ca7a71d.html",
    "title": "Data at Your Fingertips: Crafting Interactive Tables in R",
    "section": "",
    "text": "Why Interactive Tables Matter\n\n\n\nImage\n\n\nWhen people think of tables, they often picture static rows and columns, a no-frills way to present data. But interactive tables are a whole different story! These tables let users engage with the data directly, exploring it in ways that feel almost hands-on. Adding features like sorting, filtering, and searching transforms a simple table into a dynamic tool where readers can make their own discoveries.\nSo, why use interactive tables? Imagine you’re building a report for a large dataset. Instead of bogging down readers with endless rows and columns, you can let them filter out what they need or sort by their specific interests. For data professionals, this level of flexibility is invaluable — it allows anyone to find exactly what they’re looking for, without having to navigate through a mountain of data.\nIn this article, we’ll explore how R can help us create interactive tables across different contexts: from dashboards and reports to interactive web apps. With R’s powerful packages, like DT and reactable, you can bring tables to life with just a few lines of code. Let’s get started with the basics and work our way to some advanced features!\n\n\nGetting Started with DT: DataTables in R\nWhen it comes to building interactive tables in R, the DT package is a fantastic place to start. Developed as an interface to JavaScript’s DataTables library, DT enables you to quickly add interactive features to your tables without complex coding. This means you can create tables that support sorting, filtering, and navigating large datasets—all with minimal setup. Whether you’re designing a report, building a dashboard, or creating a web app, DT offers functionality that transforms static tables into dynamic data exploration tools.\nOne of the main appeals of DT is its ease of use. To get started, simply pass your dataset to DT::datatable(), and with just that, you’ll have a table that:\n\nSorts each column by clicking the column header, allowing users to view data in their preferred order.\nSearches through all table content with a convenient search box above the table, so users can instantly locate specific information.\nPaginates large datasets, displaying a specified number of rows per page, making it easy to navigate through hundreds or thousands of rows without scrolling endlessly.\n\nTo see this in action, here’s a basic example using R’s built-in iris dataset. In this example, we’re creating a table that displays five rows per page:\nlibrary(DT)\n# Creating a basic interactive table\ndatatable(iris, options = list(pageLength = 5, autoWidth = TRUE))\n\n\n\nImage\n\n\nIn this code:\n\npageLength = 5 sets the number of rows visible at once to five, which is especially useful for datasets with many rows. This setting allows users to page through rows smoothly without feeling overwhelmed by the data.\nautoWidth = TRUE automatically adjusts column widths based on the data content, ensuring your table looks clean and well-organized.\n\nThis single line of code provides a fully interactive table that you can integrate into HTML-based documents, Shiny apps, or R Markdown reports. The table is easy to navigate, visually appealing, and functional. With DT, you can create a data table that allows users to explore your dataset directly and efficiently, all without having to build custom interfaces or write extensive JavaScript.\n\n\nCustomizing Tables in DT: More Control and Style\nThe basic setup for DT tables is functional and simple, but if you want your tables to truly shine, DT offers a wealth of customization options. These let you adjust not only the appearance but also the interactivity of your tables, giving users more control over how they explore the data. Customization can be especially useful for tailored reports or web-based dashboards where readers may have specific needs, such as filtering by certain values or only viewing select columns.\n\nAdding Individual Column Filters\nIn many cases, a global search box is helpful, but if users need to filter specific columns independently, individual column filters make a big difference. For example, imagine you’re working with a dataset like iris, where users might want to see only rows with Sepal.Length above 5 or filter Species to show only specific categories. With DT, you can easily add filters for each column.\nHere’s how to enable individual column filters:\ndatatable(iris, filter = \"top\", options = list(pageLength = 5))\n\n\n\nImage\n\n\nBy setting filter = \"top\", DT automatically places a filter box at the top of each column, giving users the flexibility to search for values independently. This feature can be particularly useful when working with larger datasets where users need to narrow down rows by specific values or ranges, allowing them to:\n\nFilter categorical data: Users can select one or more categories (e.g., filtering Species for “setosa” or “versicolor”).\nFilter numeric data: Users can set numeric filters (e.g., showing only rows where Sepal.Width is greater than 3).\nSearch by partial matches: This can be helpful when columns contain text or unique identifiers.\n\nThese individual filters empower readers to explore data without cluttering the main table view. Instead of having to scan through all rows, users can focus on the exact data points they need, making for a highly personalized viewing experience.\n\n\nAdjusting Page Length and Table Layout\nWhen you’re working with large datasets, adjusting the page length — or the number of rows visible at once — improves readability and reduces scrolling. While displaying 5 rows per page works for smaller tables, larger datasets often benefit from showing more rows per page (e.g., 10 or 15), allowing users to view more data at a glance without extensive paging. You can set the page length to fit the specific needs of your project.\nThe layout, including table width and column visibility, can also affect readability. DT gives you control over layout settings through the dom parameter. This parameter specifies which elements (buttons, filters, search bars, etc.) are visible. Here’s how to adjust both page length and layout options:\ndatatable(\n  iris, \n  extensions = 'Buttons', # Enable the Buttons extension\n  options = list(\n    pageLength = 10,\n    dom = 'Bfrtip',\n    autoWidth = TRUE,\n    buttons = c('copy', 'csv', 'excel', 'pdf', 'print') # Specify the types of buttons\n  )\n)\n\n\n\nImage\n\n\nIn this example:\n\npageLength = 10 displays 10 rows at a time, making it easier to view more data per page.\ndom = 'Bfrtip' customizes the toolbar layout. Each letter represents a different component:\n\nB: Buttons (for exporting or downloading data)\nf: Filter (the search bar)\nr: Processing indicator (useful for larger tables)\nt: Table itself\np: Pagination (for navigating pages)\n\n\nThis dom setting lets you control exactly which table features appear on the page, simplifying the view for readers. For example, if you’re using the table in a Shiny app and only need the table and pagination features, you could set dom = 'tp', which hides the search bar and toolbar to give a more streamlined look.\n\nautoWidth = TRUE automatically adjusts column widths to fit the content, which helps maintain a clean, proportional look without columns being too cramped or stretched.\nbuttons = c('copy', 'csv', 'excel', 'pdf', 'print'): This argument specifies which export options to show in the toolbar.\n\n\n\nAdding Styling and Conditional Formatting\nIn addition to adjusting layout, DT allows you to style your tables to improve readability and focus attention on key values. For example, you may want to highlight high values in a “price” column, or use color to differentiate specific categories. DT supports conditional formatting using the formatStyle() function, which allows you to apply styles to individual cells based on conditions.\nHere’s how you could apply conditional formatting to highlight values in the Sepal.Length column that exceed a certain threshold:\ndatatable(iris, options = list(pageLength = 10)) %&gt;%\n  formatStyle(\n    'Sepal.Length',\n    backgroundColor = styleInterval(5.5, c('white', 'lightgreen'))\n  )\n\n\n\nImage\n\n\nIn this example:\n\nstyleInterval() sets intervals for conditional formatting. Here, all values in Sepal.Length above 5.5 will have a light green background, while values below remain white.\nThis type of formatting is particularly useful when you want to make certain data stand out. For instance, highlighting high or low values in financial data, differentiating categories by color, or adding visual cues for outliers.\n\nConditional formatting and custom styling give your tables an added layer of professionalism, especially useful in reports or presentations where certain data points need emphasis.\nThese customization options within DT allow you to tailor the look, feel, and functionality of your tables, ensuring that readers can navigate and interpret the data effectively. Whether you’re fine-tuning pagination, adding individual filters, or applying styling for impact, DT offers plenty of ways to enhance both usability and aesthetics.\n\n\n\nreactable: Creating Stylish Interactive Tables\nWhile DT is a fantastic choice for basic interactive tables, reactable takes customization to a new level, allowing for highly flexible and visually polished tables. Built on React, reactable provides rich interactivity and seamless customization, including column-specific settings, themes, and row expansions. If you’re creating tables for dashboards, reports, or any application that demands a bit more styling, reactable is a powerful tool to have.\nWith reactable, you can go beyond standard data displays by adding custom formats, colors, and even mini visualizations. Let’s start by creating a basic reactable table with the iris dataset and then dive into some customization options.\n\nCreating a Basic reactable Table\nHere’s a quick example of a basic interactive table using reactable:\nlibrary(reactable)\n\n# Basic reactable table with iris dataset\nreactable(iris, columns = list(\n  Sepal.Length = colDef(name = \"Sepal Length\"),\n  Sepal.Width = colDef(name = \"Sepal Width\"),\n  Petal.Length = colDef(name = \"Petal Length\"),\n  Petal.Width = colDef(name = \"Petal Width\"),\n  Species = colDef(name = \"Species\")\n))\n\n\n\nImage\n\n\nIn this code:\n\ncolDef() customizes each column with more readable names.\nThis setup gives you a clean, sortable table that lets users click column headers to sort data. The columns are also resizable by default, providing flexibility for users to adjust the view.\n\n\n\nAdvanced Customization with colDef\nOne of the best features of reactable is the ability to define column-specific settings through colDef(), where you can set custom formatting, alignment, background colors, and even icons based on cell values. This makes it easy to highlight certain data points or apply thematic styling to fit your application’s design.\nLet’s add a few customizations to the reactable table:\n\nWe’ll style Species cells to include icons.\nFormat Sepal.Length to two decimal places with color indicators.\n\nreactable(iris, columns = list(\n  Sepal.Length = colDef(\n    name = \"Sepal Length\",\n    align = \"center\",\n    cell = function(value) {\n      if (value &gt; 5) paste0(\"🌱 \", round(value, 2)) else round(value, 2)\n    },\n    style = function(value) {\n      if (value &gt; 5) list(color = \"green\") else list(color = \"black\")\n    }\n  ),\n  Species = colDef(\n    cell = function(value) {\n      if (value == \"setosa\") \"🌸 Setosa\" else value\n    },\n    align = \"center\"\n  )\n))\n\n\n\nImage\n\n\nIn this code:\n\nCustom Cell Content: In Sepal.Length, cells with values greater than 5 are prefixed with a small plant icon 🌱 and styled in green.\nIcons in Text Cells: For Species, we add a flower icon 🌸 for “setosa” values, making it more visually distinct.\nAlignment: By setting align = \"center\", we ensure that the values appear centered in each cell, creating a cleaner look.\n\n\n\nApplying Themes and Styling\nreactable also comes with several built-in themes, or you can create your own custom styles using CSS to match any design you’re working with. Here’s an example of how to apply the “compact” theme with striped rows, which gives your table a sleek, modern look:\nreactable(\n  iris[1:30, ],\n  searchable = TRUE,\n  striped = TRUE,\n  highlight = TRUE,\n  bordered = TRUE,\n  theme = reactableTheme(\n    borderColor = \"#dfe2e5\",\n    stripedColor = \"#f6f8fa\",\n    highlightColor = \"#fff000\",\n    cellPadding = \"8px 12px\",\n    style = list(fontFamily = \"-apple-system, BlinkMacSystemFont, Segoe UI, Helvetica, Arial, sans-serif\"),\n    searchInputStyle = list(width = \"100%\")\n  )\n)\n\n\n\nImage\n\n\nThis example adds:\n\nStriped Rows: Alternating row colors make it easier to read across large datasets.\nHighlighting: Selected rows are highlighted to improve navigation.\nCompact Layout: Reduces padding for a more compressed view, ideal for tables with many rows.\n\nWith reactable, you have flexibility over everything from themes and icons to row expandability. The package is particularly suited for dashboards, apps, and reports where style and interactivity are both high priorities.\n\n\n\nIntegrating Interactive Tables in Shiny\nInteractive tables become even more powerful in the context of Shiny apps, where they can respond to user inputs in real-time. By integrating tables from DT or reactable into a Shiny app, you can allow users to filter, sort, and explore data while responding to additional controls, like sliders or dropdowns. This flexibility makes Shiny ideal for creating dashboards, reports, or custom data exploration tools.\n\nCreating a Basic Shiny App with DT\nLet’s start with a simple Shiny app that uses DT to display an interactive table. In this example, we’ll use a slider to allow users to filter rows based on Sepal Length from the iris dataset:\nlibrary(shiny)\nlibrary(DT)\n\n# Define the UI\nui &lt;- fluidPage(\n  titlePanel(\"Iris Dataset Interactive Table\"),\n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\"sepal\", \"Filter by Sepal Length:\",\n                  min = min(iris$Sepal.Length), max = max(iris$Sepal.Length), \n                  value = c(min(iris$Sepal.Length), max(iris$Sepal.Length)))\n    ),\n    mainPanel(\n      DTOutput(\"table\")\n    )\n  )\n)\n\n# Define the server logic\nserver &lt;- function(input, output) {\n  output$table &lt;- renderDT({\n    # Filter the data based on slider input\n    filtered_data &lt;- iris[iris$Sepal.Length &gt;= input$sepal[1] & iris$Sepal.Length &lt;= input$sepal[2], ]\n    datatable(filtered_data, options = list(pageLength = 5))\n  })\n}\n\n# Run the application \nshinyApp(ui = ui, server = server)\n\n\n\nImage\n\n\nIn this example:\n\nSlider Input: The sliderInput in the UI allows users to filter the table by Sepal Length values. The slider is set to the range of Sepal Length in the dataset, so users can choose any range within those values.\nFiltering Data in Server: In the server function, we filter iris based on the slider values and then render the filtered table using renderDT().\nTable Output: DTOutput displays the filtered table in the main panel, showing 5 rows per page.\n\nThis basic Shiny app provides users with control over what they see, allowing them to explore the dataset interactively with the filter.\n\n\nUsing reactable for Customization in Shiny\nIf you want even more control over the table’s appearance and functionality, you can use reactable in your Shiny app. Here’s an example of a similar Shiny app with reactable, where we add an input for selecting specific Species to filter by:\nlibrary(shiny)\nlibrary(reactable)\n\n# Define the UI\nui &lt;- fluidPage(\n  titlePanel(\"Interactive Table with reactable\"),\n  sidebarLayout(\n    sidebarPanel(\n      selectInput(\"species\", \"Select Species:\", \n                  choices = c(\"All\", unique(as.character(iris$Species)))),\n      sliderInput(\"sepal\", \"Filter by Sepal Length:\",\n                  min = min(iris$Sepal.Length), max = max(iris$Sepal.Length), \n                  value = c(min(iris$Sepal.Length), max(iris$Sepal.Length)))\n    ),\n    mainPanel(\n      reactableOutput(\"reactable_table\")\n    )\n  )\n)\n\n# Define the server logic\nserver &lt;- function(input, output) {\n  output$reactable_table &lt;- renderReactable({\n    # Filter data based on user inputs\n    filtered_data &lt;- iris[iris$Sepal.Length &gt;= input$sepal[1] & iris$Sepal.Length &lt;= input$sepal[2], ]\n    if (input$species != \"All\") {\n      filtered_data &lt;- filtered_data[filtered_data$Species == input$species, ]\n    }\n    \n    # Render the reactable table\n    reactable(filtered_data, \n              columns = list(\n                Sepal.Length = colDef(name = \"Sepal Length\"),\n                Sepal.Width = colDef(name = \"Sepal Width\"),\n                Petal.Length = colDef(name = \"Petal Length\"),\n                Petal.Width = colDef(name = \"Petal Width\"),\n                Species = colDef(name = \"Species\")\n              ),\n              striped = TRUE, highlight = TRUE)\n  })\n}\n\n# Run the application \nshinyApp(ui = ui, server = server)\nIn this enhanced version:\n\nSpecies Filter: The selectInput lets users choose a specific species or view all species in the table. This input is especially useful for focusing on subsets within categorical data.\nSlider and Select Filter Combination: We filter by Sepal Length range and Species, providing two levels of control over what users see.\nreactable Styling: striped = TRUE and highlight = TRUE options add styling to make the table easier to read and navigate.\n\nWith reactable in Shiny, users get a polished table with styling and functionality that can adapt dynamically to any dataset they’re exploring.\n\n\n\nEnhancing Tables with Advanced Extensions\nNow that we’ve covered interactivity, let’s take a look at some tricky extensions that can add advanced customization, small in-table visualizations, and complex formatting to your tables. While they may not add interactivity in the same way as DT or reactable, these packages help you create visually stunning tables that can make your data come alive in reports and presentations. Here’s a rundown of some of the best tools for taking your tables from basic to brilliant.\n\nkableExtra: Advanced Formatting for Markdown Tables\nIf you’re using knitr::kable() to create tables in R Markdown, kableExtra is a perfect companion. It provides advanced styling options to add borders, bold headers, row grouping, and even color coding, making your tables far more visually appealing and readable.\nExample: Creating a Styled Table with kableExtra\n\n\n\nImage\n\n\nIn this example:\n\nkable_styling() adds bootstrap options to apply striping, hovering, and condensed spacing.\nrow_spec() makes the header row bold, with a custom color and background, drawing the reader’s attention to column titles.\ncolumn_spec() applies bold formatting to the first column to distinguish it visually.\nadd_header_above() creates a merged header spanning multiple columns.\n\n\n\ngtExtras: Adding Visuals to gt Tables\nIf you’re using gt for creating high-quality tables, gtExtras can help you take it to the next level. This extension enables you to add sparklines, bar charts, lollipop charts, and other mini visualizations directly within cells. It’s a great way to add trend data, comparisons, or distribution insights to your tables without relying on external plots.\nExample: Adding Sparklines and Mini Bar Charts with gtExtras\nlibrary(gt)\nlibrary(gtExtras)\nlibrary(dplyr)\n\n# Prepare example data with a trend for each row\niris_summary &lt;- iris %&gt;%\n  group_by(Species) %&gt;%\n  summarize(\n    Avg_Sepal_Length = mean(Sepal.Length),\n    Sepal_Length_Trend = list(sample(4:8, 10, replace = TRUE))\n  )\n\n# Create a gt table with sparklines for trends\ngt(iris_summary) %&gt;%\n  gt_plt_sparkline(Sepal_Length_Trend) %&gt;%\n  tab_header(title = \"Iris Species Summary\", subtitle = \"Including Sepal Length Trends\")\n\n\n\nImage\n\n\nIn this example:\n\ngt_plt_sparkline() adds a sparkline within the Sepal_Length_Trend column, showing trends for each species.\ntab_header() provides a title and subtitle for context.\n\nWith gtExtras, your tables can communicate more than just static data — they can tell a story by visually showcasing trends and distributions right in the table cells.\n\n\nformattable: In-Cell Visualizations for DataFrames\nThe formattable package is another powerful tool for creating visually enhanced tables, particularly useful for adding color-coded scales, bars, and visual indicators based on cell values. It’s designed to help you visualize comparisons directly within a data.frame, making it ideal for quick dashboards or reports.\nExample: Adding Color Scales and Mini Bars with formattable\nlibrary(formattable)\n\n# Create a formattable table with in-cell color scales and bars\nformattable(\n  iris,\n  list(\n    Sepal.Length = color_tile(\"lightblue\", \"lightgreen\"),\n    Sepal.Width = color_bar(\"pink\"),\n    Petal.Length = formatter(\"span\", \n                             style = x ~ style(font.weight = \"bold\", color = ifelse(x &gt; 4, \"red\", \"black\")))\n  )\n)\n\n\n\nImage\n\n\nIn this example:\n\ncolor_tile() applies a background color gradient to Sepal.Length, making it easy to compare values visually.\ncolor_bar() adds a color bar in Sepal.Width cells, giving a quick visual cue of relative size.\nformatter() applies conditional font styling to Petal.Length, highlighting values above a threshold in red.\n\n\n\nflextable: Creating Word and PowerPoint-Compatible Tables\nFor reports destined for Word or PowerPoint, flextable is a robust choice, offering rich customization options that ensure your tables look polished in these formats. With flextable, you can merge cells, add images, and apply various themes, making it a go-to option for tables that need to be embedded in professional documents.\nExample: Customizing a Table for Word with flextable\nlibrary(flextable)\n\n# Create a flextable with merged headers and styling\nft &lt;- flextable(head(iris))\nft &lt;- set_header_labels(ft, Sepal.Length = \"Sepal Length\", Sepal.Width = \"Sepal Width\")\nft &lt;- add_header_row(ft, values = c(\"Flower Measurements\"), colspan = 4)\nft &lt;- theme_vanilla(ft)\nft &lt;- autofit(ft)\n\n# Save to Word\n# save_as_docx(ft, path = \"iris_table.docx\")\nft\n\n\n\nImage\n\n\nEach of these packages offers unique strengths for customizing tables, making them valuable tools for any R user aiming to create more engaging, insightful, and visually appealing tables. Whether you’re building tables with in-cell visualizations, integrating trends with sparklines, or creating print-ready documents, these extensions let you go beyond basics and add a professional polish to your work.\n\n\n\nBringing Data to Life with Interactive and Enhanced Tables\nTables may seem simple, but they’re one of the most powerful tools for data communication. In this article, we’ve explored how to transform tables from static rows and columns into dynamic, interactive tools using R’s DT and reactable packages. Whether in a Shiny app or a standalone report, these tables allow readers to explore, filter, and engage with data in real-time, making data insights accessible to everyone.\nAnd when interactivity isn’t needed, we’ve looked at advanced table extensions like kableExtra, gtExtras, formattable, and flextable, which bring tables to life with beautiful formatting, in-cell visualizations, and high-quality styling options. These tools ensure your tables aren’t just functional—they’re visually compelling and professionally polished.\nBy combining interactivity with powerful formatting extensions, you have everything you need to craft tables that both captivate and communicate effectively. Now, you’re ready to bring data to life, one table at a time!"
  },
  {
    "objectID": "ds/posts/2024-10-17_Don-t-Get-Fooled-by-Numbers--Data-Literacy-as-the-New-Survival-Skill-72a484855c80.html",
    "href": "ds/posts/2024-10-17_Don-t-Get-Fooled-by-Numbers--Data-Literacy-as-the-New-Survival-Skill-72a484855c80.html",
    "title": "Don’t Get Fooled by Numbers: Data Literacy as the New Survival Skill",
    "section": "",
    "text": "Don’t Get Fooled by Numbers: Data Literacy as the New Survival Skill\n\n\n\nImage\n\n\nHave you ever looked at a headline or a graph and thought, “Well, the numbers don’t lie, right?” It’s tempting to trust the stats we see around us — whether it’s a political poll, a study about coffee’s health benefits, or a chart showing the rise of inflation. We want data to be the ultimate truth-teller. But here’s the thing: numbers can lie, or at least, they can be really good at misdirection.\nThink about it. We live in a world overflowing with data. It’s everywhere — on our phones, in our news feeds, in every presentation at work. But how often do we stop to think about what the data is actually telling us, or more importantly, what it’s not telling us? We see percentages, averages, and correlations, but without the tools to interpret them, we’re at risk of getting fooled.\nThat’s where data literacy comes in. It’s not some abstract skill reserved for data scientists or economists anymore. It’s something we all need, kind of like knowing how to swim or navigate Google Maps. In today’s data-driven world, understanding the nuances behind the numbers is a new kind of survival skill — one that can keep us from being misled by a headline, a study, or even a sales pitch.\nIn this article, we’re going to talk about why data literacy is so crucial, especially now, and how you can make sure you’re not getting caught up in the numbers game. Think of this as a friendly guide to seeing through the stats and gaining the tools to navigate today’s world of information without being duped.\n\n\nWhat is Data Literacy?\nOkay, so let’s break it down: what exactly is data literacy? It sounds technical, but at its core, it’s simply the ability to read, understand, and interpret data in a meaningful way. Think of it like learning a new language. Just like with any language, it’s not enough to recognize the words — you need to grasp the context, the tone, the nuances.\nBeing data literate means more than just being able to read a chart or decipher a spreadsheet. It’s about asking the right questions: Where did this data come from? What does it really measure? What’s missing here? These questions are key, because data rarely gives you the full story right away. And in a world where every headline and decision seems backed by numbers, having the skills to dig deeper is becoming more essential every day.\nBut here’s the thing: data literacy isn’t just for people who work with data all day. Whether you’re a marketer looking at campaign metrics, a parent deciding which school district has the best performance, or even someone just trying to make sense of COVID-19 statistics, you’re using data constantly. Yet, how often do we really stop and think about whether we’re interpreting it right?\nIt’s easy to get swept away by a flashy statistic or a well-designed infographic, but without data literacy, we might miss critical details or fall into common traps. And that’s the real danger — when we trust numbers without understanding them, we’re more vulnerable to misinterpretation, and sometimes, outright manipulation.\nSo, in a nutshell: data literacy is about not taking numbers at face value. It’s learning to read between the lines, to think critically about the data, and to understand the bigger picture. Because in today’s world, knowing how to decode data isn’t just useful — it’s a superpower.\n\n\nWhy is Data Literacy So Important Today?\nThink about the last time you made a decision based on something you saw or read. Maybe you were scrolling through your news feed, deciding whether to believe that new health study. Or perhaps you were looking at a company’s quarterly earnings report, wondering if it’s time to invest. Whatever the situation, you were likely relying on data — whether you realized it or not.\nWe’re living in a world where data surrounds us, constantly influencing the choices we make. It’s in our politics, our health, our finances, even in our social media feeds. And while that may sound empowering — hey, more information should lead to better decisions, right? — there’s a catch. Just because data is everywhere doesn’t mean it’s always clear or, more importantly, truthful.\nIn fact, data can be misleading. And not just by accident. In a world where headlines race for clicks and every product needs a competitive edge, numbers are often presented in ways that skew reality. A simple statistic can be framed to make a point sound more convincing, a chart can omit key context, and suddenly, we’re making decisions based on half-truths.\nThis is where data literacy steps in. It’s our shield against being misled by numbers that are designed to sway us. It gives us the ability to look past the surface, to dig into what those numbers are really saying (or not saying). In a world full of data, those who know how to interpret it critically are the ones who will avoid being fooled.\nLet’s be honest — none of us are completely immune to this. Even the most data-savvy people can fall into the trap of taking a flashy statistic at face value. But that’s exactly why data literacy is so important today. The sheer volume of information coming at us means that the stakes are higher than ever. Without the ability to navigate through this flood of data, we risk making decisions that aren’t based on the truth, but on a carefully framed version of it.\nIn a world grappling with misinformation, political polarization, and rapid technological changes, being able to critically evaluate data is like having a compass in the storm. It’s not just about being right or wrong — it’s about having the confidence that the decisions you’re making are based on a solid understanding of the facts.\n\n\nHow Data Literacy Impacts Business and Society\nNow, let’s zoom out a bit. Data literacy doesn’t just matter on a personal level — it’s shaping entire organizations, industries, and even societies. We live in an era where “data-driven decisions” is more than a buzzword. It’s the way businesses operate, governments govern, and even how we understand global challenges like climate change or pandemics.\n\nIn Business:\nImagine you’re a part of a company that’s about to launch a new product. There’s a ton of data coming in — market research, customer feedback, sales projections — and it’s easy to feel overwhelmed by the sheer volume of numbers. Here’s where data literacy becomes a game changer. It’s not just about having the data, it’s about knowing how to interpret it, challenge it, and ultimately make decisions that align with reality, not just the story the numbers seem to tell.\nIn businesses that foster data literacy across teams, it’s not just the data scientists or analysts who benefit — everyone does. The marketing team can better understand campaign metrics, sales teams can make smarter pitches, and leadership can make decisions based on insights rather than gut feelings. Data-literate organizations are more adaptable, more efficient, and less likely to fall into traps like misinterpreting customer trends or misallocating resources.\nBut it goes beyond making smarter decisions. Data literacy within companies fosters a culture of accountability. When everyone, from the CEO to the newest intern, has a basic understanding of how data works, it’s harder to pull the wool over anyone’s eyes. Numbers can’t be twisted as easily when everyone is trained to look deeper.\n\n\nIn Society:\nOn a larger scale, data literacy is just as critical — if not more so. Take government policies, for example. When officials base decisions on data, they’re often faced with statistics that need to be interpreted carefully. Whether it’s allocating resources during a public health crisis or setting environmental regulations, understanding data accurately can be the difference between a successful policy and one that falls flat.\nAnd here’s where the public comes in. Data literacy isn’t just important for the people making those decisions — it’s just as important for the rest of us, who are often on the receiving end of those policies. When we’re able to understand the data behind public policies, we’re in a better position to engage in informed discussions, advocate for change, or challenge decisions that don’t seem to add up.\nIt also helps us avoid falling for misinformation, which, let’s face it, is a huge issue right now. Whether it’s fake news or misleading reports, a lack of data literacy makes it all too easy for misinformation to spread. But when people are equipped to question and critically evaluate the data they see, the power of those false narratives weakens.\nUltimately, data literacy doesn’t just help us make better decisions — it helps create more transparent, accountable, and informed communities.\n\n\n\nExamples of the Risks That Come with Low Data Literacy\nNow, let’s talk about what happens when we don’t have data literacy — or when we don’t use it. The risks here aren’t just theoretical. There are plenty of real-world examples where misunderstanding data led to bad decisions, misinformation, and even harmful outcomes.\n\nMisinformation and Fake News\nPerhaps the most glaring example is the spread of fake news and misinformation. We’ve all seen those sensational headlines that claim “Studies Prove X Causes Y!” or “New Research Shows Z is Dangerous!” — only to later find out the study was poorly conducted or the data misrepresented.\nTake health misinformation, for example. During the COVID-19 pandemic, data was flying everywhere: infection rates, vaccine efficacy, mortality statistics. But without data literacy, it became incredibly easy for misinformation to spread. Some people misinterpreted basic statistical concepts — like mistaking correlation for causation — or fell for flashy statistics without understanding the nuances behind them.\nThis kind of misunderstanding doesn’t just create confusion; it can lead to real-world consequences, like vaccine hesitancy or panic buying. And it’s not just about health. In politics, too, we see data being misused or misrepresented, influencing public opinion and policy decisions in ways that don’t always reflect reality.\n\n\nMisleading Statistics in Marketing\nMarketing teams love a good statistic, and for a reason — they’re convincing. But sometimes, those numbers can be stretched to fit a narrative. Ever seen a product that claims “80% of users saw results!” but there’s no clear explanation of what that means? Or maybe a financial product that promises a “guaranteed 10% return” without mentioning the fine print or the risks involved?\nCompanies often use selective data to present their products in the best possible light, and without a good grasp of how statistics can be manipulated, consumers might fall for it. This doesn’t just apply to sales pitches. It’s something that affects all of us as consumers, whether we’re buying a new gadget, signing up for a gym membership, or investing in stocks. Data literacy helps us see through the spin.\n\n\nPublic Policy and Misinterpreted Data\nLet’s not forget the impact on public policy. Governments and organizations make decisions based on data all the time, from setting budgets to developing health regulations. But when that data is misinterpreted, the consequences can be far-reaching.\nFor instance, if a government agency misinterprets data on poverty or unemployment, they might allocate resources inefficiently or introduce policies that don’t actually address the root problems. Similarly, when environmental data is misused — like downplaying climate change impacts — it can lead to policies that are out of step with reality, ultimately harming both the planet and the people.\nThe ripple effect of poor data literacy in public policy can impact entire communities, creating solutions that look good on paper but fail to deliver in practice.\n\n\nEveryday Decisions\nIt’s easy to think of data literacy as something big corporations or governments need, but what about the decisions you make every day? Imagine you’re looking at mortgage rates, deciding which school district to move to, or even choosing which diet plan to follow. In all of these cases, data plays a role.\nWithout the ability to critically assess the information, you might end up choosing a financial product that’s not in your best interest, moving to an area with misleading education statistics, or following health advice that’s not backed by solid data. Data literacy helps you make choices that are genuinely informed, not just based on surface-level information.\n\n\n\nHow to Improve Your Own Data Literacy\nBy now, you’re probably thinking, “Okay, I get it — data literacy is important. But how do I actually get better at it?” The good news is, you don’t need to be a data scientist to start improving your data skills. Just like learning any new skill, improving your data literacy comes with practice and a few key strategies that can make a big difference in how you approach data.\n\n1. Start Asking the Right Questions\nThe first step is to get comfortable with questioning the data in front of you. Whether you’re looking at a news article, a product review, or a report at work, ask yourself: Where does this data come from? What’s the source? What’s missing? How was it collected? These simple questions can help you spot potential biases or gaps in the information.\nFor example, if you see a study that claims “75% of people prefer Product A over Product B,” dig a little deeper. How many people were surveyed? Who funded the study? These questions can reveal whether that shiny statistic is truly as meaningful as it first appears.\n\n\n2. Get Familiar with Basic Statistics\nYou don’t need to dive into complex mathematical formulas, but having a grasp of basic statistics can go a long way. Understanding terms like mean, median, mode, correlation, and causation can help you interpret data more accurately. For instance, knowing that a high correlation between two variables doesn’t mean one caused the other can save you from falling into a common data trap.\nThere are plenty of free online resources, like Khan Academy or Coursera, where you can get comfortable with the basics without feeling overwhelmed.\n\n\n3. Practice with Everyday Data\nData is everywhere, so why not start practicing with the information you encounter daily? The next time you see a headline about the economy or a post on social media with a surprising stat, take a few moments to critically evaluate it. What’s being shown? What’s missing? Is the conclusion supported by the data?\nYou can also dive into tools like Google Sheets or Excel to play around with simple datasets. Explore how changing one variable can impact the results, and experiment with different ways of visualizing data to understand the impact of presentation.\n\n\n4. Explore Tools and Resources\nIf you’re ready to take it up a notch, there are plenty of tools out there that can help improve your data literacy. For instance, platforms like Tableau and Power BI allow you to create and explore data visualizations, making it easier to see patterns and insights that might not be obvious from raw numbers.\nFor those interested in going deeper into analytics, there are also free or low-cost courses that teach you how to use R, Python, or SQL — languages commonly used for data analysis. But don’t worry, even a basic introduction to these tools can expand your understanding of how data works.\n\n\n5. Stay Curious and Skeptical\nFinally, perhaps the most important tip is to stay curious and skeptical. Data literacy isn’t just about learning technical skills; it’s about cultivating a mindset of critical thinking. Always question the story behind the numbers, and never assume data is “truth” just because it’s presented in a neat package.\nIn a world overflowing with information, being data literate isn’t just a bonus skill — it’s a necessity. The more you build this skill, the more confident you’ll become in navigating the vast sea of data around you, whether it’s at work, in the news, or even in your personal life.\n\n\n\nConclusion\nIn today’s world, data literacy is a survival skill. It helps us make better decisions, avoid misinformation, and engage more meaningfully with the world around us. Whether you’re dealing with your own finances, understanding public policy, or just trying to make sense of a viral statistic, being data literate gives you an edge.\nSo, the next time you see a flashy number or a slick-looking chart, don’t just take it at face value. Look a little deeper, ask a few more questions, and remember — you have the tools to see through the numbers and get to the truth."
  },
  {
    "objectID": "ds/posts/2024-08-23_SQL-of-the-Rings--One-Language-to-Query-Them-All--with-R--8d56c91a3439.html",
    "href": "ds/posts/2024-08-23_SQL-of-the-Rings--One-Language-to-Query-Them-All--with-R--8d56c91a3439.html",
    "title": "SQL of the Rings: One Language to Query Them All (with R)",
    "section": "",
    "text": "Concise Tutorial for R Database Interfaces\n\n\n\nImage\n\n\nIn the vast and intricate world of data, much like the realms of Middle-earth, there lies a powerful force — SQL, the language of databases. SQL (Structured Query Language) has long been the One Language to query them all, a tool that allows us to retrieve, manipulate, and manage the vast treasures of information stored within our databases. But like any great power, its true potential is unlocked only when wielded wisely.\nEnter R, the versatile and mighty language of data analysis, statistics, and visualization. R is to data scientists what the Elven rings were to their bearers — a tool of great influence, allowing them to perform incredible feats of analysis, prediction, and insight. However, as potent as R is, it cannot rule the world of data alone. The true power lies in the harmonious combination of R and SQL, much like the fellowship that united to confront the challenges of Middle-earth.\nIn this article, we start a journey — a quest, if you will — through the realms of SQL and R. Together, we will explore how these two powerful tools can be united to master the ever-growing landscape of data. Whether you are an analyst delving into complex datasets, a data scientist crafting predictive models, or a developer integrating data pipelines, the synergy of SQL and R will guide you to new heights of efficiency and insight.\nWe begin our journey by learning how to connect R to various databases, akin to unlocking the gates of Moria. We will then delve into the art of crafting secure and efficient queries, reminiscent of the dwarves forging tools in the depths of their mines. As we progress, we will harness the power of dbplyr—the One Interface that binds the simplicity of dplyr with the might of SQL. Along the way, we will face challenges, such as battling large datasets, and discover techniques to emerge victorious.\nFinally, we will touch upon the ancient art of Object-Relational Mapping (ORM), a method of wielding SQL’s power with the precision of R6 object-oriented programming, akin to forging your own rings of power.\nSo, gather your courage and prepare your tools, for we are about to embark on an epic adventure — one that will unlock the full potential of SQL and R, and elevate your data analysis to legendary status.\n\n\nSpeak, Friend, and Enter: Connecting R to Your Databases\nEvery great journey begins with a single step, and in our quest to master SQL in R, that step is establishing a connection to our database. Just as the Fellowship needed to speak the password to open the Gates of Moria, we must correctly configure our connections to access the treasures of data stored within our databases.\nIn R, the key to these gates lies in the DBI package, a robust and consistent interface for connecting to a variety of Database Management Systems (DBMS). Whether you’re delving into the depths of an SQLite file, managing the sprawling realms of a MariaDB instance, or exploring the lightning-fast DuckDB, DBI and its companion drivers will guide you through.\n\nConnecting to SQLite: The Ancient Repository\nSQLite is the equivalent of an ancient Dwarven repository — compact, self-contained, and requiring no external server. It’s perfect for projects where you need a lightweight, portable database that’s easy to manage.\nHere’s how you can connect to an SQLite database in R:\nlibrary(DBI)\n\n# Establish a connection to the SQLite database\ncon_sqlite &lt;- dbConnect(RSQLite::SQLite(), dbname = \"my_database.sqlite\")\n\n# Check connection\nprint(con_sqlite)\nThis simple command opens the gate to your SQLite database, allowing you to explore its contents, query its tables, and manipulate its data — all from within R.\n\n\nConnecting to MariaDB: The Kingdom of Data\nFor larger, more complex datasets requiring a scalable, server-based solution, MariaDB is the kingdom where your data resides. MariaDB, a powerful fork of MySQL, is well-suited for enterprise-level applications, offering robust performance and extensive features.\nConnecting to MariaDB in R is straightforward with the RMariaDB package:\nlibrary(DBI)\n\n# Establish a connection to the MariaDB database\ncon_mariadb &lt;- dbConnect(RMariaDB::MariaDB(), \n                         dbname = \"your_database_name\", \n                         host = \"localhost\", \n                         user = \"your_username\", \n                         password = \"your_password\")\n\n# Check connection\nprint(con_mariadb)\nWith this connection, you gain access to the vast resources of your MariaDB database, ready to be queried and analyzed.\n\n\nConnecting to DuckDB: The Speed of a Ranger\nDuckDB is the ranger of the database world — swift, efficient, and designed for rapid analytical queries. It’s a great choice when you need to process large datasets on the fly, without the overhead of traditional database management systems.\nHere’s how you can connect to DuckDB in R:\nlibrary(DBI)\n\n# Establish a connection to the DuckDB database\ncon_duckdb &lt;- dbConnect(duckdb::duckdb(), dbdir = \"my_database.duckdb\")\n\n# Check connection\nprint(con_duckdb)\nWith DuckDB, you can traverse large datasets with the speed and agility of a ranger, executing complex queries in a fraction of the time it might take with other systems.\n\n\nClosing the Gate: Disconnecting from Databases\nJust as it’s important to close the Gates of Moria once the Fellowship has passed through, it’s essential to properly close your database connections when you’re done. This ensures that resources are freed and that you maintain a good practice of managing your connections.\n# Disconnect from SQLite\ndbDisconnect(con_sqlite)\n\n# Disconnect from MariaDB\ndbDisconnect(con_mariadb)\n\n# Disconnect from DuckDB\ndbDisconnect(con_duckdb)\nBy mastering the art of database connections in R, you’re well on your way to unlocking the full potential of SQL and R. With the gates open, you’re ready to explore the data that lies within.\n\n\n\nForging Secure Queries in the Mines of Moria\nAs we venture deeper into the world of data, it’s essential to equip ourselves with the right tools — much like the Dwarves of Moria, who crafted their mighty weapons and armor in the deep mines. In the realm of SQL and R, these tools are encapsulated within the DBI package, which allows us to perform operations on our databases securely and efficiently.\n\nForging Queries with dbExecute and dbGetQuery\nTwo of the most fundamental tools in our SQL arsenal are dbExecute and dbGetQuery. These functions allow us to run SQL commands directly from R, retrieving or modifying data as needed.\n\ndbGetQuery: This function is used to execute SQL SELECT queries, retrieving data from the database and returning it as a data frame in R.\n\n# Retrieve data from a table\nresult &lt;- dbGetQuery(con_mariadb, \"SELECT * FROM users WHERE age &gt; 30\")\nprint(result)\n\ndbExecute: This function is used for SQL commands that modify the database, such as INSERT, UPDATE, or DELETE statements.\n\n# Insert a new record into the users table\ndbExecute(con_mariadb, \"INSERT INTO users (name, age) VALUES ('Aragorn', 87)\")\nThese tools, while powerful, must be wielded with care. Just as the Dwarves took great care in forging their weapons, we must ensure that our queries are secure and efficient.\n\n\nSecuring Queries with dbBind\nIn the Mines of Moria, the Dwarves faced many dangers, some of their own making. Similarly, careless use of SQL queries can expose your database to significant risks, particularly SQL injection attacks. This is where dbBind comes in—offering a way to securely bind parameters to your SQL queries, preventing malicious inputs from wreaking havoc.\nHere’s how you can use dbBind to safely insert user data:\n# Securely insert data using parameterized queries\nquery &lt;- \"INSERT INTO users (name, age) VALUES (?, ?)\"\ndbExecute(con_mariadb, query, params = list(\"Frodo\", 50))\nBy using dbBind, you ensure that inputs are properly escaped and handled, much like a Dwarven smith ensuring that their forge is safe from disaster.\n\n\nCrafting Complex Queries\nJust as the Dwarves crafted intricate weapons, you can use SQL to build complex, multi-step queries. These might involve subqueries, joins, or aggregations — allowing you to derive powerful insights from your data.\n# Example of a complex query using JOIN\nquery &lt;- \"\n  SELECT users.name, COUNT(orders.id) AS order_count\n  FROM users\n  JOIN orders ON users.id = orders.user_id\n  GROUP BY users.name\n  HAVING COUNT(orders.id) &gt; 5\n\"\nresult &lt;- dbGetQuery(con_mariadb, query)\nprint(result)\nWith these tools in hand, you are well-equipped to navigate the depths of your database, uncovering insights and forging a path through the data with the precision and skill of a master craftsman.\n\n\n\nOne Interface to Bind Them All: dbplyr and the Power of dplyr in Databases\nIn the saga of our data journey, dbplyr emerges as the One Interface to bind the simplicity of dplyr with the might of SQL. Much like the One Ring in Tolkien’s epic, dbplyr unites the strengths of different realms—in this case, the worlds of R and SQL—allowing us to perform powerful data manipulations without ever leaving the comfort of R’s syntax.\n\nHarnessing the Power of dbplyr\ndbplyr acts as a bridge, translating dplyr commands into SQL queries that are executed directly on the database. This means you can leverage the tidyverse’s intuitive syntax while still harnessing the full power of SQL under the hood.\nlibrary(dplyr)\nlibrary(dbplyr)\n\n# Reference a table in the database using `tbl()`\nusers_db &lt;- tbl(con_mariadb, \"users\")\n\n# Perform a `dplyr` operation (automatically translated to SQL)\nresult &lt;- users_db %&gt;%\n  filter(age &gt; 30) %&gt;%\n  select(name, age) %&gt;%\n  arrange(desc(age)) %&gt;%\n  collect()\n\nprint(result)\nIn this example, the dplyr operations are seamlessly converted into SQL, executed on the database, and the results are returned to R. This allows you to work efficiently with large datasets, keeping data processing within the database until you need to bring the results into R.\n\n\nSeeing the SQL Behind the Magic: show_query()\nOne of the great powers of dbplyr is its transparency—you can see exactly what SQL is being generated by your dplyr commands. This is where the show_query() function comes into play, revealing the SQL code that will be executed on your database.\n# Use `show_query()` to see the SQL query that `dplyr` generates\nusers_db %&gt;%\n  filter(age &gt; 30) %&gt;%\n  select(name, age) %&gt;%\n  arrange(desc(age)) %&gt;%\n  show_query()\n\n&lt;SQL&gt;\nSELECT `name`, `age`\nFROM `users`\nWHERE `age` &gt; 30.0\nORDER BY `age` DESC\nThis output shows the exact SQL query that dbplyr has generated based on your dplyr code. This transparency is invaluable for debugging, optimizing queries, and understanding how your data manipulations translate into SQL.\n\n\nSecuring Your Queries with glue_sql\nWhile dbplyr offers a powerful interface, there are times when you might need to write custom SQL queries directly. When doing so, it’s crucial to ensure these queries are secure, especially if they include user inputs. This is where glue_sql from the glue package comes in—offering a way to safely construct SQL queries by automatically escaping inputs.\nlibrary(glue)\n\n# Example of using `glue_sql` to create a safe query\nage_threshold &lt;- 30\nquery &lt;- glue_sql(\"SELECT * FROM users WHERE age &gt; {age_threshold}\", .con = con_mariadb)\nresult &lt;- dbGetQuery(con_mariadb, query)\n\nprint(result)\nIn this example, glue_sql ensures that user inputs are safely handled, much like the One Ring carefully managed by those who understand its power.\nWith dbplyr as your guide, you can harness the full potential of SQL within R, while maintaining the clarity and simplicity that the tidyverse is known for. By using show_query(), you gain insight into the underlying SQL, giving you the control to optimize and refine your data manipulations.\n\n\n\nCouncil of Elrond: Techniques for Querying with dplyr and Friends\nAt the Council, the wisest and most experienced characters gathered to strategize for the journey ahead. Similarly, when working with SQL in R, it’s important to gather and leverage the best tools and techniques for querying your data. In this chapter, we’ll explore how to use dplyr, dbplyr, and other tidyverse packages to execute powerful and efficient queries, drawing from the wisdom of these tools to unlock the full potential of your data.\n\nBasic Data Manipulations with dplyr and dbplyr\ndplyr provides a suite of functions designed to simplify data manipulation, and with dbplyr, these same functions can be applied directly to data stored in a database. This approach allows you to work with large datasets efficiently, keeping the heavy lifting on the database server until you’re ready to bring the results into R.\nHere are some of the fundamental dplyr functions you can use with dbplyr:\n\nfilter(): Subset rows based on conditions.\nselect(): Choose specific columns to return.\nmutate(): Create new columns or modify existing ones.\nsummarize(): Aggregate data, often combined with group_by().\narrange(): Sort data based on one or more columns.\n\nExample: Suppose you have a table of users, and you want to find the names and ages of users who are older than 30, ordered by age.\nusers_db &lt;- tbl(con_mariadb, \"users\")\n\nresult &lt;- users_db %&gt;%\n  filter(age &gt; 30) %&gt;%\n  select(name, age) %&gt;%\n  arrange(desc(age)) %&gt;%\n  collect()\n\nprint(result)\nIn this example, the dplyr commands are automatically translated into SQL queries, executed on the database, and the results are returned to R.\n\n\nAdvanced Query Techniques: Joins, Subqueries, and More\nIn many cases, data analysis requires more than just basic filtering and selection. You might need to combine data from multiple tables, perform calculations, or execute complex subqueries. dplyr and dbplyr provide functions that make these operations straightforward and readable.\nJoins: Combining data from two or more tables based on a common key is a frequent operation in SQL. dplyr offers a range of join functions (inner_join, left_join, right_join, full_join) that are easy to use and understand.\nExample: Joining two tables, users and orders, to find out which users have placed orders.\norders_db &lt;- tbl(con_mariadb, \"orders\")\n\nresult &lt;- users_db %&gt;%\n  inner_join(orders_db, by = \"user_id\") %&gt;%\n  select(name, order_id, order_date) %&gt;%\n  collect()\n\nprint(result)\nSubqueries: Sometimes, you need to create a query that is based on the results of another query. dplyr allows you to nest operations, which dbplyr then translates into subqueries in SQL.\nExample: Finding users who have placed more than five orders.\nresult &lt;- orders_db %&gt;%\n  group_by(user_id) %&gt;%\n  summarize(order_count = n()) %&gt;%\n  filter(order_count &gt; 5) %&gt;%\n  inner_join(users_db, by = \"user_id\") %&gt;%\n  select(name, order_count) %&gt;%\n  collect()\n\nprint(result)\nBy gathering the wisdom of dplyr, dbplyr, and other tidyverse tools, you’re equipped to handle even the most complex data queries with elegance and power—just as the Council of Elrond strategized to overcome the challenges of Middle-earth.\n\n\n\nYou Shall Not Pass: Handling Large Datasets with R and SQL\nIn the depths of Khazad-dûm, the Fellowship faced one of their greatest challenges — the Balrog, a monstrous being of fire and shadow. Similarly, in the world of data, large datasets can present formidable obstacles, threatening to overwhelm your system’s memory and processing capabilities. But just as Gandalf stood firm against the Balrog, wielding his power to protect the Fellowship, you can leverage R and SQL to handle massive datasets efficiently and effectively.\n\nWhy Use Databases for Large Datasets?\nWhen working with large datasets, the limitations of in-memory processing in R become apparent. R’s data frames, while powerful for smaller datasets, can quickly exhaust available memory when handling millions of rows or complex operations. This is where SQL databases excel — by keeping data on disk and only processing what’s necessary, databases can handle much larger datasets without the same memory constraints.\n\n\nChunked Processing: Breaking the Problem into Manageable Pieces\nOne of the key techniques for working with large datasets in R is chunked processing. Instead of loading the entire dataset into memory, you can process it in smaller, more manageable chunks. This approach is particularly useful when you’re performing operations that don’t require access to the entire dataset at once, such as filtering, aggregating, or writing results incrementally.\nExample: Suppose you have a large table of transactions and you want to calculate the total sales for each product. Instead of loading the entire table into R, you can process it in chunks:\nlibrary(dplyr)\n\n# Assume transactions_db is a large table in your database\ntransactions_db &lt;- tbl(con_mariadb, \"transactions\")\n\n# Define a function to process each chunk\nprocess_chunk &lt;- function(chunk) {\n  chunk %&gt;%\n    group_by(product_id) %&gt;%\n    summarize(total_sales = sum(sales_amount)) %&gt;%\n    collect()\n}\n\n# Use a loop or a functional approach to process the table in chunks\nresults &lt;- list()\n\nfor (i in seq(1, n_chunks)) {\n  chunk &lt;- transactions_db %&gt;%\n    filter(chunk_id == i)  # Assuming chunk_id is a column that segments the data\n  results[[i]] &lt;- process_chunk(chunk)\n}\n\n# Combine the results from all chunks\nfinal_result &lt;- bind_rows(results)\nprint(final_result)\nThis approach ensures that each chunk is processed independently, avoiding memory overload and making it possible to work with very large datasets.\n\n\nLeveraging Database Power: Keeping the Heavy Lifting on the Server\nAnother strategy for handling large datasets is to push as much computation as possible to the database server. SQL is designed for efficient data processing, so by using dbplyr to perform operations directly on the database, you can take full advantage of the database’s capabilities.\nExample: Calculating the total sales for each product directly on the database:\nresult &lt;- transactions_db %&gt;%\n  group_by(product_id) %&gt;%\n  summarize(total_sales = sum(sales_amount)) %&gt;%\n  collect()\n\nprint(result)\nIn this example, the aggregation is performed entirely on the database, minimizing the amount of data that needs to be transferred to R and reducing the load on R’s memory.\n\n\nEmerging Victorious from the Battle\nBy combining the strengths of R and SQL, you can tackle even the most challenging datasets with confidence. Whether through chunked processing, leveraging the database’s power, or using a combination of both, you can ensure that large datasets do not become insurmountable obstacles. Just as Gandalf’s stand against the Balrog allowed the Fellowship to continue their journey, these techniques will allow you to continue your data analysis journey, no matter how large the datasets you encounter.\n\n\n\nReclaiming the Throne: Deploying SQL and R in Production\nAfter the epic battles have been fought and the dust has settled, the time comes for the rightful king to take his place on the throne, restoring order to the realm. Similarly, once you’ve developed your data analysis and processing workflows using SQL and R, the next step is to deploy these workflows into production, ensuring they run smoothly, reliably, and securely. In this chapter, we’ll explore how to take your SQL and R solutions from development to production, much like the return of Aragorn to reclaim his kingdom.\n\nBuilding Automated Data Pipelines\nIn a production environment, data workflows often need to run on a regular schedule — whether it’s updating reports, refreshing data models, or performing routine data transformations. Building automated data pipelines ensures that these tasks are executed consistently and without manual intervention.\nScheduling Scripts: One of the simplest ways to automate R scripts that use SQL is to schedule them with tools like cron (on Unix-based systems) or Task Scheduler (on Windows). These tools allow you to specify when and how often your scripts should run.\nExample: A cron job to run an R script daily at midnight.\n0 0 * * * /usr/bin/Rscript /path/to/your_script.R\nUsing RStudio Connect: RStudio Connect is a powerful platform that allows you to deploy R scripts, Shiny apps, and R Markdown documents. It provides scheduling capabilities, version control, and easy sharing with stakeholders.\nExample: Deploying an R Markdown report that queries a database and generates daily summaries.\nrmarkdown::render(\"daily_report.Rmd\")\n\n\nManaging Database Credentials Securely\nWhen deploying SQL and R workflows in production, it’s crucial to manage database credentials securely. Hardcoding usernames and passwords in your scripts is risky, as it can lead to unauthorized access if the script is shared or exposed.\nEnvironment Variables: Store sensitive information like database credentials in environment variables. This keeps them out of your code and allows you to change them easily without modifying your scripts.\nExample: Accessing database credentials from environment variables in R.\ndb_user &lt;- Sys.getenv(\"DB_USER\")\ndb_password &lt;- Sys.getenv(\"DB_PASSWORD\")\n\ncon_mariadb &lt;- dbConnect(RMariaDB::MariaDB(), \n                         dbname = \"your_database_name\", \n                         host = \"localhost\", \n                         user = db_user, \n                         password = db_password)\nConfiguration Files: Another approach is to use a configuration file, such as config.yml, to store your database settings. The config package in R makes it easy to read and use these settings in your scripts.\nExample: Using the config package to manage database configurations.\nlibrary(config)\n\ndb_config &lt;- config::get(\"database\")\n\ncon_mariadb &lt;- dbConnect(RMariaDB::MariaDB(), \n                         dbname = db_config$dbname, \n                         host = db_config$host, \n                         user = db_config$user, \n                         password = db_config$password)\n\n\nMonitoring and Maintenance\nOnce your SQL and R workflows are in production, ongoing monitoring and maintenance are essential to ensure they continue to run smoothly.\n\nMonitoring: Set up alerts and monitoring tools to notify you if a script fails, if a database becomes unreachable, or if performance issues arise. This allows you to respond quickly and minimize downtime.\nRegular Updates: Keep your R environment and packages up to date, but be cautious with major updates that might introduce breaking changes. Test updates in a development environment before deploying them to production.\n\nBy following these best practices, you can deploy your SQL and R workflows into production with confidence, knowing that they are secure, reliable, and maintainable. Just as Aragorn restored order to the kingdom, you can ensure that your data processes run smoothly, delivering consistent and accurate results to your stakeholders.\n\n\n\nForging Your Own Rings: Implementing ORM with R6 in R\nIn the realm of Middle-earth, the Rings of Power were forged to bring order and control over the different races, each ring granting its bearer immense power and influence. Similarly, in the world of data, Object-Relational Mapping (ORM) can be thought of as a powerful tool that brings order and control over database interactions, allowing developers to work with databases using familiar object-oriented principles.\nIn this chapter, we’ll explore how to implement a simple ORM system in R using R6 classes, which can encapsulate database operations within objects. This approach not only streamlines the interaction with databases but also makes your code more modular, reusable, and easier to maintain.\n\nIntroduction to ORM\nORM is a technique that allows you to interact with a database by mapping tables to classes, rows to objects, and columns to attributes. This abstraction makes it easier to manage database interactions within your application, as you can work with objects and methods rather than writing raw SQL queries.\n\nWhy Use ORM?: ORM simplifies database operations by encapsulating them within objects, making your code more intuitive and less error-prone. It also provides a layer of abstraction that can make your application more portable across different database systems.\n\n\n\nSetting Up R6 Classes: Forging the Rings\nR6 classes in R provide a framework for creating object-oriented structures, where you can define methods (functions) and fields (attributes) that correspond to your database operations. Let’s start by creating an R6 class that represents a simple User object, corresponding to a users table in a database.\nStep 1: Define the R6 Class\nlibrary(R6)\nlibrary(DBI)\n\n# Define the User class\nUser &lt;- R6Class(\"User\",\n  public = list(\n    con = NULL,  # Database connection\n    table_name = \"users\",  # Table name\n\n    initialize = function(con) {\n      self$con &lt;- con\n    },\n\n    # Method to create a new user\n    create = function(name, age) {\n      dbExecute(self$con, \n                paste(\"INSERT INTO\", self$table_name, \"(name, age) VALUES (?, ?)\"), \n                params = list(name, age))\n    },\n\n    # Method to find a user by ID\n    find_by_id = function(user_id) {\n      result &lt;- dbGetQuery(self$con, \n                           paste(\"SELECT * FROM\", self$table_name, \"WHERE id = ?\", user_id))\n      return(result)\n    },\n\n    # Method to update a user's age\n    update_age = function(user_id, new_age) {\n      dbExecute(self$con, \n                paste(\"UPDATE\", self$table_name, \"SET age = ? WHERE id = ?\"), \n                params = list(new_age, user_id))\n    },\n\n    # Method to delete a user by ID\n    delete = function(user_id) {\n      dbExecute(self$con, \n                paste(\"DELETE FROM\", self$table_name, \"WHERE id = ?\"), \n                params = list(user_id))\n    },\n\n    # Method to list all users\n    list_all = function() {\n      result &lt;- dbGetQuery(self$con, \n                           paste(\"SELECT * FROM\", self$table_name))\n      return(result)\n    }\n  )\n)\nThis class encapsulates basic CRUD (Create, Read, Update, Delete) operations for a users table. Each method corresponds to a database operation, making it easier to interact with the database without writing raw SQL in your main application code.\nStep 2: Using the R6 Class Now that we have our User class, we can use it to manage users in the database.\n# Connect to a database\ncon &lt;- dbConnect(RMariaDB::MariaDB(), \n                 dbname = \"your_database_name\", \n                 host = \"localhost\", \n                 user = \"your_username\", \n                 password = \"your_password\")\n\n# Create a new User object\nuser &lt;- User$new(con)\n\n# Create a new user\nuser$create(\"Frodo Baggins\", 50)\n\n# Find a user by ID\nfrodo &lt;- user$find_by_id(1)\nprint(frodo)\n\n#  id          name age\n#1  1 Frodo Baggins  50\n\n# Update a user's age\nuser$update_age(1, 51)\n\n# Delete a user by ID\nuser$delete(1)\n\n# List all users\nall_users &lt;- user$list_all()\nprint(all_users)\n\n# Disconnect from the database\ndbDisconnect(con)\nWith this setup, you can easily manage users in your database using simple method calls, much like wielding a Ring of Power. The operations are intuitive, encapsulated within the object, and shielded from the complexity of raw SQL.\n\n\nExtending the ORM: Forging Additional Rings\nThe power of ORM comes from its extensibility. You can create additional classes for other tables in your database, each with methods tailored to specific operations. For example, you could create an Order class for managing orders, or an Inventory class for tracking products.\nEach class can interact with the others, allowing you to build complex operations while maintaining clear, organized, and reusable code. This modular approach is especially beneficial in larger projects where maintaining hundreds or thousands of lines of SQL would be unwieldy and error-prone.\n\n\nAdvantages and Trade-offs of ORM\nWhile ORM can greatly simplify database interactions and make your code more maintainable, it’s important to recognize the trade-offs:\nAdvantages:\n\nAbstraction: ORM hides the complexity of SQL, making database operations more intuitive.\nModularity: Code is organized into classes, making it easier to manage and extend.\nReusability: Methods can be reused across your application, reducing code duplication.\n\nTrade-offs:\n\nPerformance: In some cases, ORM may introduce a slight performance overhead compared to writing optimized raw SQL queries.\nComplexity: For very complex queries, the abstraction provided by ORM might make it harder to optimize or understand what’s happening under the hood.\nLearning Curve: If you’re new to object-oriented programming or R6, there may be a learning curve involved.\n\n\n\n\nSailing into the West: The Journey’s End and New Beginnings\nBy forging your own ORM with R6 in R, you gain a powerful toolset that brings order to your database interactions, much like the Rings of Power brought structure and control to Middle-earth. With this approach, you can build robust, maintainable, and scalable applications that harness the full potential of both R and SQL.\nJust as the journey of the Fellowship in “The Lord of the Rings” eventually led them to the Grey Havens — a place of reflection, peace, and new beginnings — so too does our exploration of SQL in R bring us to a moment of reflection and readiness for future adventures in data analysis.\nThroughout this journey, we’ve traversed the vast landscape of SQL and R, discovering how these two powerful tools can be harmonized to achieve greater efficiency, security, and clarity in managing and analyzing data. From the basics of establishing connections to databases, to the intricacies of secure query execution and the elegance of using dbplyr to bridge the gap between R’s tidyverse and SQL’s relational power, you’ve gained the knowledge to wield these tools like the Rings of Power.\nWe also delved into the challenges of handling large datasets, learning how to keep the heavy lifting on the database server and how to process data in manageable chunks to avoid being overwhelmed. The techniques shared are your tools to stand firm against the Balrog-like challenges that large datasets can pose.\nMoreover, we explored the deployment of SQL and R in production environments, ensuring that your workflows are robust, secure, and reliable. With best practices in automation, error handling, and monitoring, you are equipped to ensure that your data pipelines run as smoothly as a well-governed kingdom.\nFinally, we embraced the concept of ORM with R6, understanding how to encapsulate database interactions within object-oriented structures, much like forging your own Rings of Power. This approach not only streamlines your database operations but also opens up new possibilities for building scalable, maintainable, and modular applications.\nAs you sail into the West, leaving behind this foundational journey, remember that the end of one journey is merely the beginning of another. The skills and techniques you’ve acquired here are just the starting points for further exploration, deeper mastery, and more complex challenges. The realms of data are vast and ever-expanding, and with SQL and R by your side, you are well-prepared to venture into new territories, uncover hidden insights, and perhaps, discover your own unique path to data mastery.\nSo, whether you are analyzing data for business insights, developing data-driven applications, or simply exploring the vast possibilities that R and SQL offer together, may your journey be filled with discovery, growth, and the confidence that comes with knowing you are equipped with the tools to conquer any data challenge that lies ahead."
  },
  {
    "objectID": "ds/posts/2024-06-20_Writing-R-Code-the--Good-Way--f696c1cdc163.html",
    "href": "ds/posts/2024-06-20_Writing-R-Code-the--Good-Way--f696c1cdc163.html",
    "title": "Writing R Code the ‘Good Way’",
    "section": "",
    "text": "Embracing the Tidyverse Style Guide\n\n\n\nImage\n\n\nHey there, fellow R coder! When it comes to writing code in R, making it functional is just the beginning. Trust me, I’ve been there — debugging code at 2 AM, wondering what I was thinking when I wrote that line. This is where the Tidyverse Style Guide comes to the rescue, transforming your functional code into a masterpiece of readability and maintainability.\n\n\nWhy Coding Style Matters\nImagine reading a book with no punctuation or structure. Nightmare, right? The same goes for code. Good coding style ensures that your future self and your colleagues can comprehend and extend your work. As they say, “Today I know and God knows, but in a week only God will know how this should work.”\n\n\nFiles and Directories\n\nFile Naming Conventions\nProper file naming is crucial. Imagine rummaging through a folder named “folder2” — frustrating, right? Descriptive, meaningful names make it easier for others to understand the purpose of each file at a glance.\nGood Example:\ndata_analysis.R\nBad Example:\nData Analysis.R\nPros: Clear, concise, and consistent naming conventions make files easy to understand and manage, enhancing collaboration and avoiding issues with operating systems.\nCons: Inconsistent naming can lead to confusion, errors, and inefficiencies in managing and collaborating on projects.\n\n\nDirectory Structure\nA well-organized directory structure helps in navigating the project efficiently. It separates data, scripts, and results, making it easier to locate and manage files.\nGood Example:\nproject/\n├── data/\n├── scripts/\n└── results/\nBad Example:\nproject/\n├── folder1/\n├── folder2/\n└── random_folder/\nPros: A clear directory structure improves readability, navigation, and file management. It enhances collaboration by providing a standardized layout.\nCons: Poor organization leads to confusion, difficulty in finding files, increased errors, and reduced collaboration efficiency.\n\n\n\nSyntax\n\nIndentation and Spacing\nThink of indentation and spacing as the grammar of your code. Proper indentation and spacing make your code more readable and maintainable. The tidyverse style guide recommends using two spaces per indentation level and avoiding tabs.\nGood Example:\nif (condition) {\n  do_something()\n}\nBad Example:\nif(condition){\ndo_something()}\nPros: Using consistent indentation and spacing enhances readability and ensures that your code looks clean and professional. It makes it easier for others to follow your logic.\nCons: Inconsistent indentation makes the code hard to read and understand, leading to potential errors and misinterpretations.\n\n\nLine Length and Breaks\nKeeping lines under 80 characters and breaking lines after operators improve code readability, especially on smaller screens.\nGood Example:\nmy_function &lt;- function(arg1, arg2) {\n  long_expression &lt;- arg1 + \n    arg2\n  return(long_expression)\n}\nBad Example:\nmy_function &lt;- function(arg1, arg2) {\n  long_expression &lt;- arg1 + arg2\n  return(long_expression)\n}\nPros: Maintaining a maximum line length and breaking lines appropriately makes your code easier to read and prevents horizontal scrolling.\nCons: Ignoring this practice can lead to cramped and hard-to-follow code, making debugging and collaboration more challenging.\n\n\nNaming Conventions\nAdopting consistent naming conventions, such as snake_case for object names and UpperCamelCase for function names, helps in making the code more predictable and easier to understand.\nGood Example:\ndata_frame &lt;- data.frame(x = 1:10, y = 10:1)\nBad Example:\nDataFrame &lt;- data.frame(x = 1:10, y = 10:1)\nPros: Consistent naming conventions enhance readability and maintainability by providing a clear and predictable structure to your code. Cons: Inconsistent naming can cause confusion and errors, making it harder for others (and your future self) to understand and work with the code.\n\n\n\nFunctions\n\nWriting Functions\nFunctions should have clear, descriptive names and be designed to perform a single task. This improves readability and maintainability.\nGood Example:\nadd_numbers &lt;- function(a, b) {\n  return(a + b)\n}\nBad Example:\naddnumbers &lt;- function(a,b){return(a+b)}\nPros: Clear, descriptive names and single-task functions make code easier to understand and maintain.\nCons: Ambiguous names and multifunctional code increase complexity, making it harder to debug and extend.\n\n\nFunction Arguments\nUse default arguments where appropriate and document all arguments and return values. This makes functions more flexible and easier to use.\nGood Example:\nplot_data &lt;- function(data, x_col, y_col, color = \"blue\") {\n  plot(data[[x_col]], data[[y_col]], col = color)\n}\nBad Example:\nplot_data &lt;- function(data, x_col, y_col, color) {\n  plot(data[[x_col]], data[[y_col]], col = color)\n}\nPros: Default arguments provide flexibility and make functions easier to use. Proper documentation aids in understanding.\nCons: Lack of defaults and documentation can lead to misuse and confusion.\n\n\nReturn Values\nEnsure functions always return a value and that the return type is consistent. This makes the behavior of functions predictable and easier to debug.\nGood Example:\nadd_numbers &lt;- function(a, b) {\n  return(a + b)\n}\nBad Example:\nadd_numbers &lt;- function(a, b) {\n  result &lt;- a + b\n  # No return statement\n}\nPros: Consistent return values make functions predictable and easier to integrate.\nCons: Inconsistent or missing return values create ambiguity, making debugging and integration challenging.\n\n\n\nPipes\n\nUsing Pipes\nPipes, introduced by the magrittr package and widely used in the tidyverse, streamline code by chaining operations in a readable manner.\nGood Example:\nlibrary(dplyr)\ndata %&gt;%\n  filter(x &gt; 1) %&gt;%\n  summarise(mean_y = mean(y))\nBad Example:\nlibrary(dplyr)\nsummarise(filter(data, x &gt; 1), mean_y = mean(y))\nPros: Pipes enhance readability by breaking down operations into clear, sequential steps, making complex data transformations easier to follow. Cons: Without pipes, code becomes nested and harder to read, increasing the likelihood of errors and making debugging more difficult.\n\n\nPipe Practices\nTo ensure clarity, avoid performing complex operations within a single pipe chain. Instead, break down steps to maintain readability. This example is little bit exaggerated, because we have only 6 lines, but it is not unusual to have pipe made of 30 or more lines, and this rule should be used in that case.\nGood Example:\ndata_cleaned &lt;- data %&gt;%\n  filter(!is.na(x)) %&gt;%\n  mutate(z = x + y)\n\nresult &lt;- data_cleaned %&gt;%\n  group_by(category) %&gt;%\n  summarise(mean_z = mean(z))\nBad Example:\nresult &lt;- data %&gt;%\n  filter(!is.na(x)) %&gt;%\n  mutate(z = x + y) %&gt;%\n  group_by(category) %&gt;%\n  summarise(mean_z = mean(z))\nPros: Breaking down pipe chains improves readability and makes each step understandable and debuggable.\nCons: Long, complex pipes can be difficult to follow and troubleshoot, reducing code clarity and increasing maintenance difficulty.\n\n\n\nggplot2\n\nBreaking Code on Operators\nBreaking code on operators enhances readability and maintains a clean structure. This practice is particularly useful when dealing with long lines of code.\nGood Example:\nggplot(data, aes(x = x, y = y)) +\n  geom_point() +\n  theme_minimal() +\n  labs(title = \"Scatter Plot\", \n       x = \"X Axis\", \n       y = \"Y Axis\")\nPros: Each operation is on a new line, making the code easier to read and modify.\n\n\nProper Order of Layers\nMaintaining a proper order of layers in ggplot2 ensures that each layer is applied correctly, making the visualization more accurate and aesthetically pleasing.\nGood Example:\nggplot(data, aes(x = x, y = y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  theme_minimal()\nPros: The smoothing layer is applied on top of the points, and the theme is applied last, ensuring a clean and logical structure.\n\n\n\nDocumentation\n\nIn-Code Documentation\nIn-code documentation using comments helps others (and your future self) understand the logic and purpose of your code. It’s important to strike a balance between too many and too few comments.\nGood Example:\n# Calculate the mean of a numeric vector\ncalculate_mean &lt;- function(x) {\n  mean(x)\n}\nPros: Provides clear, concise information about the function’s purpose.\n\n\nRoxygen2 for Functions\nUsing Roxygen2 for documenting functions ensures comprehensive, consistent, and machine-readable documentation. This is particularly useful for creating package documentation.\nGood Example:\n#’ Calculate the mean of a numeric vector\n#’\n#’ @param x A numeric vector\n#’ @return The mean of the vector\n#’ @export\ncalculate_mean &lt;- function(x) {\n  mean(x)\n}\nPros: Provides a structured and detailed description, making it easy to generate documentation files automatically.\nGood in-code documentation and comprehensive function documentation using Roxygen2 enhance code readability, usability, and maintainability. Poor documentation leads to confusion, errors, and increased time spent understanding and debugging code.\n\n\n\nMiscellaneous Style Guidelines\n\nAssignment Using &lt;- Not =\nThe assignment operator &lt;- is preferred over = for clarity and consistency in R code.\nGood Example:\nx &lt;- 10\nPros: Clear distinction between assignment and equality checks.\n\n\nProper Spacing\nUsing proper spacing, especially near operators, enhances code readability.\nGood Example:\nresult &lt;- a + b\nPros: Improves readability and reduces errors.\n\n\nAvoiding Reserved Names\nAvoid using reserved names like c, T, or F as variable names to prevent conflicts with built-in functions and constants.\nGood Example:\nvec &lt;- c(1, 2, 3)\nPros: Avoids conflicts with the c() function.\n\n\nCode Organization\nOrganizing code using empty lines and breaking long lines helps in maintaining a clean and readable structure.\nGood Example:\ncalculate_sum &lt;- function(a, b) {\n  result &lt;- a + b\n  \n  return(result) \n}\nPros: Use of empty lines and line breaks improves readability and structure.\n\n\n\nConclusion\nBy embracing the Tidyverse Style Guide for R coding, you’re not just writing code; you’re crafting a readable, maintainable, and collaborative masterpiece. These guidelines will help you avoid those 2 AM debugging sessions and make your code a joy to work with. Consistent coding style reduces errors, improves project efficiency, and facilitates long-term maintenance. Embrace these guidelines to enhance your coding practices and project success. Happy coding, and remember, good style is the key to long-term coding happiness!\nPS. Ugly and unreadable code will work either way, but you will not like to work with this code."
  },
  {
    "objectID": "ds/posts/2024-05-09_The-Rebus-Code--Unveiling-the-Secrets-of-Regex-in-R-50f9ff52bdd9.html",
    "href": "ds/posts/2024-05-09_The-Rebus-Code--Unveiling-the-Secrets-of-Regex-in-R-50f9ff52bdd9.html",
    "title": "The Rebus Code: Unveiling the Secrets of Regex in R",
    "section": "",
    "text": "The Rebus Code: Unveiling the Secrets of Regex in R\n\n\n\nImage\n\n\nIn the intricate world of data analysis, the task of text pattern recognition and extraction is akin to unlocking a secret cipher hidden within ancient manuscripts. This is the realm of regular expressions (regex), a powerful yet often underappreciated tool in the data scientist’s toolkit. Much like the cryptex from Dan Brown’s “The Da Vinci Code,” which holds the key to unraveling historical and cryptic puzzles, regular expressions unlock the patterns embedded in strings of text data.\nHowever, the power of regex comes at a cost — its syntax is notoriously complex and can be as enigmatic as the riddles solved by Robert Langdon in his thrilling adventures. For those not versed in its arcane symbols, crafting regex patterns can feel like deciphering a code without a Rosetta Stone. This is where the rebus package in R provides a lifeline. It simplifies the creation of regex expressions, transforming them from a cryptic sequence of characters into a readable and manageable code, akin to translating a hidden message in an old relic.\nIn this tutorial, we embark on a journey akin to that of Langdon’s through Paris and London, but instead of ancient symbols hidden in art, we’ll navigate through the complexities of text data. We will explore the fundamental principles of regex that form the backbone of text manipulation tasks. From basic pattern matching to crafting intricate regex expressions with the rebus package, this guide will illuminate the path towards mastering regex in R, making the process as engaging as uncovering a secret passage in an ancient temple.\nJust as Langdon used his knowledge of symbolism to solve mysteries, we will use rebus to demystify regex in R, making this powerful tool accessible and practical for everyday data tasks. Whether you’re a seasoned data scientist or a novice in the field, understanding how to effectively use regex is like discovering a hidden map that leads to buried treasure, providing you with the insights necessary to make informed decisions based on your data.\nWith our thematic setting now established, let us delve deeper into the world of regular expressions and reveal how the rebus package can transform your approach to data analysis, turning a daunting task into an intriguing puzzle-solving adventure.\n\n\nUnveiling the Symbols\nRegular expressions operate through special characters that, when combined, form patterns capable of matching and extracting text with incredible precision. Here are a few fundamental symbols to understand:\n\nDot (.): Like the omnipresent eye in a Da Vinci painting, the dot matches any single character, except newline characters. It sees all but the end of a line.\nAsterisk (*): Mirroring the endless loops in a Fibonacci spiral, the asterisk matches the preceding element zero or more times, extending its reach across the string.\nPlus (+): This symbol requires the preceding element to appear at least once, much like insisting on the presence of a key motif in an artwork.\nQuestion Mark (?): It makes the preceding element optional, introducing ambiguity into the pattern, akin to an unclear symbol whose meaning might vary.\nCaret (^): Matching the start of a string, the caret sets the stage much like the opening scene in a historical mystery.\nDollar Sign ($): This symbol matches the end of a string, providing closure and ensuring that the pattern adheres strictly to the end of the text.\n\n\n\nExample: Simple Patterns in Action\nUsing the stringr library enhances readability and flexibility in handling regular expressions. Let’s apply this to find specific patterns:\nlibrary(stringr)\ntext_vector &lt;- c(\"Secrets are hidden within.\", \"The key is under the mat.\", \n                 \"Look inside, find the truth.\", \"Bridge is damaged by the storm\")\nstr_detect(text_vector, \"\\\\bis\\\\b\")\nThis code chunk checks if the word “is” is anywhere in the given sentence.\n\n\nCrafting Your First Regex\nTo identify any word that ends with ‘ed’, signaling past actions, akin to uncovering traces of events long gone:\n# Match words ending with 'ed'\nstr_extract(text_vector, \"\\\\b[A-Za-z]+ed\\\\b\")\nThis expression uses \\\\b to ensure that ‘ed’ is at the end of the word, capturing complete words and not fragments—critical when every detail in a coded message matters.\n\n\nDeciphering a Complex Regex\nLet’s consider a more intricate regex pattern:\ndate_pattern &lt;- \"\\\\b(0[1-9]|[12][0-9]|3[01])[- /.](0[1-9]|1[012])[- /.](19|20)\\\\d\\\\d\\\\b\"\n# first check if pattern is present\nstr_detect(\"She was born on 12/08/1993, and he on 04/07/1989.\", date_pattern)\n\n# second extract the pattern\nstr_extract_all(\"She was born on 12/08/1993, and he on 04/07/1989.\", date_pattern)\nThis regex looks extremely unfriendly at first glance, resembling an arcane code more than a helpful tool. It uses capturing groups, ranges, and alternations to accurately match dates in a specific format. Here’s the breakdown:\n\n\\b: Word boundary, ensuring we match whole dates.\n(0[1–9]|[12][0–9]|3[01]): Matches days from 01 to 31.\n[- /.]: Matches separators which can be a dash, space, dot, or slash.\n(0[1–9]|1[012]): Matches months from 01 to 12.\n(19|20)\\d\\d: Matches years from 1900 to 2099.\n\nThis example shows how raw regex can quickly become complex and hard to follow, much like a cryptic puzzle waiting to be solved. The rebus package can help simplify these expressions, making them more accessible and easier to manage.\n\n\nBuilding Blocks of Rebus\nJust as Robert Langdon in “The Da Vinci Code” used his knowledge of symbology to decode complex historical puzzles, the rebus package in R enables us to build regular expressions from understandable components, transforming arcane syntax into legible code. This approach not only simplifies regex creation but also enhances readability and maintenance, making regex patterns as approachable as reading a museum guidebook.\n\n\nAssembling the Codex\nRebus operates on the principle of constructing regex patterns piece by piece using function calls, which represent different regex components. This method aligns with piecing together clues from a scattered array of symbols to form a coherent understanding. Here are some of the building blocks provided by rebus:\n\ndigit(): Matches any number, simplifying digit recognition.\nor(): Specifies a set of characters to match, allowing customization akin to selecting specific tools for a dig site.\n\n\n\nExample: Email Pattern Construction with Rebus\nCrafting an email validation pattern with rebus is akin to assembling a puzzle where each piece must fit precisely:\nlibrary(rebus)\n\n# Define the pattern for a standard email\nemail_pattern &lt;- START %R%\n  one_or_more(WRD) %R% \"@\" %R%\n  one_or_more(WRD) %R% DOT %R%\n  or(\"com\", \"org\", \"net\")\n\n# Use the pattern to find valid emails\nsample_text &lt;- c(\"contact@example.com\", \"hello@world.net\", \"not-an-email\")\nstr_detect(sample_text, email_pattern)\nThis pattern, built with rebus functions, makes it easy to understand at a glance which components form the email structure, demystifying the regex pattern much like Langdon revealing the secrets behind a hidden inscription.\n\n\nDeciphering Complex Text Patterns with Rebus\nConsider a more complicated scenario where you need to validate date formats within a text. Using basic regex might involve a lengthy and cryptic pattern, but with rebus, we can construct it step-by-step:\n# Define a pattern for dates in the format DD/MM/YYYY\ndate_pattern &lt;- \n  digit(2) %R% \"/\" %R%\n  digit(2) %R% \"/\" %R%\n  digit(4) \n\n# Sample text for pattern matching\ndates_text &lt;- \"Important dates are 01/01/2020 and 31/12/2020.\"\n\n# First check if pattern can be found in text.\nstr_detect(dates_text, date_pattern)\n\n# Then what it extracts.\nstr_extract_all(dates_text, date_pattern)\nThis example shows how rebus simplifies complex regex tasks, turning them into a series of logical steps, much like solving a riddle in an ancient tome.\nBut wait a minute… It is always a good idea to dig in documentation, and check out what can be found there.\ndmy_pattern = DMY\n\nstr_detect(dates_text, dmy_pattern)\nstr_extract_all(dates_text, dmy_pattern)\n\n\nTips for Crafting Expressions with Rebus\nWhile rebus makes it easier to create and understand regex patterns, there are tips to further enhance your mastery:\n\nStart Simple: Begin with basic components and gradually add complexity.\nTest Often: Use sample data to test and refine your patterns frequently.\nComment Your Code: Annotate your rebus expressions to explain the purpose of each component, especially in complex patterns.\n\n\n\nExtracting Complex Medical Data from Clinical Notes\nIn the vein of a detective novel, akin to “The Da Vinci Code,” where each clue unravels a part of a larger mystery, this scenario involves deciphering clinical notes to extract specific medical information. This requires a keen understanding of the text’s structure and content, mirroring the precision needed to solve a cryptic puzzle left in an ancient artifact.\n\n\nSetting the Scene: Medical Data Extraction Challenge\nClinical notes are packed with crucial medical details in a format that is often not standardized, making the extraction of specific information like medication prescriptions and patient diagnoses a complex task. Our goal is to develop regex patterns that can accurately identify and extract this information from varied text formats.\n\n\nStep-by-Step Pattern Construction Using Rebus\n\nDefine Complex Patterns:\n\nMedications often mentioned with dosages and frequencies.\nDiagnoses that may include medical terms and conditions.\n\nlibrary(rebus)\nlibrary(stringr)\n\n# Pattern for medication prescriptions\n# Example format: [Medication Name] [Dosage in mg] [Frequency]\nmedication_pattern &lt;- one_or_more(WRD) %R% SPACE %R% one_or_more(DGT) %R% \"mg\" %R% SPACE %R% one_or_more(WRD)\n\n# Pattern for diagnoses\n# Example format: Diagnosed with [Condition]\ndiagnosis_pattern &lt;- \"Diagnosed with \" %R% one_or_more(WRD %R% optional(SPACE %R% WRD))\n\nclinical_notes &lt;- c(\"Patient was prescribed Metformin 500mg twice daily for type 2 diabetes.\",\n                    \"Diagnosed with Chronic Heart Failure and hypertension.\",\n                    \"Amlodipine 10mg once daily was recommended.\",\n                    \"Review scheduled without any new prescriptions.\")\n\n\n\nSample Clinical Notes:\nclinical_notes &lt;- c(\"Patient was prescribed Metformin 500mg twice daily for type 2 diabetes.\",\n                    \"Diagnosed with Chronic Heart Failure and hypertension.\",\n                    \"Amlodipine 10mg once daily was recommended.\",\n                    \"Review scheduled without any new prescriptions.\")\n\n\nExtract and Validate Medical Data:\n# Extracting medication details\nmedication_details &lt;- str_extract_all(clinical_notes, medication_pattern)\n\n# Extracting diagnoses\ndiagnoses_found &lt;- str_extract_all(clinical_notes, diagnosis_pattern)\n\n\nExample: Advanced Code Walkthrough\nBy running the above patterns against the clinical notes, we extract structured information about medications and diagnoses:\nprint(medication_details)\n\n[[1]]\n[1] \"Metformin 500mg twice\"\n\n[[2]]\ncharacter(0)\n\n[[3]]\n[1] \"Amlodipine 10mg once\"\n\n[[4]]\ncharacter(0)\n\nprint(diagnoses_found)\n\n[[1]]\ncharacter(0)\n\n[[2]]\n[1] \"Diagnosed with Chronic Heart Failure and hypertension\"\n\n[[3]]\ncharacter(0)\n\n[[4]]\ncharacter(0)\nThis code extracts arrays containing detailed medication prescriptions and diagnosed conditions from each note, if available.\n\n\nHandling Edge Cases and Variability\nMedical terms and prescriptions can vary greatly:\n\nExpand Vocabulary in Rebus: Include variations and synonyms of medical conditions and medication names.\nAdjust for Complex Dosage Instructions: Medications might have dosages described in different units or intervals.\n\n\n\nMastering Medical Data Extraction\nJust as each puzzle piece in “The Da Vinci Code” led to deeper historical insights, each regex pattern crafted with rebus reveals vital medical information from clinical notes, enabling better patient management and data-driven decision-making in healthcare.\n\n\nMastering Regex with Rebus for Complex Data Extraction\nNavigating through complex data with regex and the rebus package is akin to deciphering hidden codes and symbols in a Dan Brown novel. Just as Robert Langdon uses his knowledge of symbology to unravel mysteries in “The Da Vinci Code,” data scientists and analysts use regex patterns crafted with rebus to unlock the mysteries within their data sets. This guide has shown how rebus transforms an intimidating script into a manageable and understandable set of building blocks, enabling precise data extraction across various domains, from legal documents to medical records.\n\n\nFinal Thoughts: The Art of Regex Crafting\n\nIterative Development: Like solving a cryptic puzzle, developing effective regex patterns often requires an iterative approach. Start with a basic pattern, test it, refine it based on the outcomes, and gradually incorporate complexity as needed.\nComprehensive Testing: Ensure your regex patterns perform as expected across all possible scenarios. This includes testing with diverse data samples to cover all potential variations and edge cases, mirroring the meticulous verification of clues in a historical investigation.\nDocumentation and Comments: Regex patterns, especially complex ones, can quickly become inscrutable. Document your patterns and use comments within your rebus expressions to explain their purpose and structure. This practice ensures that your code remains accessible not just to you but to others who may work on it later, much like leaving a detailed map for those who follow in your footsteps.\nStay Updated: Just as new archaeological discoveries can change historical understandings, advancements in programming and new versions of packages like rebus can introduce more efficient ways to handle data. Keeping your skills and knowledge up to date is crucial.\nShare Knowledge: Just as scholars share their discoveries and insights, sharing your challenges and solutions in regex with the community can help others. Participate in forums, write blogs, or give talks on your regex strategies and how you’ve used rebus to solve complex data extraction problems.\n\n\n\nStrategies for Employing rebus Effectively\n\nUtilize rebus Libraries: Leverage the full suite of rebus functionalities by familiarizing yourself with all its helper functions and modules. Each function is designed to simplify a specific aspect of regex pattern creation, which can drastically reduce the complexity of your code.\nPattern Modularity: Build your regex patterns in modular chunks using rebus, similar to constructing a narrative or solving a multi-part puzzle. This approach not only simplifies the development and testing of regex patterns but also enhances readability and maintenance.\nAdvanced Matching Techniques: For highly variable data, consider advanced regex features like lookaheads, lookbehinds, and conditional statements, which can be integrated into your rebus patterns. These features allow for more dynamic and flexible pattern matching, akin to adapting your hypothesis in light of new evidence.\n\n\n\nEpilogue: The Power of Clarity in Data Parsing\nIn conclusion, mastering rebus and regex is like becoming fluent in a secret language that opens up vast archives of data, ready to be explored and understood. This guide has equipped you with the tools to start this journey, providing the means to reveal the stories hidden within complex datasets, enhance analytical accuracy, and drive insightful decisions.\nJust as every clue solved brings Langdon closer to the truth in “The Da Vinci Code,” each pattern you decipher with rebus brings you closer to mastering the art of data. The path is laid out before you—begin your adventure, solve the puzzles, and unlock the potential of your data with confidence.\n\n\nAppendix: The Regex Rosetta Stone — A Comprehensive Reference Guide\nThis appendix is designed as a quick yet comprehensive reference guide to using the rebus package for crafting regex expressions in R. Here you will find a brief description of some of the most pivotal functions, character classes, ready-made patterns, and interesting trivia on less commonly used regex features.\n\n\n1. Most Common Functions in rebus\nLet’s explore some of the essential rebus functions that you can use to construct regex patterns more intuitively:\n\nor(): Combines multiple patterns and matches any of them. Useful for alternatives in a pattern.\nexactly(): Specifies that the preceding element should occur an exact number of times.\nliteral(): Treats the following string as literal text, escaping any special regex characters.\noptional(): Indicates that the preceding element is optional, matching it zero or one time.\nzero_or_more(): Matches zero or more occurrences of the preceding element.\none_or_more(): Matches one or more occurrences of the preceding element.\nlookahead(): Checks for a match ahead of the current position without consuming characters.\nlookbehind(): Asserts something to be true behind the current position in the text.\nrepeated(): Matches a specified number of repetitions of the preceding element.\nwhole_word(): Ensures that the pattern matches a complete word.\n\n\n\n2. Most Common Character Classes\nCharacter classes simplify the specification of a set of characters to match:\n\nDGT (Digit): Matches any digit, shorthand for digit().\nALNUM (Alphanumeric): Matches any alphanumeric character.\nLOWER: Matches any lowercase letter.\nUPPER: Matches any uppercase letter.\nSPECIALS: Matches any special characters typically found on a keyboard.\nROMAN: Matches Roman numerals.\nPUNCT (Punctuation): Matches any punctuation character.\nNOT_DGT (Not Digit): Matches any character that is not a digit.\nHEX_DIGIT (Hexadecimal Digit): Matches hexadecimal digits (0-9, A-F).\nKATAKANA, HIRAGANA: Matches characters from the Japanese Katakana and Hiragana scripts.\nHEBREW, CYRILLIC, ARABIC: Matches characters from the Hebrew, Cyrillic, and Arabic scripts.\n\n\n\n3. Ready Patterns\nrebus also includes functions for common pattern templates:\n\nYMD: Matches dates in Year-Month-Day format.\nTIME: Matches time in HH:MM:SS format.\nAM_PM: Matches time qualifiers AM or PM.\nCURRENCY_SYMBOLS: Matches common currency symbols.\nHOUR12: Matches hour in 12-hour format.\n\n\n\n4. Interesting But Less Used Character Classes (Trivia)\nExplore some unique and less commonly used character classes:\n\nDOMINO_TILES: Matches Unicode representations of domino tiles.\nPLAYING_CARDS: Matches Unicode characters representing playing cards.\n\nThese unique character classes add a fun and often surprising depth to regex capabilities, allowing for creative data parsing and matching scenarios, much like uncovering an unexpected twist in a puzzle or story.\nBy familiarizing yourself with these tools, you can significantly enhance your ability to analyze and manipulate data effectively, transforming complex text into structured and insightful information. Keep this guide handy as a reference to navigate the vast landscape of regex with confidence and precision.\n\n\nFinal Tip:\nIf you haven’t already noted it, there is one small trick that will help you make step from using rebus to use “vanilla” regular expressions. When you place pattern in variable in your environment it is storing it as real RegExp, so if you would like to see it, and maybe use it directly in code, just print it to console.\n# Imagine that there is some official number that consists of following parts\n# Date in format YYYYMMDD, then letter T, then time in format HHMMSS and indicator AM or PM\n# Looks pretty simple, and indeed is using rebus\n\npattern = YMD %R% \"T\" %R% HMS %R% AM_PM\n\n# Now look to raw RegExp version.\nprint(pattern)\n# [0-9]{1,4}[-/.:,\\ ]?(?:0[1-9]|1[0-2])[-/.:,\\ ]?(?:0[1-9]|[12][0-9]|3[01])T(?:[01][0-9]|2[0-3])[-/.:,\\ ]?[0-5][0-9][-/.:,\\ ]?(?:[0-5][0-9]|6[01])(?:am|AM|pm|PM)\n\nvalid =  \"20180101T120000AM\"\n\nstr_detect(valid, pattern)\n# [1] TRUE"
  },
  {
    "objectID": "ds/posts/2024-04-11_Crafting-Elegant-Scientific-Documents-in-RStudio--A-LaTeX-and-R-Markdown-Tutorial-a5b788d8a38d.html",
    "href": "ds/posts/2024-04-11_Crafting-Elegant-Scientific-Documents-in-RStudio--A-LaTeX-and-R-Markdown-Tutorial-a5b788d8a38d.html",
    "title": "Crafting Elegant Scientific Documents in RStudio: A LaTeX and R Markdown Tutorial",
    "section": "",
    "text": "Image\n\n\n\nIntroduction\nIn the world of scientific research and academic writing, the clarity, precision, and aesthetics of your documents can significantly impact their reception and comprehension. LaTeX, a powerful typesetting system, has long been revered for its ability to create beautifully formatted documents, especially those requiring complex mathematical expressions and detailed layouts. However, the steep learning curve associated with LaTeX can deter many. Enter R Markdown, a tool that simplifies the creation of dynamic documents, presentations, and reports directly from R code. When combined with the versatility of RStudio, it offers a more accessible entry point into the world of LaTeX, without sacrificing the depth and precision that professional documents require.\nThis tutorial aims to bridge the gap between the high-quality typesetting capabilities of LaTeX and the dynamic, code-integrated documentation of R Markdown. Whether you’re compiling research findings, drafting an academic paper, or preparing a report with rich data visualizations, integrating LaTeX with R Markdown in RStudio enhances both the appearance and functionality of your work. By the end of this guide, you’ll be equipped with the knowledge to leverage the best of both worlds, crafting documents that stand out for their elegance and precision.\n\n\nPrerequisites and Setup\n\nInstalling RStudio and LaTeX\nBefore we dive into the intricacies of combining LaTeX with R Markdown, let’s ensure you have all the necessary tools installed. RStudio is an indispensable IDE for anyone working with R, and it provides seamless support for R Markdown. LaTeX, on the other hand, is a typesetting system that excels in document preparation, especially for those containing complex mathematical formulas.\n\nRStudio: If you haven’t already, download and install RStudio. Choose the version appropriate for your operating system.\nLaTeX Distribution: For LaTeX, you need a distribution based on your operating system. Windows users can opt for MiKTeX, macOS users for MacTeX, and Linux users for TeX Live. Installation links and instructions are readily available on their respective websites.\n\nAfter installing both RStudio and your LaTeX distribution, ensure that RStudio can locate your LaTeX installation. This integration is typically automatic, but you can verify or adjust the settings in RStudio by navigating to Tools &gt; Global Options &gt; Sweave.\n\n\nConfiguring RStudio for LaTeX and R Markdown\nWith RStudio and LaTeX installed, the next step is to configure your RStudio environment for an optimal working experience. This involves:\n\nInstalling Necessary R Packages: Open RStudio and install the rmarkdown package, which supports the integration of R code with Markdown (and by extension, LaTeX) for dynamic document generation. Install it by running:\n\ninstall.packages(\"rmarkdown\")\n\nTesting Your Setup: To confirm everything is set up correctly, create a new R Markdown document. Go to File &gt; New File &gt; R Markdown…, then choose PDF as the output format. This action requires LaTeX for PDF generation, so if it succeeds without errors, your setup is correct.\n\nThis section’s goal is to ensure you have a smooth start with all the necessary tools at your disposal. Once you’re set up, the real fun begins: exploring the synergy between LaTeX and R Markdown to create stunning scientific documents.\n\n\n\nYour First R Markdown Document with LaTeX\nCreating your first R Markdown document integrated with LaTeX in RStudio is a simple yet exciting process. This section will guide you through creating a basic document, adding LaTeX for formatting and equations, and generating a PDF output.\n\nCreating an R Markdown Document\n\nStart a New R Markdown File: In RStudio, go to File &gt; New File &gt; R Markdown… This opens a dialog where you can set the document’s title and output format. For now, select PDF and click OK.\nExplore the Default Content: RStudio will generate a sample document filled with some basic Markdown content and example code chunks. This template serves as an excellent introduction to R Markdown’s capabilities.\n\n\n\nIntegrating Basic LaTeX Elements\nWithin your R Markdown document, you can start integrating LaTeX directly. Here’s how you can add some basic LaTeX commands for text formatting and sections:\nThis is an R Markdown document with \\LaTeX. Markdown allows you to write using an easy-to-read, easy-to-write plain text format, which then converts to \\LaTeX for high-quality document production.\n\n\\section{Introduction}\nThis is a section created using LaTeX.\n\n\\subsection{Background}\nThis subsection provides background information, also formatted using LaTeX.\n\n\\textbf{Bold text} and \\textit{italicized text} can easily be added with LaTeX commands.\n\n\nAdding Mathematical Expressions\nOne of LaTeX’s strengths is its ability to format complex mathematical expressions beautifully. In R Markdown, you can include these expressions by enclosing them in dollar signs for inline equations or double dollar signs for displayed equations:\nHere is an inline equation: \\(E=mc^2\\).\n\nAnd a displayed equation:\n\n$$\na^2 + b^2 = c^2\n$$\n\n\nCompiling to PDF\nAfter adding your content, compile the document to PDF by clicking the “Knit” button in RStudio and selecting PDF. RStudio will use LaTeX to process your document, incorporating any LaTeX commands or mathematical expressions you’ve included, and generate a PDF.\n\n\n\nImage\n\n\nThis simple exercise demonstrates the power of combining R Markdown’s dynamic capabilities with LaTeX’s typesetting prowess, all within the RStudio environment. Whether you’re documenting research findings, drafting a paper, or preparing a report, this approach allows you to create professional, elegantly formatted documents efficiently.\n\n\n\nAdvanced LaTeX Features in R Markdown\nHaving grasped the basics of integrating LaTeX into R Markdown documents, we’ll now delve into advanced features to further elevate your scientific document’s quality. This segment highlights enhanced figure and table management, utilizing custom LaTeX commands, and effectively handling bibliographies within RStudio.\n\nWorking with Figures and Tables\nLaTeX is renowned for its precise control over figures and tables, but in R Markdown, we approach these elements differently, leveraging Markdown and R code chunks for dynamic content integration and formatting.\nFigures\nFor static images, use Markdown syntax:\n![Caption for the figure.](my_address_to_logo){width=20%}\nFor dynamically generated figures from R:\n{r label, echo=FALSE, fig.cap=\"Caption for the figure.\"}\ndata(mtcars)\nplot(mtcars$wt, mtcars$mpg)\n\n\n\nImage\n\n\nTables\nTo create detailed and customizable tables in your R Markdown document using LaTeX, you’ll directly use the tabular environment provided by LaTeX. This allows for precise control over the table’s appearance, alignment, and overall structure. Here’s a basic example of creating a table with LaTeX:\n\\begin{table}[h]\n\\centering\n\\caption{Sample Data Table}\n\\begin{tabular}{lcr}\n\\hline\n\\textbf{Left Align} & \\textbf{Center} & \\textbf{Right Align} \\\\\n\\hline\nData 1 & Data 2 & Data 3 \\\\\nMore & Data & Here \\\\\n\\hline\n\\end{tabular}\n\\label{tab:sample_table}\n\\end{table}\nThis LaTeX code snippet places a table with headers aligned to the left, center, and right. The \\hline command creates horizontal lines for clarity, and \\textbf is used for bold header text. The \\caption{} and \\label{} commands are used for the table’s caption and referencing it in the text, respectively.\n\n\nDefining and Using Custom LaTeX Commands\nYou can define custom LaTeX commands for repetitive tasks or to simplify complex formatting. Custom commands are defined in the YAML header of your R Markdown document using header-includes:\nheader-includes:\n  - \\newcommand{\\highlight}[1]{\\textbf{\\textcolor{red}{#1}}}\nThis command, \\highlight{}, makes specified text bold and red. To use this command within your document:\nThis is regular text and this is \\highlight{highlighted text}.\n\n\nApplying Custom Commands in Tables\nYour custom LaTeX commands can be utilized within tables to emphasize specific pieces of data or apply consistent formatting. Using the previously defined \\highlight{} command:\n\\begin{table}[h]\n\\centering\n\\caption{Demonstrating Custom Commands in Tables}\n\\begin{tabular}{lc}\n\\hline\n\\textbf{Description} & \\textbf{Data} \\\\\n\\hline\nRegular Data & 123 \\\\\nHighlighted Data & \\highlight{456} \\\\\n\\hline\n\\end{tabular}\n\\label{tab:custom_command_table}\n\\end{table}\nThis example shows how to apply the \\highlight{} command within a table to make specific data stand out.\n\n\n\nImage\n\n\nIn this chapter, we’ve explored how to enhance your R Markdown documents with figures and sophisticated table formatting using LaTeX and the creation and application of custom LaTeX commands. Starting with the tabular environment, we demonstrated the method to craft detailed tables that meet specific aesthetic and structural requirements. Additionally, we covered how to define and utilize custom LaTeX commands within your document, allowing for efficient and consistent formatting across your scientific documents. This approach ensures that your work not only conveys information effectively but also adheres to the high standards of professional and academic presentation.\n\n\n\nCrafting Complex Scientific Equations with LaTeX in R Markdown\nThe seamless integration of LaTeX within R Markdown particularly shines when dealing with complex scientific equations, which are cumbersome, if not impossible, to accurately represent in plain text or basic Markdown. LaTeX provides a comprehensive set of tools for typesetting mathematical expressions, from simple fractions to elaborate equations used in advanced physics and mathematics. This chapter demonstrates how to leverage LaTeX for this purpose within an R Markdown document.\n\nBasic Mathematical Expressions\nLaTeX allows for the inline and block display of mathematical expressions. For inline equations, enclose your LaTeX code in single dollar signs ($), and for equations that should be displayed as a separate block, use double dollar signs ($$).\nInline Equation:\nEinstein's famous equation can be represented inline as $E=mc^2$.\nDisplayed Equation:\n$$E=mc^2$$\nThis displays the equation centered on its own line, making it stand out for emphasis.\n\n\nAdvanced Equation Formatting\nLaTeX excels in formatting complex equations, such as systems of equations, matrices, and functions involving sums, integrals, and limits.\nSystem of Equations:\n$$\n\\begin{align*}\nx + y &= 10 \\\\\n2x - y &= 4\n\\end{align*}\n$$\nMatrix:\n$$\n\\begin{pmatrix}\na & b \\\\\nc & d\n\\end{pmatrix}\n$$\nIntegral:\n$$\n\\int_0^\\infty e^{-x}dx\n$$\nThese examples demonstrate just a fraction of the capabilities LaTeX offers for mathematical typesetting. When utilized within R Markdown, it enables authors to seamlessly integrate complex mathematical content into their documents, enhancing both readability and professionalism.\n\n\nUtilizing LaTeX for Scientific Notation\nScientific documents often require notation that is difficult or awkward to express in other formats. LaTeX addresses this with a broad array of symbols and structures designed specifically for scientific writing:\n$$\n\\gamma + \\pi \\approx 3.14 \\text{, where } \\gamma \\text{ is the Euler-Mascheroni constant, and } \\pi \\text{ is the mathematical constant pi.}\n$$\nThe combination of R Markdown and LaTeX provides a powerful toolset for scientists, mathematicians, and anyone else working with complex equations or scientific notation. It brings together the best of both worlds: the dynamism and reproducibility of R Markdown with the precise typesetting and extensive capabilities of LaTeX.\n\n\nSome more complex equations\nFourier Series:\n$$\nf(x) = a_0 + \\sum_{n=1}^{\\infty} \\left( a_n \\cos \\frac{2\\pi nx}{P} + b_n \\sin \\frac{2\\pi nx}{P} \\right)\n$$\nSchrodinger equation:\n$$\ni\\hbar\\frac{\\partial}{\\partial t}\\Psi(\\mathbf{r}, t) = \\left[ \\frac{-\\hbar^2}{2\\mu}\\nabla^2 + V(\\mathbf{r}, t) \\right] \\Psi(\\mathbf{r}, t)\n$$\nGeneral relativity field equation:\n$$\nG_{\\mu\\nu} + \\Lambda g_{\\mu\\nu} = \\frac{8\\pi G}{c^4} T_{\\mu\\nu}\n$$\nNavier-Stokes Equations for Fluid Dynamics:\n$$\n\\rho \\left( \\frac{\\partial \\mathbf{v}}{\\partial t} + \\mathbf{v} \\cdot \\nabla \\mathbf{v} \\right) = -\\nabla p + \\mu \\nabla^2 \\mathbf{v} + \\mathbf{f}\n$$\nAnd render of all equations included in chapter.\n\n\n\nImage\n\n\n\n\n\nCompiling Documents and Customizing Outputs in R Markdown\nR Markdown provides a seamless workflow for creating dynamic documents, reports, presentations, and more, directly from R. When incorporating LaTeX, you gain additional control over the document’s appearance, enabling the creation of professional-grade scientific documents. This chapter explores how to compile your R Markdown documents into PDFs, leveraging LaTeX for advanced formatting, and how to customize these outputs to fit various academic and professional standards.\n\nCompiling R Markdown Documents to PDF\nTo compile an R Markdown document to PDF with LaTeX formatting:\n\nEnsure LaTeX is Installed: Before compiling, make sure you have a LaTeX distribution installed on your computer, as discussed in the setup chapter.\nUse the ‘Knit’ Button: In RStudio, the simplest way to compile your document is by using the Knit button. When you click Knit, RStudio automatically renders your document into a PDF, incorporating any LaTeX code or styling you’ve included.\nCustomizing the Build Process: For more control over the compilation process, you can use the rmarkdown::render() function in the R console:\n\nrmarkdown::render(\"your_document.Rmd\", output_format = \"pdf_document\")\nThis function allows for additional arguments and customization, offering more flexibility than the Knit button.\n\n\nCustomizing PDF Output with LaTeX\nLaTeX allows for extensive customization of PDF output through the use of packages and settings defined in the preamble of your R Markdown document. Here are a few ways to customize your PDF documents:\n\nPage Layout and Fonts: Use LaTeX packages such as geometry to adjust margins, fancyhdr for custom headers and footers, and fontspec for font customization.\n\nheader-includes:\n  - \\usepackage{geometry}\n  - \\geometry{left=3cm,right=3cm,top=2cm,bottom=2cm}\n  - \\usepackage{fancyhdr}\n  - \\pagestyle{fancy}\n  - \\usepackage{fontspec}\n  - \\setmainfont{Times New Roman}\n\nSection Formatting: Customize section titles using the titlesec package.\n\nheader-includes:\n  - \\usepackage{titlesec}\n  - \\titleformat*{\\section}{\\Large\\bfseries}\n\nIncluding External LaTeX Files: For complex documents, you might want to maintain your LaTeX preamble in a separate .tex file and include it in your R Markdown document.\n\nheader-includes:\n  - \\input{preamble.tex}\n\n\nAdvanced Document Features\nLeveraging LaTeX within R Markdown also allows for the inclusion of advanced document features that are typically challenging to implement, such as conditional text rendering, custom automatic numbering for figures and tables, and intricate mathematical typesetting, which we’ve covered in the previous chapter.\nThe combination of R Markdown and LaTeX offers unparalleled flexibility and power for scientific document creation. By mastering the compilation process and customizing the output, you can produce documents that not only meet the rigorous standards of academic and professional communication but also reflect your personal style and preferences.\n\n\n\nFurther Resources for Mastering LaTeX in R Markdown\nHaving explored the fundamentals and some advanced techniques for integrating LaTeX into R Markdown documents, it’s beneficial to know where to look for further information, tutorials, and community support to continue enhancing your skills. This final chapter provides a curated list of resources, including books, online tutorials, forums, and packages, designed to deepen your understanding and proficiency in using LaTeX with R Markdown for creating professional and sophisticated documents.\n\nBooks\n\n“R Markdown: The Definitive Guide” by Yihui Xie, J.J. Allaire, and Garrett Grolemund. This comprehensive guide provides a thorough introduction to R Markdown, including its integration with LaTeX for producing high-quality documents.\n“The LaTeX Companion” by Frank Mittelbach and Michel Goossens. A detailed reference book for LaTeX users, covering a wide range of topics from basic document formatting to more complex customizations and extensions.\n“Practical R Markdown” by Benjamin Soltoff. This book focuses on the practical aspects of using R Markdown in research and data analysis, with sections dedicated to integrating LaTeX for academic writing.\n\n\n\nOnline Tutorials and Guides\n\nOverleaf’s LaTeX Tutorials: Overleaf offers a comprehensive series of tutorials for LaTeX beginners and advanced users alike, covering everything from basic document structure to complex mathematical typesetting.\nRStudio’s R Markdown Documentation: The official R Markdown website by RStudio provides extensive documentation, tutorials, and galleries of examples to help users harness the full potential of R Markdown, including its LaTeX capabilities.\n\n\n\nCommunity Forums and Support\n\nStack Exchange TeX — LaTeX Stack Exchange: A question and answer site for users of TeX, LaTeX, ConTeXt, and related typesetting systems. It’s an excellent resource for getting help with specific LaTeX questions or issues.\nRStudio Community: The RStudio Community forum is a great place to ask questions and share insights about using R Markdown and LaTeX.\n\n\n\nPackages and Tools\n\ntinytex: An R package that provides a lightweight, portable, and easy-to-maintain LaTeX distribution. It’s specifically designed to simplify the management of LaTeX distributions in R Markdown workflows.\nLaTeX Workshop for Visual Studio Code: For users who prefer Visual Studio Code as their editor, this extension enhances the LaTeX experience with features like build automation, comprehensive linting, and preview.\n\nWhile we’ve covered substantial ground in this guide, the journey to mastering LaTeX in R Markdown is ongoing. The resources listed in this chapter offer pathways to further exploration and mastery. Whether you’re looking to refine your document designs, tackle complex typesetting challenges, or simply stay updated on new packages and features, the LaTeX and R Markdown communities offer a wealth of knowledge and support.\nRemember, the key to proficiency in LaTeX and R Markdown is practice and engagement with the community. Don’t hesitate to experiment with your documents, ask questions, and share your knowledge with others. With these resources at your disposal, you’re well-equipped to take your document creation skills to new heights."
  },
  {
    "objectID": "ds/posts/2023-11-23_Object-Oriented-Express--Refactoring-in-R-3b33b728042b.html",
    "href": "ds/posts/2023-11-23_Object-Oriented-Express--Refactoring-in-R-3b33b728042b.html",
    "title": "Object-Oriented Express: Refactoring in R",
    "section": "",
    "text": "The Journey to OOP in R\n\n\n\nImage\n\n\nIn the world of programming, embarking on the path of Object-Oriented Programming (OOP) is akin to boarding a high-speed train towards more structured, efficient, and maintainable code. As we continue our series, our next stop is the “Object-Oriented Express,” where we delve into the transformative power of OOP in the R programming language. This journey isn’t just about adopting a new syntax; it’s about embracing a new mindset that revolves around objects and classes, a stark contrast to the procedural paths we’ve treaded so far.\nThe protagonist of our story, the data_quality_report() function, has served us well in its procedural form. However, as the complexity of our data analysis tasks grows, so does the need for a more scalable and maintainable structure. By refactoring this function into an R6 class, we will not only improve its organization but also enhance its functionality and extendibility. This transition to OOP will illustrate how your R code can evolve from a linear script to an elegant symphony of interacting objects and methods, each playing a specific role in the data analysis orchestra.\n\n\nRefactoring with R6 Classes\nOur journey into OOP begins with the foundational step of refactoring our existing data_quality_report() function into an R6 class. R6 classes in R represent a more advanced and versatile system for OOP, offering both the power of encapsulation and the flexibility of reference semantics.\n\nDefining the R6 Class\nWe start by defining the structure of our new class. This class will encapsulate all functionalities of our original function, transforming them into methods — functions that belong to and operate on the class itself.\nlibrary(R6)\nlibrary(tidyverse)\n\nset.seed(123) # Ensuring reproducibility\ndummy_data &lt;- tibble(\n  id = 1:1000,\n  category = sample(c(\"A\", \"B\", \"C\", NA), 1000, replace = TRUE),\n  value = c(rnorm(997), -10, 100, NA), # Including outliers and a missing value\n  date = seq.Date(from = as.Date(\"2020-01-01\"), by = \"day\", length.out = 1000),\n  text = sample(c(\"Lorem\", \"Ipsum\", \"Dolor\", \"Sit\", NA), 1000, replace = TRUE)\n)\n\nDataQualityReport &lt;- R6Class(\n  \"DataQualityReport\",\n  public = list(\n    data = NULL,\n    \n    initialize = function(data) {\n      if (!is.data.frame(data)) {\n        stop(\"Data must be a dataframe.\")\n      }\n      self$data &lt;- data\n    },\n    \n    calculate_missing_values = function() {\n      return(\n        self$data %&gt;%\n          summarize(across(everything(), ~sum(is.na(.)))) %&gt;%\n          pivot_longer(cols = everything(), names_to = \"column\", values_to = \"missing_values\")\n      )\n    },\n    \n    detect_outliers = function() {\n      return(\n        self$data %&gt;%\n          select(where(is.numeric)) %&gt;%\n          imap(~{\n            qnt &lt;- quantile(.x, probs = c(0.25, 0.75), na.rm = TRUE)\n            iqr &lt;- IQR(.x, na.rm = TRUE)\n            lower_bound &lt;- qnt[1] - 1.5 * iqr\n            upper_bound &lt;- qnt[2] + 1.5 * iqr\n            outlier_count &lt;- sum(.x &lt; lower_bound | .x &gt; upper_bound, na.rm = TRUE)\n            tibble(column = .y, lower_bound, upper_bound, outlier_count)\n          }) %&gt;%\n          bind_rows()\n      )\n    },\n    \n    summarize_data_types = function() {\n      return(\n        self$data %&gt;%\n          summarize(across(everything(), ~paste(class(.), collapse = \", \"))) %&gt;%\n          pivot_longer(cols = everything(), names_to = \"column\", values_to = \"data_type\")\n      )\n    },\n    \n    generate_report = function() {\n      return(\n        list(\n          MissingValues = self$calculate_missing_values(),\n          Outliers = self$detect_outliers(),\n          DataTypes = self$summarize_data_types()\n        )\n      )\n    }\n  )\n)\n\n# Example of creating an instance and using the class\ndata_report_instance &lt;- DataQualityReport$new(dummy_data)\nreport &lt;- data_report_instance$generate_report()\n\nprint(report)\n\n$MissingValues\n# A tibble: 5 × 2\n  column   missing_values\n  &lt;chr&gt;             &lt;int&gt;\n1 id                    0\n2 category            246\n3 value                 1\n4 date                  0\n5 text                180\n\n$Outliers\n# A tibble: 2 × 4\n  column lower_bound upper_bound outlier_count\n  &lt;chr&gt;        &lt;dbl&gt;       &lt;dbl&gt;         &lt;int&gt;\n1 id         -498.       1500.               0\n2 value        -2.71        2.67             9\n\n$DataTypes\n# A tibble: 5 × 2\n  column   data_type\n  &lt;chr&gt;    &lt;chr&gt;    \n1 id       integer  \n2 category character\n3 value    numeric  \n4 date     Date     \n5 text     character\nIn this refactoring, each key task of the original function becomes a method within our R6 class. The initialize method sets up the object with the necessary data. The calculate_missing_values, detect_outliers, and summarize_data_types methods each handle a specific aspect of the data quality report, encapsulating the functionality in a clear and organized manner. The generate_report method brings these pieces together to produce the final report.\n\n\n\nThe Power of Modular Design\nThe transition to an R6 class structure is not just a change in syntax; it’s a shift towards a more modular design. Modular programming is a design technique that breaks a program into separate, interchangeable modules, each handling a specific subtask. This approach has several benefits:\n\nImproved Readability: When functions are broken down into smaller, purpose-specific methods, it becomes easier to understand what each part of the code does. This clarity is invaluable, especially as the complexity of the codebase grows.\nEnhanced Maintainability: With a modular structure, updating the code becomes more straightforward. If a specific aspect of the functionality needs to be changed, you only need to modify the relevant method, rather than wading through a monolithic function.\nEasier Debugging and Testing: Each module or method can be tested independently, simplifying the debugging process. This independent testability ensures that changes in one part of the code do not inadvertently affect other parts.\nReusability: Modular design promotes the reuse of code. Methods in an R6 class can be reused across different projects or datasets, facilitating a more efficient and DRY (Don’t Repeat Yourself) coding practice.\n\nIn our DataQualityReport class, the modular design is evident. The class acts as a container for related methods, each responsible for a different aspect of data quality reporting. This organization makes it clear what each part of the code is doing, and allows for easy modifications and extensions in the future.\n\n\nExtending Functionality\nA key advantage of OOP and our R6 class structure is the ease of extending functionality. For example, we can add a new method to our DataQualityReport class that exports the generated report to a CSV file. This extension demonstrates how we can build upon our existing class without altering its core functionality:\nDataQualityReport$set(\"public\", \"export_to_csv\", function(file_name) {\n  report &lt;- self$generate_report()\n  write.csv(report$MissingValues, paste0(file_name, \"_missing_values.csv\"))\n  write.csv(report$Outliers, paste0(file_name, \"_outliers.csv\"))\n  write.csv(report$DataTypes, paste0(file_name, \"_data_types.csv\"))\n  message(\"Report exported to CSV files with base name: \", file_name)\n})\n\ndata_report_instance2 &lt;- DataQualityReport$new(dummy_data)\n\ndata_report_instance2$export_to_csv(\"data_report\")\n\n#&gt; Report exported to CSV files with base name: data_report\nWith this new export_to_csv method, our class not only analyzes the data but also provides an easy way to export the results, enhancing the user experience and the utility of our class.\n\n\nOOP in R — A Paradigm Shift\nThe journey of refactoring our data_quality_report() function into an R6 class represents more than just an exercise in coding. It signifies a paradigm shift in the way we think about and structure our R code. By embracing OOP, we’re not only streamlining our workflow but also opening doors to more advanced programming practices that can handle larger, more complex tasks with ease.\nThe modular design, enhanced maintainability, and extensibility we’ve achieved with our DataQualityReport class illustrate the profound impact OOP can have. This shift in approach, from procedural to object-oriented, is a crucial step towards writing more robust, scalable, and efficient R code.\nAs we continue our exploration in R programming, I encourage readers to experiment with OOP. Embrace its principles in your projects and discover how it can transform your code, making it not only more powerful but also a joy to work with."
  },
  {
    "objectID": "ds/posts/2023-04-13_The-purrr-Package--A-Conductor-s-Baton-for-the-Tidyverse-Orchestra-in-R-57762fd1e4bb.html",
    "href": "ds/posts/2023-04-13_The-purrr-Package--A-Conductor-s-Baton-for-the-Tidyverse-Orchestra-in-R-57762fd1e4bb.html",
    "title": "The purrr Package: A Conductor’s Baton for the Tidyverse Orchestra in R",
    "section": "",
    "text": "The purrr package is a vital player in the tidyverse, an ecosystem of R packages designed to streamline data analysis tasks. As the Swiss Army knife of functional programming in R, purrr provides a versatile toolkit for working with data structures, especially lists and data frames. By simplifying complex operations, it brings clarity and elegance to your code, enabling you to manipulate, transform, and summarize data with ease. Picture purrr as the conductor of an orchestra, harmonizing the different sections of the tidyverse to create a beautiful symphony of data analysis. In this article, we’ll delve into the intricacies of purrr and discover how it can help you harness the full potential of R in your data analysis journey.\n\nUnderstanding purrr: The Functional Programming Paradigm\nTo fully appreciate the purrr package, it’s essential to understand the functional programming paradigm, which serves as the foundation of purrr’s capabilities.\nFunctional programming is a programming approach that treats computation as the evaluation of mathematical functions while avoiding changing state and mutable data. This style of programming emphasizes the use of pure functions, which are functions that, given the same input, will always produce the same output without any side effects. Picture functional programming like a composer, who brings together various instruments, playing their individual parts in perfect harmony, to create a unified and elegant piece of music.\nFunctional programming offers several benefits when working with R, particularly for data analysis. Some of these advantages include:\n\nReadability: Functional programming promotes writing clean and modular code, making it easier for others (and yourself) to understand and maintain the code. Think of it as a well-organized musical score, with each section clearly marked and easy to follow.\nReusability: Pure functions can be easily reused across different parts of your code, as they don’t rely on any external state. This reduces the need to write repetitive code and allows you to create a library of versatile functions, much like a conductor reusing musical motifs throughout a symphony.\nEase of debugging: By minimizing the use of mutable data and global state, functional programming reduces the likelihood of unexpected bugs, making the code more predictable and easier to debug. It’s akin to a conductor being able to isolate and resolve any discordant notes within the orchestra.\nParallel processing: The absence of side effects in functional programming allows for more efficient parallel processing, enabling you to harness the full power of modern multi-core processors. It’s like having multiple conductors working in perfect sync, seamlessly leading the orchestra in harmony.\n\nThe purrr package is designed to work seamlessly with R’s functional programming capabilities. One of its key strengths lies in its ability to apply functions to elements within data structures, such as lists and data frames. The package offers a range of “map” functions that allow you to elegantly iterate over these structures, transforming and manipulating the data as needed. This powerful feature of purrr serves as the conductor’s baton, guiding the flow of your data analysis and helping you create a harmonious and efficient workflow.\nIn the following sections, we will explore purrr’s key functions and demonstrate how they can help you streamline your data analysis process in R.\n\n\nA Closer Look at purrr’s Key Functions\nNow that we have a solid understanding of the functional programming paradigm, let’s dive into some of the key functions that the purrr package offers. These functions, like a conductor’s hand gestures, guide the flow of data through various operations, ensuring an efficient and harmonious analysis.\nmap() and its variants: Turning a caterpillar of code into a butterfly\nThe map() function is the cornerstone of the purrr package, allowing you to apply a function to each element of a list or vector. This versatile function can simplify your code by replacing cumbersome for loops and lapply() calls with a more concise and expressive syntax. The map() function comes in several variants, each tailored to return a specific type of output, such as map_lgl() for logical, map_chr() for character, and map_dbl() for double values. This flexibility enables you to transform your code into a more elegant and streamlined form, much like a caterpillar metamorphosing into a beautiful butterfly.\npmap(): Mastering multiple inputs like juggling balls\nThe pmap() function is designed to handle multiple input lists or vectors, iterating over them in parallel and applying a specified function. This powerful function allows you to juggle multiple inputs effortlessly, enabling complex data manipulation and transformation with ease. Like a skilled juggler, pmap() keeps all the input “balls” in the air, ensuring that they’re processed and combined as intended.\nkeep() and discard(): Handpicking data like sorting apples\nWhen you need to filter data based on specific criteria, purrr’s keep() and discard() functions come to the rescue. keep() retains elements that meet a given condition, while discard() removes elements that meet the condition. These functions let you handpick data elements as if you were sorting apples, keeping the good ones and discarding the bad. With their intuitive syntax and functional programming approach, keep() and discard() make data filtering a breeze.\nreduce(): Folding data like origami\nThe reduce() function in purrr allows you to successively apply a function to elements of a list or vector, effectively “folding” the data like an intricate piece of origami. This function is particularly useful when you need to aggregate data or combine elements in a specific manner. By iteratively applying a specified function, reduce() skillfully folds your data into the desired shape or form.\nsafely(): Handling errors gracefully like a trapeze artist\nIn data analysis, errors and unexpected situations can arise. The safely() function in purrr enables you to handle these scenarios with grace and poise, much like a trapeze artist performing a complex routine. safely() takes a function as input and returns a new function that, when applied, captures any errors and returns them as part of the output, rather than halting the execution. This allows you to identify and address errors without disrupting the flow of your analysis.\nThese key functions, along with many others in the purrr package, provide a powerful toolkit for efficient and harmonious data analysis in R. In the next sections, we’ll explore how to apply these functions to real-life data analysis tasks and demonstrate their practical applications.\n\n\nApplying purrr to Real-Life Data Analysis Tasks\nNow that we’ve explored the key functions of the purrr package, let’s examine how they can be applied to real-life data analysis tasks. By integrating purrr into your workflow, you can master the art of data analysis like a skilled conductor, guiding the flow of data through various operations and producing harmonious results.\nData transformation: Cleaning up a messy room\nData transformation is an essential step in the data analysis process, as real-world data can often be messy and unstructured. Using purrr’s map() functions, you can easily apply cleaning and transformation operations to your data, much like tidying up a cluttered room. For example, you might use map_chr() to extract specific information from text strings, or map_dbl() to convert data types within a data frame. By applying these functions iteratively, you can transform and reshape your data into a more structured and usable format.\nData aggregation: Assembling a puzzle\nIn many cases, you’ll need to aggregate data from multiple sources or perform complex calculations to derive insights. The reduce() function in purrr allows you to combine data elements like puzzle pieces, iteratively applying a function to merge or aggregate data as needed. Whether you’re summing up values, calculating averages, or performing custom aggregations, reduce() can help you assemble the data puzzle and reveal the bigger picture.\nData summarization: Condensing a novel into a short story\nData summarization is the process of distilling large amounts of information into concise, meaningful insights. Using purrr’s functional programming approach, you can create custom summary functions that extract relevant information from your data, much like condensing a novel into a short story. By chaining together map() functions with other tidyverse tools, such as dplyr’s summarize() and mutate() functions, you can generate insightful summaries that highlight the most important aspects of your data.\nIterative operations: Unraveling the threads of data\nMany data analysis tasks require performing iterative operations, such as running simulations, fitting models, or processing data in chunks. With purrr’s pmap() function, you can effortlessly juggle multiple inputs and apply functions across them in parallel. This enables you to unravel the threads of data, revealing patterns and relationships that might otherwise remain hidden. Additionally, by combining purrr’s functions with other R tools, such as parallel processing packages or machine learning libraries, you can further enhance the efficiency and power of your iterative operations.\nIn summary, purrr’s functional programming capabilities enable you to tackle a wide range of data analysis tasks with elegance and efficiency. By integrating purrr into your workflow, you can master the art of data analysis, conducting your data orchestra in perfect harmony.\n\n\nCase Study: Building Models and Creating Visualizations with purrr and Nested Data\nIn R we usually have many function vectorized which mean that for example they can be used on column of dataframe without using loop, apply or map. Purrr’s map functions can of course be used to apply vectorized functions, but is too easy. Let me show you something little bit harder and showing more of purrr’s capabilities.\nIn this case study, we will demonstrate how to use purrr with nested data to build multiple models and create custom visualizations.\nIntroducing the dataset: A collection of diverse species\nImagine we have a dataset containing measurements of various iris species, including sepal length, sepal width, petal length, and petal width, as well as the species classification. Our goal is to create separate linear regression models for each species to predict petal length based on petal width and visualize the results.\nData preparation: Nesting the data like a matryoshka doll\nTo begin, we need to split the dataset by species and create a nested data frame. We can use dplyr’s group_by() and tidyr’s nest() functions for this task:"
  },
  {
    "objectID": "bi/posts/2023-04-13_The-purrr-Package--A-Conductor-s-Baton-for-the-Tidyverse-Orchestra-in-R-57762fd1e4bb.html",
    "href": "bi/posts/2023-04-13_The-purrr-Package--A-Conductor-s-Baton-for-the-Tidyverse-Orchestra-in-R-57762fd1e4bb.html",
    "title": "The purrr Package: A Conductor’s Baton for the Tidyverse Orchestra in R",
    "section": "",
    "text": "The purrr package is a vital player in the tidyverse, an ecosystem of R packages designed to streamline data analysis tasks. As the Swiss Army knife of functional programming in R, purrr provides a versatile toolkit for working with data structures, especially lists and data frames. By simplifying complex operations, it brings clarity and elegance to your code, enabling you to manipulate, transform, and summarize data with ease. Picture purrr as the conductor of an orchestra, harmonizing the different sections of the tidyverse to create a beautiful symphony of data analysis. In this article, we’ll delve into the intricacies of purrr and discover how it can help you harness the full potential of R in your data analysis journey.\n\nUnderstanding purrr: The Functional Programming Paradigm\nTo fully appreciate the purrr package, it’s essential to understand the functional programming paradigm, which serves as the foundation of purrr’s capabilities.\nFunctional programming is a programming approach that treats computation as the evaluation of mathematical functions while avoiding changing state and mutable data. This style of programming emphasizes the use of pure functions, which are functions that, given the same input, will always produce the same output without any side effects. Picture functional programming like a composer, who brings together various instruments, playing their individual parts in perfect harmony, to create a unified and elegant piece of music.\nFunctional programming offers several benefits when working with R, particularly for data analysis. Some of these advantages include:\n\nReadability: Functional programming promotes writing clean and modular code, making it easier for others (and yourself) to understand and maintain the code. Think of it as a well-organized musical score, with each section clearly marked and easy to follow.\nReusability: Pure functions can be easily reused across different parts of your code, as they don’t rely on any external state. This reduces the need to write repetitive code and allows you to create a library of versatile functions, much like a conductor reusing musical motifs throughout a symphony.\nEase of debugging: By minimizing the use of mutable data and global state, functional programming reduces the likelihood of unexpected bugs, making the code more predictable and easier to debug. It’s akin to a conductor being able to isolate and resolve any discordant notes within the orchestra.\nParallel processing: The absence of side effects in functional programming allows for more efficient parallel processing, enabling you to harness the full power of modern multi-core processors. It’s like having multiple conductors working in perfect sync, seamlessly leading the orchestra in harmony.\n\nThe purrr package is designed to work seamlessly with R’s functional programming capabilities. One of its key strengths lies in its ability to apply functions to elements within data structures, such as lists and data frames. The package offers a range of “map” functions that allow you to elegantly iterate over these structures, transforming and manipulating the data as needed. This powerful feature of purrr serves as the conductor’s baton, guiding the flow of your data analysis and helping you create a harmonious and efficient workflow.\nIn the following sections, we will explore purrr’s key functions and demonstrate how they can help you streamline your data analysis process in R.\n\n\nA Closer Look at purrr’s Key Functions\nNow that we have a solid understanding of the functional programming paradigm, let’s dive into some of the key functions that the purrr package offers. These functions, like a conductor’s hand gestures, guide the flow of data through various operations, ensuring an efficient and harmonious analysis.\nmap() and its variants: Turning a caterpillar of code into a butterfly\nThe map() function is the cornerstone of the purrr package, allowing you to apply a function to each element of a list or vector. This versatile function can simplify your code by replacing cumbersome for loops and lapply() calls with a more concise and expressive syntax. The map() function comes in several variants, each tailored to return a specific type of output, such as map_lgl() for logical, map_chr() for character, and map_dbl() for double values. This flexibility enables you to transform your code into a more elegant and streamlined form, much like a caterpillar metamorphosing into a beautiful butterfly.\npmap(): Mastering multiple inputs like juggling balls\nThe pmap() function is designed to handle multiple input lists or vectors, iterating over them in parallel and applying a specified function. This powerful function allows you to juggle multiple inputs effortlessly, enabling complex data manipulation and transformation with ease. Like a skilled juggler, pmap() keeps all the input “balls” in the air, ensuring that they’re processed and combined as intended.\nkeep() and discard(): Handpicking data like sorting apples\nWhen you need to filter data based on specific criteria, purrr’s keep() and discard() functions come to the rescue. keep() retains elements that meet a given condition, while discard() removes elements that meet the condition. These functions let you handpick data elements as if you were sorting apples, keeping the good ones and discarding the bad. With their intuitive syntax and functional programming approach, keep() and discard() make data filtering a breeze.\nreduce(): Folding data like origami\nThe reduce() function in purrr allows you to successively apply a function to elements of a list or vector, effectively “folding” the data like an intricate piece of origami. This function is particularly useful when you need to aggregate data or combine elements in a specific manner. By iteratively applying a specified function, reduce() skillfully folds your data into the desired shape or form.\nsafely(): Handling errors gracefully like a trapeze artist\nIn data analysis, errors and unexpected situations can arise. The safely() function in purrr enables you to handle these scenarios with grace and poise, much like a trapeze artist performing a complex routine. safely() takes a function as input and returns a new function that, when applied, captures any errors and returns them as part of the output, rather than halting the execution. This allows you to identify and address errors without disrupting the flow of your analysis.\nThese key functions, along with many others in the purrr package, provide a powerful toolkit for efficient and harmonious data analysis in R. In the next sections, we’ll explore how to apply these functions to real-life data analysis tasks and demonstrate their practical applications.\n\n\nApplying purrr to Real-Life Data Analysis Tasks\nNow that we’ve explored the key functions of the purrr package, let’s examine how they can be applied to real-life data analysis tasks. By integrating purrr into your workflow, you can master the art of data analysis like a skilled conductor, guiding the flow of data through various operations and producing harmonious results.\nData transformation: Cleaning up a messy room\nData transformation is an essential step in the data analysis process, as real-world data can often be messy and unstructured. Using purrr’s map() functions, you can easily apply cleaning and transformation operations to your data, much like tidying up a cluttered room. For example, you might use map_chr() to extract specific information from text strings, or map_dbl() to convert data types within a data frame. By applying these functions iteratively, you can transform and reshape your data into a more structured and usable format.\nData aggregation: Assembling a puzzle\nIn many cases, you’ll need to aggregate data from multiple sources or perform complex calculations to derive insights. The reduce() function in purrr allows you to combine data elements like puzzle pieces, iteratively applying a function to merge or aggregate data as needed. Whether you’re summing up values, calculating averages, or performing custom aggregations, reduce() can help you assemble the data puzzle and reveal the bigger picture.\nData summarization: Condensing a novel into a short story\nData summarization is the process of distilling large amounts of information into concise, meaningful insights. Using purrr’s functional programming approach, you can create custom summary functions that extract relevant information from your data, much like condensing a novel into a short story. By chaining together map() functions with other tidyverse tools, such as dplyr’s summarize() and mutate() functions, you can generate insightful summaries that highlight the most important aspects of your data.\nIterative operations: Unraveling the threads of data\nMany data analysis tasks require performing iterative operations, such as running simulations, fitting models, or processing data in chunks. With purrr’s pmap() function, you can effortlessly juggle multiple inputs and apply functions across them in parallel. This enables you to unravel the threads of data, revealing patterns and relationships that might otherwise remain hidden. Additionally, by combining purrr’s functions with other R tools, such as parallel processing packages or machine learning libraries, you can further enhance the efficiency and power of your iterative operations.\nIn summary, purrr’s functional programming capabilities enable you to tackle a wide range of data analysis tasks with elegance and efficiency. By integrating purrr into your workflow, you can master the art of data analysis, conducting your data orchestra in perfect harmony.\n\n\nCase Study: Building Models and Creating Visualizations with purrr and Nested Data\nIn R we usually have many function vectorized which mean that for example they can be used on column of dataframe without using loop, apply or map. Purrr’s map functions can of course be used to apply vectorized functions, but is too easy. Let me show you something little bit harder and showing more of purrr’s capabilities.\nIn this case study, we will demonstrate how to use purrr with nested data to build multiple models and create custom visualizations.\nIntroducing the dataset: A collection of diverse species\nImagine we have a dataset containing measurements of various iris species, including sepal length, sepal width, petal length, and petal width, as well as the species classification. Our goal is to create separate linear regression models for each species to predict petal length based on petal width and visualize the results.\nData preparation: Nesting the data like a matryoshka doll\nTo begin, we need to split the dataset by species and create a nested data frame. We can use dplyr’s group_by() and tidyr’s nest() functions for this task:"
  },
  {
    "objectID": "ds/index.html",
    "href": "ds/index.html",
    "title": "Data Science",
    "section": "",
    "text": "Data at Your Fingertips: Crafting Interactive Tables in R\n\n\n19 min\n\n\n\nNov 3, 2024\n\n\n\n\n\nWord Count\n\n\n3778 words\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable It Like a Pro: Print-Ready Tables in R\n\n\n13 min\n\n\n\nOct 24, 2024\n\n\n\n\n\nWord Count\n\n\n2435 words\n\n\n\n\n\n\n\n\n\n\n\n\n\nDon’t Get Fooled by Numbers: Data Literacy as the New Survival Skill\n\n\n14 min\n\n\n\nOct 17, 2024\n\n\n\n\n\nWord Count\n\n\n2642 words\n\n\n\n\n\n\n\n\n\n\n\n\n\nR You Ready? Git Your Code Under Control!\n\n\n21 min\n\n\n\nOct 10, 2024\n\n\n\n\n\nWord Count\n\n\n4044 words\n\n\n\n\n\n\n\n\n\n\n\n\n\nSQL of the Rings: One Language to Query Them All (with R)\n\n\n26 min\n\n\n\nAug 23, 2024\n\n\n\n\n\nWord Count\n\n\n5190 words\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhy Every Data Scientist Needs the janitor Package\n\n\n23 min\n\n\n\nAug 16, 2024\n\n\n\n\n\nWord Count\n\n\n4566 words\n\n\n\n\n\n\n\n\n\n\n\n\n\nWriting R Code the ‘Good Way’\n\n\n8 min\n\n\n\nJun 20, 2024\n\n\n\n\n\nWord Count\n\n\n1475 words\n\n\n\n\n\n\n\n\n\n\n\n\n\nMastering purrr: From Basic Maps to Functional Magic in R\n\n\n29 min\n\n\n\nMay 23, 2024\n\n\n\n\n\nWord Count\n\n\n5670 words\n\n\n\n\n\n\n\n\n\n\n\n\n\nShiny and Beyond: Mastering Interactive Web Applications with R and Appsilon Packages\n\n\n22 min\n\n\n\nMay 16, 2024\n\n\n\n\n\nWord Count\n\n\n4203 words\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Rebus Code: Unveiling the Secrets of Regex in R\n\n\n15 min\n\n\n\nMay 9, 2024\n\n\n\n\n\nWord Count\n\n\n2921 words\n\n\n\n\n\n\n\n\n\n\n\n\n\nNavigating the Data Pipes: An R Programming Journey with Mario Bros.\n\n\n13 min\n\n\n\nApr 18, 2024\n\n\n\n\n\nWord Count\n\n\n2581 words\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe purrr Package: A Conductor’s Baton for the Tidyverse Orchestra in R\n\n\n9 min\n\n\n\nApr 13, 2024\n\n\n\n\n\nWord Count\n\n\n1624 words\n\n\n\n\n\n\n\n\n\n\n\n\n\nCrafting Elegant Scientific Documents in RStudio: A LaTeX and R Markdown Tutorial\n\n\n14 min\n\n\n\nApr 11, 2024\n\n\n\n\n\nWord Count\n\n\n2761 words\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Visualization Reloaded: Equipping Your Reports with the Ultimate R Package Arsenal\n\n\n15 min\n\n\n\nMar 28, 2024\n\n\n\n\n\nWord Count\n\n\n2957 words\n\n\n\n\n\n\n\n\n\n\n\n\n\nObject-Oriented Express: Refactoring in R\n\n\n9 min\n\n\n\nNov 23, 2023\n\n\n\n\n\nWord Count\n\n\n1647 words\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Fast and the Curious: Optimizing R\n\n\n14 min\n\n\n\nNov 16, 2023\n\n\n\n\n\nWord Count\n\n\n2716 words\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf you know English, you would be able to code in R…\n\n\n5 min\n\n\n\nOct 27, 2022\n\n\n\n\n\nWord Count\n\n\n930 words\n\n\n\n\n\n\n\n\n\n\n\n\n\nSequel of SQL…\n\n\n3 min\n\n\n\nOct 18, 2022\n\n\n\n\n\nWord Count\n\n\n507 words\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf you don’t know it, it’s only temporary state…\n\n\n3 min\n\n\n\nOct 3, 2022\n\n\n\n\n\nWord Count\n\n\n561 words\n\n\n\n\n\n\n\n\n\n\n\n\n\nTools — you’re defining them or they define you?\n\n\n3 min\n\n\n\nSep 27, 2022\n\n\n\n\n\nWord Count\n\n\n562 words\n\n\n\n\n\n\n\n\n\n\n\n\n\nHumanist in firm grip with world of maths…\n\n\n2 min\n\n\n\nSep 20, 2022\n\n\n\n\n\nWord Count\n\n\n318 words\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "ds/posts/2023-11-16_The-Fast-and-the-Curious--Optimizing-R-991aea0f7945.html",
    "href": "ds/posts/2023-11-16_The-Fast-and-the-Curious--Optimizing-R-991aea0f7945.html",
    "title": "The Fast and the Curious: Optimizing R",
    "section": "",
    "text": "The Need for Speed in R\n\n\n\nImage\n\n\nIn the realm of data science, where the landscape is ever-changing and data volumes are incessantly swelling, speed and efficiency in processing aren’t mere conveniences — they’re indispensable. As we unveil the second chapter of our series, we turn the spotlight onto a crucial yet often understated aspect of R programming: performance optimization. Our focal point remains the data_quality_report() function, which has already proven its mettle in dissecting datasets. But now, akin to a seasoned protagonist in an action-packed sequel, it faces a new, thrilling challenge: boosting its performance for heightened speed and enhanced memory efficiency.\nThis journey into the optimization realm transcends mere code acceleration. It’s a deep dive into the heart of R programming, unraveling the intricate layers of what makes code run faster, consume less memory, and perform at its peak. We’re not just tweaking a function here and there; we’re embarking on a quest to understand the very sinews and muscles of R’s performance anatomy. It’s about transforming our data_quality_report() from a reliable workhorse into a sleek, agile thoroughbred.\nAs we embark on this adventure, we’ll explore the intricate avenues of R’s performance tuning, navigate through the complex terrains of memory management, and discover the art of writing code that not only does its job well but does it with remarkable efficiency. This article is not just for those who use our data_quality_report() function; it’s a guide for every R programmer who yearns to see their scripts shedding the extra milliseconds, to make their analysis as swift as the wind. So, strap in and get ready; we’re about to turbocharge our R functions!\n\n\nProfiling Performance\nThe first step in our optimization odyssey is akin to a strategic pause, a moment of introspection to assess the current state of affairs. In the world of high-performance cars, this would be the time spent in the pit stop, meticulously inspecting every component to shave off those crucial milliseconds on the track. Similarly, in R programming, this phase is all about profiling. Profiling is like our diagnostic toolkit, a means to peer into the inner workings of our function and pinpoint exactly where our computational resources are being expended the most.\nEnter profvis, R’s equivalent of a high-tech diagnostic tool. It’s not just about finding the slow parts of our code; it’s about understanding the why and the how. By profiling our data_quality_report() function, we get a visual representation of where the function spends most of its time. Is it getting bogged down while calculating missing values? Are the outlier detection algorithms dragging their feet? Or is it the data type summarization that’s adding those extra seconds?\nWe’ll begin our journey with the following simple yet powerful profiling exercise:\nlibrary(profvis)\n\n# Profiling the data_quality_report function\nprofvis({\n  data_quality_report(dummy_data)\n})\n\n\n\nImage\n\n\nThis profiling run will lay it all bare in front of us, showcasing through an intuitive interface where our precious computational seconds are being spent. We might find surprises, functions or lines of code that are more resource-intensive than anticipated. This insight is our starting line, the baseline from which we leap into the world of optimization. We now have a map, a guide to focusing our efforts where they are needed the most.\nIn the upcoming section, we’ll dissect these profiling results. We will roll up our sleeves and delve into our first round of optimizations, where we will explore how data.table and dplyr can be harnessed to not just do things right, but to do them fast. Our data_quality_report() is about to get a serious performance makeover.\n\n\nEfficient Data Processing with data.table and dplyr\nOptimizing with data.table: data.table is a powerhouse for handling large datasets efficiently in R. Its syntax is a bit different from dplyr, but it excels in speedy operations and memory efficiency. Let’s optimize the missing values calculation and outlier detection using data.table.\nFirst, converting our dataset to a data.table object:\nlibrary(data.table)\n\n# Converting the dataset to a data.table\ndt_data &lt;- as.data.table(dummy_data)\nNow, let’s optimize the missing values calculation:\n# Optimized missing values calculation using data.table\nmissing_values_dt &lt;- dt_data[, lapply(.SD, function(x) sum(is.na(x))), .SDcols = names(dt_data)]\nFor outlier detection, data.table can also provide a significant speed-up:\n# Enhanced outlier detection using data.table\noutliers_dt &lt;- dt_data[, lapply(.SD, function(x) {\n  if (is.numeric(x)) {\n    bounds &lt;- quantile(x, probs = c(0.25, 0.75), na.rm = TRUE)\n    iqr &lt;- IQR(x, na.rm = TRUE)\n    list(sum(x &lt; (bounds[1] - 1.5 * iqr) | x &gt; (bounds[2] + 1.5 * iqr), na.rm = TRUE))\n  } else {\n    NA_integer_\n  }\n}), .SDcols = names(dt_data)]\n\nEnhancing with dplyr\nWhile data.table focuses on performance, dplyr offers a more readable and intuitive syntax. Let’s utilize dplyr for the same tasks to compare:\nlibrary(dplyr)\n\n# Using dplyr for missing values calculation\nmissing_values_dplyr &lt;- dummy_data %&gt;%\n  summarize(across(everything(), ~sum(is.na(.)))) %&gt;%\n  pivot_longer(cols = everything(), names_to = \"column\", values_to = \"missing_values\")\n\n# Using dplyr for outlier detection\noutliers_dplyr &lt;- dummy_data %&gt;%\n  summarize(across(where(is.numeric), ~list(\n    sum(. &lt; (quantile(., 0.25, na.rm = TRUE) - 1.5 * IQR(., na.rm = TRUE)) | \n        . &gt; (quantile(., 0.75, na.rm = TRUE) + 1.5 * IQR(., na.rm = TRUE)), na.rm = TRUE)\n  ))) %&gt;%\n  pivot_longer(cols = everything(), names_to = \"column\", values_to = \"outliers\")\nThese snippets illustrate how data.table and dplyr can be used for optimizing specific parts of the data_quality_report() function. The data.table approach offers a significant performance boost, especially with larger datasets, while dplyr maintains readability and ease of use.\nIn the following sections, we’ll explore memory management techniques and vectorization strategies to further enhance our function’s performance.\n\n\n\nMemory Management Techniques\nOptimizing for speed is one part of the equation; optimizing for memory usage is another crucial aspect, especially when dealing with large datasets. Efficient memory management in R can significantly reduce the risk of running into memory overflows and can speed up operations by reducing the need for frequent garbage collection.\n\nUnderstanding R’s Memory Model\nR’s memory model is inherently different from languages like Python or Java. It makes copies of objects often, especially in standard operations like subsetting or modifying data frames. This behavior can quickly lead to high memory usage. Being aware of this is the first step in writing memory-efficient R code.\n\n\nIn-Place Modification with data.table\ndata.table shines not only in speed but also in memory efficiency, primarily due to its in-place modification capabilities. Unlike data frames or tibbles in dplyr, which often create copies of the data, data.table modifies data directly in memory. This approach drastically reduces memory footprint.\nLet’s modify the data_quality_report() function to leverage in-place modification for certain operations:\n# Adjusting the function for in-place modification using data.table\ndata_quality_report_dt &lt;- function(data) {\n  setDT(data) # Convert to data.table in place\n  \n  # In-place modification for missing values\n  missing_values &lt;- data[, lapply(.SD, function(x) sum(is.na(x))), .SDcols = names(data)]\n  \n  # In-place modification for outlier detection\n  outliers &lt;- data[, lapply(.SD, function(x) {\n    if (is.numeric(x)) {\n      bounds &lt;- quantile(x, probs = c(0.25, 0.75), na.rm = TRUE)\n      iqr &lt;- IQR(x, na.rm = TRUE)\n      sum(x &lt; (bounds[1] - 1.5 * iqr) | x &gt; (bounds[2] + 1.5 * iqr), na.rm = TRUE)\n    } else {\n      NA_integer_\n    }\n  }), .SDcols = names(data)] \n\n  # Convert back to tibble if needed\n  as_tibble(list(MissingValues = missing_values, Outliers = outliers))\n}\n\n# Example use of the function\noptimized_report &lt;- data_quality_report_dt(dummy_data)\n\n\nChoosing the Right Data Structures\nAnother approach to optimize memory usage is by using efficient data structures. For instance, using matrices or arrays instead of data frames for homogenous data can be more memory-efficient. Additionally, packages like vctrs offer efficient ways to build custom data types in R, which can be tailored for memory efficiency.\n\n\nGarbage Collection and Memory Pre-allocation\nR performs garbage collection automatically, but sometimes manual garbage collection can be useful, especially after removing large objects. Also, pre-allocating memory for objects, like creating vectors or matrices of the required size before filling them, can reduce the overhead of resizing these objects during data manipulation.\nBy implementing these memory management techniques, the data_quality_report() function can become more efficient in handling large datasets without straining the system’s memory.\n\n\n\nVectorization over Looping\nIn the world of R programming, vectorization is often hailed as a cornerstone for writing efficient code. Vectorized operations are not only more concise but also significantly faster than their looped counterparts. This is because vectorized operations leverage optimized C code under the hood, reducing the overhead of repeated R function calls.\n\nUnderstanding Vectorization\nVectorization refers to the method of applying a function simultaneously to multiple elements of an object, like a vector or a column of a dataframe. In R, many functions are inherently vectorized. For instance, arithmetic operations on vectors or columns are automatically vectorized.\n\n\nApplying Vectorization in data_quality_report()\nLet’s apply vectorization to the data_quality_report() function. Our goal is to eliminate explicit loops or iterative lapply() calls, replacing them with vectorized alternatives where possible.\nFor example, let’s optimize the missing values calculation by vectorizing it:\n# Vectorized calculation of missing values\nvectorized_missing_values &lt;- function(data) {\n  colSums(is.na(data))\n}\n\nmissing_values_vectorized &lt;- vectorized_missing_values(dummy_data)\nSimilarly, we can vectorize the outlier detection. However, outlier detection by nature involves conditional logic which can be less straightforward to vectorize. We’ll need to carefully handle this part to ensure that we don’t compromise readability:\nvectorized_outlier_detection &lt;- function(data) {\n  # Filter only numeric columns\n  numeric_data &lt;- data[, sapply(data, is.numeric), drop = FALSE]\n  \n  # Ensure numeric_data is a dataframe and has columns\n  if (!is.data.frame(numeric_data) || ncol(numeric_data) == 0) {\n    return(NULL) # or appropriate return value indicating no numeric columns or invalid input\n  }\n  \n  # Compute quantiles and IQR for numeric columns\n  bounds &lt;- apply(numeric_data, 2, function(x) quantile(x, probs = c(0.25, 0.75), na.rm = TRUE))\n  iqr &lt;- apply(numeric_data, 2, IQR, na.rm = TRUE)\n  \n  lower_bounds &lt;- bounds[\"25%\", ] - 1.5 * iqr\n  upper_bounds &lt;- bounds[\"75%\", ] + 1.5 * iqr\n  \n  sapply(seq_along(numeric_data), function(i) {\n    x &lt;- numeric_data[[i]]\n    lower &lt;- lower_bounds[i]\n    upper &lt;- upper_bounds[i]\n    sum(x &lt; lower | x &gt; upper, na.rm = TRUE)\n  })\n}\n\noutliers_vectorized &lt;- vectorized_outlier_detection(dummy_data)\n\n\nBalancing Vectorization and Readability\nWhile vectorization is key for performance, it’s crucial to balance it with code readability. Sometimes, overly complex vectorized code can be difficult to understand and maintain. Hence, it’s essential to strike the right balance — vectorize where it makes the code faster and more concise, but not at the cost of making it unreadable or unmaintainable.\nWith these vectorized improvements, our data_quality_report() function is evolving into a more efficient tool. It’s a testament to the saying in R programming: “Think vectorized.”\n\n\n\nParallel Processing with purrr and future\nIn the final leg of our optimization journey, we venture into the realm of parallel processing. R, by default, operates in a single-threaded mode, executing one operation at a time. However, modern computers are equipped with multiple cores, and we can harness this hardware capability to perform multiple operations simultaneously. This is where parallel processing shines, significantly reducing computation time for tasks that can be executed concurrently.\n\nIntroducing Parallel Processing in R\nParallel processing can be particularly effective for operations that are independent of each other and can be run simultaneously without interference. Our data_quality_report() function, with its distinct and independent calculations for missing values, outliers, and data types, is a prime candidate for this approach.\n\n\nLeveraging purrr and future\nThe purrr package, a member of the tidyverse family, is known for its functions to iterate over elements in a clean and functional programming style. When combined with the future package, it allows us to easily apply these iterations in a parallel manner.\nLet’s parallelize the computation in our function:\nlibrary(furrr)\nlibrary(dplyr)\n\n# Set up future to use parallel backends\nplan(multicore)\n\n# Complete Parallelized version of data_quality_report using furrr\ndata_quality_report_parallel &lt;- function(data) {\n  # Ensure data is a dataframe\n  if (!is.data.frame(data)) {\n    stop(\"Input must be a dataframe.\")\n  }\n  \n  # Prepare a list of column names for future_map\n  column_names &lt;- names(data)\n  \n  # Parallel computation for missing values\n  missing_values &lt;- future_map_dfc(column_names, ~sum(is.na(data[[.x]])), .progress = TRUE) %&gt;%\n    set_names(column_names) %&gt;%\n    pivot_longer(cols = everything(), names_to = \"column\", values_to = \"missing_values\")\n  \n  # Parallel computation for outlier detection\n  outliers &lt;- future_map_dfc(column_names, ~{\n    column_data &lt;- data[[.x]]\n    if (is.numeric(column_data)) {\n      bounds &lt;- quantile(column_data, probs = c(0.25, 0.75), na.rm = TRUE)\n      iqr &lt;- IQR(column_data, na.rm = TRUE)\n      lower_bound &lt;- bounds[1] - 1.5 * iqr\n      upper_bound &lt;- bounds[2] + 1.5 * iqr\n      sum(column_data &lt; lower_bound | column_data &gt; upper_bound, na.rm = TRUE)\n    } else {\n      NA_integer_\n    }\n  }, .progress = TRUE) %&gt;%\n    set_names(column_names) %&gt;%\n    pivot_longer(cols = everything(), names_to = \"column\", values_to = \"outlier_count\")\n  \n  # Parallel computation for data types\n  data_types &lt;- future_map_dfc(column_names, ~paste(class(data[[.x]]), collapse = \", \"), .progress = TRUE) %&gt;%\n    set_names(column_names) %&gt;%\n    pivot_longer(cols = everything(), names_to = \"column\", values_to = \"data_type\")\n  \n  # Combine all the elements into a list\n  list(\n    MissingValues = missing_values,\n    Outliers = outliers,\n    DataTypes = data_types\n  )\n}\n\n# Example use of the function with dummy_data\n# Ensure dummy_data is defined and is a dataframe before running this\nparallel_report &lt;- data_quality_report_parallel(dummy_data)\nThis function now uses parallel processing for each major computation, which should enhance performance, especially for larger datasets. Note that parallel processing is most effective on systems with multiple cores and for tasks that are significantly computationally intensive.\nRemember to test this function with your specific datasets and use cases to ensure that the parallel processing setup is beneficial for your scenarios.\n\n\n\nRevised Conclusion\nAs we wrap up our exploration in “The Fast and the Curious: Optimizing R,” the results from our performance benchmarking present an intriguing narrative. While the data.table-optimized version, data_quality_report_dt(), showcased a commendable improvement in speed over the original, handling data operations more efficiently, our foray into parallel processing yielded surprising results. Contrary to our expectations, the parallelized version, data_quality_report_parallel(), significantly lagged behind, being over 100 times slower than its predecessors.\nlibrary(microbenchmark)\n\n# dummy data with 1000 rows\nmicrobenchmark(\n  data_table = data_quality_report_dt(dummy_data),\n  prior_version = data_quality_report(dummy_data),\n  parallelized = data_quality_report_parallel(dummy_data),\n  times = 10\n)\nUnit: milliseconds\n          expr       min        lq       mean    median        uq       max neval cld\n    data_table    3.8494    6.9226   13.36179    8.8422   17.2609   42.0615    10  a \n prior_version   51.9415   55.7101   61.26745   57.7909   66.5635   77.2151    10  a \n  parallelized 2622.9041 2749.6199 2895.25921 2828.4161 2977.8426 3438.4195    10   b\nThis outcome serves as a crucial reminder of the complexities inherent in parallel computing, especially in R. Parallel processing is often seen as a silver bullet for performance issues, but this is not always the case. The overhead associated with managing multiple threads and the nature of the tasks being parallelized can sometimes outweigh the potential gains from parallel execution. This is particularly true for operations that are not inherently time-consuming or for datasets that are not large enough to justify the parallelization overhead.\nSuch results emphasize the importance of context and the need to tailor optimization strategies to specific scenarios. What works for one dataset or function may not necessarily be the best approach for another. It’s a testament to the nuanced nature of performance optimization in data analysis — a balance between understanding the tools at our disposal and the unique challenges posed by each dataset.\nAs we move forward in our series, these findings underscore the need to approach optimization with a critical eye. We’ll continue to explore various facets of R programming, seeking not just to improve performance, but also to deepen our understanding of when and how to apply these techniques effectively."
  },
  {
    "objectID": "ds/posts/2024-03-28_Data-Visualization-Reloaded--Equipping-Your-Reports-with-the-Ultimate-R-Package-Arsenal-0ef33c2fd4cf.html",
    "href": "ds/posts/2024-03-28_Data-Visualization-Reloaded--Equipping-Your-Reports-with-the-Ultimate-R-Package-Arsenal-0ef33c2fd4cf.html",
    "title": "Data Visualization Reloaded: Equipping Your Reports with the Ultimate R Package Arsenal",
    "section": "",
    "text": "Embracing the Tidyverse Style Guide\n\n\n\nImage\n\n\nIn the vast and ever-expanding universe of data, the ability to not just see but truly understand the stories hidden within numbers becomes paramount. This journey of comprehension isn’t unlike the iconic moment from The Matrix, where Neo, standing amidst the endless possibilities of the digital realm, declares his need for “Guns, lots of guns.” In the context of our exploration, these “guns” are not weapons of destruction but powerful tools of creation and insight — data visualization packages for R, each with its unique capabilities to transform raw data into compelling narratives.\nOur quest is navigated through the versatile landscapes of Quarto and R Markdown (Rmd), platforms that serve as the backbone for our reports. Whether you’re drafting an interactive web document, a static PDF, or a neatly formatted Word file, these tools are the canvases upon which our data stories will unfold. But a canvas alone does not make art — it’s the brushes, colors, and techniques that bring a scene to life. Similarly, our chosen R packages — each a brushstroke of genius — allow us to paint intricate pictures with our data.\nThis article will serve as your guide through this arsenal of visualization packages. From the foundational ggplot2 to the interactive plotly, the geospatial leaflet, and the detailed gt for tabular artistry, we’ll cover a spectrum of tools that cater to every analyst’s, researcher’s, and data storyteller’s needs. We’ll delve into how each package can be utilized within Quarto and R Markdown to create reports that not only convey information but also engage and enlighten your audience.\nAs we embark on this journey together, remember that the power of these tools lies not just in their individual capabilities but in how they can be combined to tell a cohesive, compelling story. By the end of this exploration, you’ll be equipped with a diverse and potent arsenal, ready to tackle any data visualization challenge that comes your way.\nLet the journey begin.\n\n\nThe Foundation with ggplot2\nAt the heart of our data visualization arsenal lies ggplot2, a package that has revolutionized the way we think about and create graphics in R. Inspired by Leland Wilkinson’s Grammar of Graphics, ggplot2 allows users to assemble plots layer by layer, making the creation of complex visualizations both intuitive and accessible.\nggplot2 shines in its ability to break down and understand data visualization as a series of logical steps: data selection, aesthetic mapping, geometric objects, and statistical transformations. This structured approach enables users to craft nearly any type of graphic, from simple scatter plots to intricate layered visualizations. The package’s extensive customization options—through scales, themes, and coordinates—further empower users to tailor their visuals to the precise narrative they wish to convey.\nFor reports in Quarto or R Markdown, ggplot2 acts as the foundational tool for data visualization. Its versatility is unmatched, offering crisp, publication-quality graphics for static outputs (PDF, DOCX) and adaptable visuals for dynamic HTML documents. Whether you’re creating a formal report, a comprehensive academic paper, or an engaging web article, ggplot2 provides the necessary tools to visually articulate your data’s story.\nTo illustrate the power of ggplot2, let’s create a simple yet elegant scatter plot:\nlibrary(ggplot2)\n\n# Sample data\ndf &lt;- data.frame(\n  x = rnorm(100),\n  y = rnorm(100)\n)\n\n# Scatter plot\nggplot(df, aes(x=x, y=y)) +\n  geom_point(color = 'blue') +\n  theme_minimal() +\n  ggtitle(\"Sample Scatter Plot\") +\n  xlab(\"X-axis Label\") +\n  ylab(\"Y-axis Label\")\n\n\n\nSample Scatter Plot\n\n\nThis code snippet highlights ggplot2’s simplicity and elegance, creating a plot that is both visually appealing and informative. As we proceed to explore more specialized packages, ggplot2 remains our trusted foundation, enabling us to build upon it and enhance our reports with diverse visual narratives.\n\n\nEnhancing Interactivity with plotly\nIn the dynamic world of web-based reporting, plotly stands out as a beacon of interactivity. It builds upon the static beauty of ggplot2 plots by adding a layer of engagement through interactive elements. Users can hover over data points, zoom in on areas of interest, and filter through datasets directly within their plots, transforming a static visualization into an interactive exploration.\nplotly offers a wide range of interactive chart types, including line charts, bar charts, scatter plots, and more, all with the added benefit of user interaction. It’s particularly adept at handling large datasets, making it possible to explore and interpret complex data in real-time. The package’s ability to integrate with ggplot2 means that users can easily elevate their existing visualizations from static to dynamic with minimal effort.\nFor HTML reports created in Quarto or R Markdown, plotly enhances the reader’s experience by making the data exploration an integral part of the narrative. This level of interactivity invites the audience to engage with the data on a deeper level, facilitating a more personalized exploration of the findings. It’s especially useful in scenarios where understanding data nuances is crucial, such as in exploratory data analysis or when presenting results to a diverse audience.\nHere’s how to transform a ggplot2 plot into an interactive plotly plot:\nlibrary(ggplot2)\nlibrary(plotly)\n\n# Create a ggplot\np &lt;- ggplot(mtcars, aes(wt, mpg)) +\n  geom_point(aes(text = rownames(mtcars)), size = 4) +\n  labs(title = \"Motor Trend Car Road Tests\",\n       x = \"Weight (1000 lbs)\",\n       y = \"Miles/(US) gallon\") +\n  theme_minimal()\n\n# Convert to plotly\nggplotly(p, tooltip = \"text\")\nThis code demonstrates the ease with which a static ggplot2 visualization can be converted into an interactive plotly graph. By incorporating plotly into your data storytelling toolkit, you unlock a world where data visualizations are not just seen but experienced.\n\n\nMapping Data with leaflet\nGeospatial data visualization is a critical aspect of storytelling in many fields, from environmental science to urban planning. leaflet for R brings the power of interactive mapping to your reports, allowing you to create detailed, dynamic maps that can be embedded directly into HTML documents. Based on the Leaflet.js library, it is the premier tool for building interactive maps in the R ecosystem.\nWith leaflet, you can layer multiple data sources on a single map, customize map appearances, and add interactive features like pop-ups and markers. It supports various map types, including base maps from OpenStreetMap, Mapbox, and Google Maps. Whether you’re tracking migration patterns, visualizing climate change data, or showcasing demographic trends, leaflet makes geospatial data accessible and engaging.\nFor Quarto or R Markdown reports destined for the web, leaflet maps offer a dynamic way to present geospatial data. Unlike static maps, leaflet enables readers to zoom in and out, explore different layers, and interact with the data points directly. This interactivity enhances the user’s engagement and understanding, making leaflet an invaluable tool for reports that include location-based analysis or findings.\nCreating an interactive map with leaflet is straightforward:\nlibrary(leaflet)\n\n# Sample data: Locations of some major cities\ncities &lt;- data.frame(\n  lon = c(-74.00597, -0.127758, 151.20732),\n  lat = c(40.71278, 51.50735, -33.86785),\n  city = c(\"New York\", \"London\", \"Sydney\")\n)\n\n# Create a leaflet map\nleaflet(cities) %&gt;%\n  addTiles() %&gt;%  # Add default OpenStreetMap map tiles\n  addMarkers(~lon, ~lat, popup = ~city)\n\n\n\nInteractive Map\n\n\nThis example demonstrates how to create a basic interactive map showing specific locations. With leaflet, the complexity and depth of your geospatial visualizations are limited only by your imagination.\n\n\nInteractive Tables with DT\nIn the realm of data presentation, tables are indispensable for displaying detailed information in a structured manner. DT (DataTables) is an R package that integrates the jQuery DataTables plugin, transforming static tables into interactive exploration tools. It enables users to search, sort, and paginate tables directly within HTML reports, enhancing the user’s ability to engage with and understand the data.\nDT offers a plethora of features to make tables more interactive and user-friendly. Highlights include automatic or custom column filtering, options for table styling, and the ability to include buttons for exporting the table to CSV, Excel, or PDF formats. These functionalities are particularly useful in reports that contain large datasets, allowing readers to navigate and focus on the data that interests them most.\nFor reports generated in Quarto or R Markdown with an HTML output, DT provides a superior way to present tabular data. It bridges the gap between static tables, which can be overwhelming and difficult to navigate, and the need for dynamic, accessible data presentation. Whether you’re summarizing survey results, financial data, or scientific measurements, DT tables can significantly improve the readability and usability of your reports.\nHere’s a simple example of how to create an interactive table with DT:\nlibrary(DT)\n\n# Sample data: A subset of the mtcars dataset\ndata(mtcars)\nmtcars_subset &lt;- head(mtcars, 10)\n\n# Render an interactive table\ndatatable(mtcars_subset, options = list(pageLength = 5, autoWidth = TRUE))\n\n\n\nInteractive Table\n\n\nThis code snippet demonstrates how to convert a subset of the mtcars dataset into an interactive table, complete with pagination and adjustable column widths. By integrating DT into your reporting toolkit, you can ensure that even the densest data tables become navigable and insightful components of your narrative.\n\n\nThe Grammar of Tables with gt\nWhile DT focuses on interactivity for data tables, the gt package brings unparalleled levels of customization and styling to table creation in R. Standing for “Grammar of Tables,” gt allows you to create highly detailed and beautifully formatted tables that communicate information clearly and effectively, akin to how ggplot2 revolutionizes plot creation.\ngt enables you to craft tables that go beyond mere data presentation; it allows you to tell a story with your data. From adding footnotes, coloring cells based on values, to creating complex layouts with grouped headers and spanning labels, gt provides a comprehensive suite of tools for enhancing the aesthetic and functional aspects of tables in your reports.\nIn Quarto or R Markdown reports, regardless of the output format (HTML, PDF, or DOCX), gt tables can significantly elevate the visual standard and readability of your presentations. Especially in PDFs and printed documents, where interactive elements are not feasible, the detailed customization gt offers makes your tables not just data containers but key narrative elements of your report.\nTo demonstrate the capabilities of gt, let’s create a simple yet styled table using a subset of the mtcars dataset:\nlibrary(gt)\n\n# Sample data: A subset of the mtcars dataset\ndata &lt;- head(mtcars, 10)\n\ngt_table &lt;- gt(data) %&gt;%\n  tab_header(\n    title = \"Motor Trend Car Road Tests\",\n    subtitle = \"A subset of the mtcars dataset\"\n  ) %&gt;%\n  cols_label(\n    mpg = \"Miles/(US) gallon\",\n    cyl = \"Number of Cylinders\",\n    disp = \"Displacement (cu.in.)\"\n  ) %&gt;%\n  fmt_number(\n    columns = vars(mpg, disp),\n    decimals = 2\n  ) %&gt;%\n  tab_style(\n    style = cell_fill(color = \"gray\"),\n    locations = cells_column_labels(columns = TRUE)\n  ) %&gt;%\n  tab_style(\n    style = cell_text(color = \"white\"),\n    locations = cells_column_labels(columns = TRUE)\n  )\n\ngt_table\n\n\n\nStyled Table\n\n\nThis code snippet highlights how gt not only allows for the structuring and presentation of tabular data but also for the artistic expression within data reporting, making your tables both informative and visually appealing.\n\n\nBringing Plots to Life with ggiraph\nIn the quest to make reports more engaging, ggiraph emerges as a powerful ally, enabling the transformation of static ggplot2 graphics into interactive visual stories. ggiraph allows elements within ggplot2 plots, such as points, lines, and bars, to become interactive, supporting tooltips, hover actions, and even hyperlinks. This interactivity enriches the user experience, allowing for a deeper exploration and understanding of the underlying data.\nThe ggiraph package shines when you want to add a layer of engagement to your data visualizations. With it, viewers can hover over specific elements to see more details or click on parts of the graph to access external resources. This capability is invaluable for online reports, where reader engagement and interactivity are paramount.\nFor HTML-based reports created with Quarto or R Markdown, ggiraph enhances the storytelling potential by making data visualizations a two-way interaction channel. This feature is especially useful for exploratory data analysis, educational materials, or any report aiming to provide an immersive data exploration experience. While ggiraph excels in web environments, the static versions of these enriched plots still retain their aesthetic and informational value in PDF or DOCX outputs.\nHere’s a basic example of how to create an interactive plot with ggiraph, making use of a simple ggplot2 bar chart:\n# Example taken from https://www.productive-r-workflow.com/quarto-tricks#ggiraph\n# It was too good not to share it with you.\n# You can find more Quatro tricks on this site. \n\nlibrary(ggplot2)\nlibrary(ggiraph)\nlibrary(patchwork)\n\n# Example data - replace with your data\nmap_data &lt;- data.frame(\n  id = 1:3,\n  lat = c(40, 42, 37),\n  lon = c(-100, -120, -95),\n  group = c(\"A\", \"B\", \"C\")\n)\n\nline_data &lt;- data.frame(\n  id = rep(1:3, each = 10),\n  time = rep(seq(as.Date(\"2021-01-01\"), by = \"1 month\", length.out = 10), 3),\n  value = rnorm(30),\n  group = rep(c(\"A\", \"B\", \"C\"), each = 10)\n)\n\n# Map with interactive points\nmap_plot &lt;- ggplot() +\n  borders(\"world\", colour = \"gray80\", fill = \"gray90\") +  # Add a world map background\n  geom_point_interactive(data = map_data, aes(x = lon, y = lat, size = 5, color=group, tooltip = group, data_id = group)) +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  coord_sf(xlim = c(-130, -65), ylim = c(10, 75)) \n\n\n# Line chart with interactive lines\nline_plot &lt;- ggplot(line_data, aes(x = time, y = value, group = group, color=group)) +\n  geom_line_interactive(aes(data_id = group, tooltip = group))\n\ncombined_plot &lt;- girafe(\n  ggobj = map_plot + plot_spacer() + line_plot + plot_layout(widths = c(0.35, 0, 0.65)),\n  options = list(\n    opts_hover(css = ''),\n    opts_hover_inv(css = \"opacity:0.1;\"), \n    opts_sizing(rescale = FALSE)\n  ),\n  height_svg = 4,\n  width_svg = 12\n)\n\n\n\nInteractive Plot\n\n\nThis example assumes a scenario where clicking on a point on the map would dynamically highlight the corresponding line on the line chart on the left. As you see, the alpha of lines for categories that are not pointed decreases to emphasize the clicked one.\n\n\nSeamless Plot Compositions with patchwork\nWhile ggiraph brings individual plots to life with interactivity, patchwork is the tool for harmoniously combining multiple ggplot2 plots into a cohesive composition. patchwork simplifies the process of arranging multiple plots, allowing for complex layouts that maintain a unified aesthetic. It’s akin to assembling a visual symphony from individual notes, where each plot plays its part in the overarching data narrative.\npatchwork excels in its flexibility and ease of use, offering a syntax that is both intuitive and powerful. It allows for the vertical, horizontal, and nested arrangement of plots, and gives you control over spacing, alignment, and even shared legends. This capability is invaluable when you need to compare different aspects of your data side by side or tell a multi-faceted story through a series of visualizations.\nIn both Quarto and R Markdown reports, regardless of the output format, patchwork enables you to create visually appealing and informative plot arrangements. For static reports (PDF, DOCX), these compositions can help convey complex information in a digestible format. For HTML reports, while patchwork does not add interactivity to the plots themselves, the strategic arrangement of visual elements can guide the reader’s exploration of the data.\nTo demonstrate the power of patchwork, let’s create a composition of two simple ggplot2 plots:\nlibrary(ggplot2)\nlibrary(patchwork)\n\n# First plot: A scatter plot\np1 &lt;- ggplot(mtcars, aes(mpg, disp)) + \n  geom_point(aes(color = cyl)) + \n  labs(title = \"Displacement vs. MPG\")\n\n# Second plot: A bar plot\np2 &lt;- ggplot(mtcars, aes(factor(cyl))) + \n  geom_bar(aes(fill = factor(cyl))) + \n  labs(title = \"Cylinder Count\")\n\n# Combine the plots with patchwork\nplot_combo &lt;- p1 + p2 + \n  plot_layout(ncol = 1, heights = c(1, 1)) +\n  plot_annotation(title = \"Vehicle Characteristics\")\n\n# Display the combined plot\nplot_combo\n\n\n\nCombined Plot\n\n\nThis example illustrates how patchwork seamlessly combines two distinct ggplot2 plots into a single, coherent visual statement. By arranging plots in a thoughtfully designed layout, you can enhance the storytelling impact of your data visualizations in reports.\n\n\nMastering Your Data Visualization Arsenal\nOur journey through the landscape of R packages for enhancing reports in Quarto and R Markdown mirrors the pivotal scene from The Matrix, where an array of tools is summoned with a clear mission in mind. In our narrative, these tools — ggplot2, plotly, leaflet, DT, gt, ggiraph, and patchwork—form a robust arsenal, each offering unique capabilities to make our data reports not just informative, but compelling and engaging.\n\nggplot2 laid the foundation, offering a versatile platform for creating a wide range of plots with deep customization options, ensuring that every chart precisely conveys its intended message.\nplotly and ggiraph introduced interactivity, transforming static images into dynamic conversations, inviting readers to explore and interact with the data on their terms.\nleaflet allowed us to map our narratives, providing geographical context and making location data more accessible and understandable.\nDT and gt revolutionized how we present tabular data, turning dense tables into clear, engaging visual elements of our reports.\npatchwork taught us the art of composition, enabling us to weave individual plots into coherent visual stories that guide the reader through our analyses seamlessly.\n\nEach of these packages can be seen as a different type of “firearm” in our data visualization arsenal, equipped to tackle specific challenges and objectives in the realm of digital reporting. Whether we’re aiming for clarity, engagement, interactivity, or all of the above, our toolkit is now fully stocked to bring any data story to life.\nAs we conclude this exploration, remember that the true power of these tools lies not just in their individual capabilities but in how they can be combined to tell a cohesive, compelling story. Just as Neo chose his arsenal for the mission ahead, you now have the knowledge to select the right tools for your data visualization needs, ensuring your reports are not only seen but remembered.\nThe landscape of data storytelling is vast and ever-changing, but with this arsenal at your disposal, you’re well-equipped to make your mark. So, take these tools, explore their potential, and start crafting data stories that resonate, inform, and inspire."
  },
  {
    "objectID": "ds/posts/2024-04-18_Navigating-the-Data-Pipes--An-R-Programming-Journey-with-Mario-Bros--1aa621af1926.html",
    "href": "ds/posts/2024-04-18_Navigating-the-Data-Pipes--An-R-Programming-Journey-with-Mario-Bros--1aa621af1926.html",
    "title": "Navigating the Data Pipes: An R Programming Journey with Mario Bros.",
    "section": "",
    "text": "Welcome to the Mushroom Kingdom\n\n\n\nImage\n\n\nIn the vast and varied landscape of data analysis, navigating through complex datasets and transformation processes can often feel like an adventure through unknown lands. For those who embark on this journey using R, there’s a powerful tool at their disposal, reminiscent of the magical pipes found in the iconic Mushroom Kingdom of the Mario Bros. series: piping.\nJust as Mario relies on green pipes to travel quickly and safely across the kingdom, data scientists and analysts use piping in R to streamline their data processing workflows. Piping allows for the output of one function to seamlessly become the input of the next, creating a fluid and understandable sequence of data transformations. This method not only makes our code cleaner and more readable but also transforms the coding process into an adventure, guiding data from its raw state to insightful conclusions.\nThe concept of piping in R, introduced through packages like magrittr and now embraced in base R with the |&gt; operator, is a game-changer. It simplifies the way we write and think about code, turning complex sequences of functions into a straightforward, linear progression of steps. Imagine, if you will, entering a green pipe with your raw data in hand, hopping from one transformation to the next, and emerging with insights as clear and vibrant as the flag at the end of a Mario level.\nIn this journey, we’ll explore the tools and techniques that make such transformations possible, delve into the power-ups that enhance our piping strategies, and learn how to navigate the challenges and obstacles that arise along the way. So, let’s jump into that first green pipe and start our adventure through the data pipes of R programming.\n\n\nJumping Into the Green Pipe\n\nEntering the World of R Piping\nIn the world of R programming, the journey through data analysis often begins with raw, unstructured data. Just as Mario stands at the entrance of a green pipe, pondering the adventures that lie ahead, so do we stand at the precipice of our data analysis journey, ready to transform our data into insightful conclusions. The tool that enables this seamless journey is known as piping. Piping, in R, is symbolized by operators such as %&gt;% from the magrittr package and the native |&gt; introduced in R version 4.1.0.\n\n\nThe Basics of Pipe Travel\nTo understand the power of piping, let’s start with a simple example using R’s built-in mtcars dataset. Imagine you want to calculate the average miles per gallon (MPG) for cars with different numbers of cylinders.\nWithout piping, the code might look fragmented and harder to read:\nmean(subset(mtcars, cyl == 4)$mpg)\nHowever, with the magic of the %&gt;% pipe, our code transforms into a clear and linear sequence:\nlibrary(magrittr)\nmtcars %&gt;% \n  subset(cyl == 4) %&gt;% \n  .$mpg %&gt;% \n  mean()\nThis sequence of operations, akin to Mario hopping from one platform to the next, is not only more readable but also easier to modify and debug.\n\n\nLevel Up: Exploring the magrittr and Base R Pipes\nWhile the %&gt;% operator from the magrittr package has been widely celebrated for its clarity and functionality, the introduction of the native |&gt; pipe in base R offers a streamlined alternative. Let’s compare how each can be used to achieve similar outcomes:\n\nUsing magrittr’s %&gt;%:\n\nlibrary(magrittr)\nmtcars %&gt;% \n  filter(cyl == 6) %&gt;% \n  select(mpg, wt) %&gt;% \n  head()\n\nUsing base R’s |&gt;:\n\nmtcars |&gt; \n  subset(cyl == 6, select = c(mpg, wt)) |&gt;\n  head()\nEach pipe has its context and advantages, and understanding both allows us to choose the best tool for our coding journey.\n\n\n\nThe Power-Ups: Enhancing Your Journey\nIn the Mario Bros. universe, power-ups like mushrooms, fire flowers, and super stars provide Mario with the extra abilities he needs to navigate through the Mushroom Kingdom. Similarly, in the world of R programming, there are “power-ups” that enhance the functionality of our pipes, making our data analysis journey smoother and more efficient.\n\nMagrittr’s Magic Mushrooms: Additional Features\nThe magrittr package doesn’t just stop at the %&gt;% pipe operator; it offers several other functionalities that can significantly power up your data manipulation game. These include the compound assignment pipe operator %&lt;&gt;%, which allows you to update a dataset in place, and the tee operator %T&gt;%, which lets you branch out the pipeline for side operations. Think of these as the Super Mushrooms and Fire Flowers of your R scripting world, empowering you to tackle bigger challenges with ease.\n\nExample of %&lt;&gt;%:\n\nlibrary(magrittr)\nmtcars2 = mtcars \n\nmtcars %&lt;&gt;% \n  transform(mpg = mpg * 1.60934) \n\n\n\nImage\n\n\n\nExample of %T&gt;%:\n\nlibrary(magrittr)\n\nmtcars %T&gt;% \n  plot(mpg ~ wt, data = .) %&gt;% # We are generating plot \"meanwhile\", without changing process\n  filter(cyl == 4) %&gt;% \n  select(mpg, wt)\n\n\nThe Fire Flower: Filtering and Selecting Data\nJust as the Fire Flower gives Mario the ability to throw fireballs, the dplyr package (which integrates seamlessly with magrittr’s piping) equips us with powerful functions like filter() and select(). These functions allow us to narrow down our data to the most relevant pieces, throwing away what we don’t need and keeping what’s most useful.\n\nFiltering data:\n\nlibrary(dplyr)\nmtcars %&gt;% \n  filter(mpg &gt; 20) %&gt;% \n  select(mpg, cyl, gear)\n\n# Keeps only cars with MPG greater than 20, selecting relevant columns.\nThis process of filtering and selecting is like navigating through a level with precision, avoiding obstacles and focusing on the goal.\n\n\nSide Quest: Joining Data Frames\nOur data analysis journey often requires us to merge different data sources, akin to Mario teaming up with Luigi or Princess Peach. The dplyr package provides several functions for this purpose, such as inner_join(), left_join(), and more, allowing us to bring together disparate data sets into a unified whole.\n# Assuming we have another data frame, car_details, with additional information on cars.\nmtcars %&gt;% \n  inner_join(car_details, by = \"model\") \n\n# Combines data based on the \"model\" column.\n\n\nBoss Level: Grouped Operations\nFinally, much like facing a boss in a Mario game, grouped operations in R require a bit of strategy. Using the group_by() function from dplyr, we can perform operations on our data grouped by certain criteria, effectively handling what could otherwise be a daunting task.\nmtcars %&gt;% \n  group_by(cyl) %&gt;% \n  summarise(avg_mpg = mean(mpg)) \n\n# Calculates the average MPG for cars, grouped by cylinder count.\n\n\n\nAvoiding Goombas: Debugging Your Pipe\nIn the realms of the Mushroom Kingdom, Mario encounters various obstacles, from Goombas to Koopa Troopas, each requiring a unique strategy to overcome. Similarly, as we navigate through our data analysis pipeline in R, we’re bound to run into issues — our own version of Goombas and Koopas — that can disrupt our journey. Debugging becomes an essential skill, allowing us to identify and address these challenges without losing our progress.\n\nSpotting and Squashing Bugs\nJust as Mario needs to stay vigilant to spot Goombas on his path, we need to be observant of the potential errors in our pipeline. Errors can arise from various sources: incorrect data types, unexpected missing values, or simply syntax errors. To spot these issues, it’s crucial to test each segment of our pipeline independently, ensuring that each step produces the expected output.\nConsider using the print() or View() functions strategically to inspect the data at various stages of your pipeline. This approach is akin to Mario checking his surroundings carefully before making his next move.\nlibrary(dplyr)\n\nmtcars %&gt;% \n  filter(mpg &gt; 20) %&gt;% \n  View()  # Inspect the filtered dataset\n\n\nThe ViewPipeSteps Tool: Your Map Through the Mushroom Kingdom\nThe ViewPipeSteps package acts like a map through the Mushroom Kingdom, providing visibility into each step of our journey. By allowing us to view the output at each stage of our pipeline, it helps us identify exactly where things might be going wrong.\nTo use ViewPipeSteps, you’d typically wrap your pipeline within the print_pipe_steps() function, which then executes each step interactively, printing the results so you can inspect the data at each point.\nExample:\nlibrary(ViewPipeSteps)\n\ndiamonds %&gt;% \n  filter(color == \"E\", cut == \"Ideal\") %&gt;% \n  select(carat, cut, price) %&gt;%\n  print_pipe_steps()\n1. diamonds\n# A tibble: 53,940 × 10\n   carat cut       color clarity depth table price     x     y     z\n   &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n 3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n 4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n 5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n 6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n 7  0.24 Very Good I     VVS1     62\nYou can also use another feature of this package, its addin. You just need to select pipe you want to check, find and click addin’s function “View Pipe Chain Steps” and voila!\n\n\n\nImage\n\n\n\n\n\nImage\n\n\n\n\nNavigating Complex Pipes: When to Use Warp Pipes\nSometimes, our data processing tasks are so complex that they feel like navigating through Bowser’s Castle. In these situations, breaking down our pipeline into smaller, manageable segments can be incredibly helpful. This approach is similar to finding secret Warp Pipes in Mario that allow you to bypass difficult levels, making the journey less daunting.\nFor instance, if a particular transformation is complicated, consider isolating it into its own script or function. Test it thoroughly until you’re confident it works as expected, then integrate it back into your main pipeline. This method ensures that each part of your pipeline is robust and less prone to errors.\n\n\n\nBowser’s Castle: Tackling Complex Data Challenges\nAs we near the end of our journey in the Mushroom Kingdom of R programming, we face the ultimate test of our skills: Bowser’s Castle. This chapter represents the complex data challenges that often seem as daunting as the fire-breathing dragon himself. However, just as Mario uses his skills, power-ups, and a bit of strategy to rescue Princess Peach, we’ll employ advanced piping techniques, performance considerations, and the power of collaboration to conquer these challenges.\n\nAdvanced Piping Techniques\nTo navigate through Bowser’s Castle, Mario must leverage every skill and power-up acquired throughout his journey. Similarly, tackling complex data tasks requires a sophisticated understanding of piping and the ability to combine various R functions and packages seamlessly.\n\nUsing purrr for Functional Programming:\n\nOne way to enhance our piping strategies is by integrating the purrr package, which allows for functional programming. This approach can be particularly powerful when dealing with lists or performing operations on multiple columns or datasets simultaneously.\nlibrary(purrr)\nlibrary(dplyr)\n\nmtcars %&gt;% \n  split(.$cyl) %&gt;% \n  map(~ .x %&gt;% summarise(avg_mpg = mean(mpg), avg_hp = mean(hp)))\n\n$`4`\n   avg_mpg   avg_hp\n1 26.66364 82.63636\n\n$`6`\n   avg_mpg   avg_hp\n1 19.74286 122.2857\n\n$`8`\n  avg_mpg   avg_hp\n1    15.1 209.2143\nThis example splits the mtcars dataset by cylinder count and then applies a summarization function to each subset, showcasing how purrr can work in tandem with dplyr and piping to handle complex data operations.\n\n\nBoss Battle: Performance Considerations\nIn every final boss battle, efficiency is key. The same goes for our R scripts when facing large datasets or complex transformations. Here, the choice of tools and techniques can significantly impact performance.\n\nVectorization Over Loops: Whenever possible, use vectorized operations, which are typically faster and more efficient than loops.\ndata.table for Large Data and dtplyr as a Secret Power-Up: The data.table package is renowned for its speed and efficiency with large datasets. But what if you could harness data.table’s power with dplyr’s syntax? Enter dtplyr, a bridge between these two worlds, allowing you to write dplyr code that is automatically translated into data.table operations behind the scenes. To use dtplyr, you’ll wrap your data.table in lazy_dt(), then proceed with dplyr operations as usual. The dtplyr package will translate these into data.table operations, maintaining the speed advantage without sacrificing the readability and familiarity of dplyr syntax.\n\nlibrary(data.table)\nlibrary(dtplyr)\nlibrary(dplyr)\n\n# Convert to a lazy data.table\nlazy_dt_cars &lt;- mtcars %&gt;% \n  as.data.table() %&gt;% \n  lazy_dt()\n\n# Perform dplyr operations\nlazy_dt_cars %&gt;% \n  group_by(cyl) %&gt;% \n  summarise(avg_mpg = mean(mpg), avg_hp = mean(hp)) \n\nSource: local data table [3 x 3]\nCall:   `_DT1`[, .(avg_mpg = mean(mpg), avg_hp = mean(hp)), keyby = .(cyl)]\n\n    cyl avg_mpg avg_hp\n  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1     4    26.7   82.6\n2     6    19.7  122. \n3     8    15.1  209. \n\n# Use as.data.table()/as.data.frame()/as_tibble() to access results\nThis approach can significantly reduce computation time, akin to finding a secret shortcut in Bowser’s Castle.\n\n\nThe Final Power-Up: Collaboration and Community\nMario rarely faces Bowser alone; he often has allies. In the world of data science and R programming, collaboration and community are equally valuable. Platforms like GitHub, Stack Overflow, and RStudio Community are akin to Mario’s allies, offering support, advice, and shared resources.\nSharing your code, seeking feedback, and collaborating on projects can enhance your skills, broaden your understanding, and help you tackle challenges that might initially seem insurmountable.\n\n\n\nLowering the Flag on Our Adventure\nAs our journey through the Mushroom Kingdom of R programming comes to a close, we lower the flag, signaling the end of a thrilling adventure. Along the way, we’ve navigated through green pipes of piping with %&gt;% and |&gt;, powered up our data transformation skills with dplyr and purrr, and avoided the Goombas of bugs with strategic debugging and the ViewPipeSteps tool. We’ve collected coins of insights through data visualization and summarization, tackled the complex challenges of Bowser’s Castle with data.table and dtplyr, and recognized the power of collaboration and community in our quest for data analysis mastery.\nOur expedition has shown us that, with the right tools and a bit of ingenuity, even the most daunting datasets can be transformed into valuable insights, much like Mario’s quest to rescue Princess Peach time and again proves that persistence, courage, and a few power-ups can overcome any obstacle.\nBut every end in the Mushroom Kingdom is merely the beginning of a new adventure. The skills and techniques we’ve acquired are not just for one-time use; they are the foundation upon which we’ll build our future data analysis projects. The world of R programming is vast and ever-evolving, filled with new packages to explore, techniques to master, and data challenges to conquer.\nSo, as we bid farewell to the Mushroom Kingdom for now, remember that in the world of data science, every question answered and every challenge overcome leads to new adventures. Keep exploring, keep learning, and above all, keep enjoying the journey.\nThank you for joining me on this adventure. May your path through the world of R programming be as exciting and rewarding as a quest in the Mushroom Kingdom. Until our next adventure!"
  },
  {
    "objectID": "ds/posts/2024-05-16_Shiny-and-Beyond--Mastering-Interactive-Web-Applications-with-R-and-Appsilon-Packages-d29d91e38ad2.html",
    "href": "ds/posts/2024-05-16_Shiny-and-Beyond--Mastering-Interactive-Web-Applications-with-R-and-Appsilon-Packages-d29d91e38ad2.html",
    "title": "Shiny and Beyond: Mastering Interactive Web Applications with R and Appsilon Packages",
    "section": "",
    "text": "Shiny and Beyond\n\n\n\n\nIn today’s data-driven world, the ability to create dynamic, interactive web applications is a highly valuable skill. Shiny, a package developed by RStudio, provides an elegant framework for building such applications using R. It enables data scientists and analysts to transform their analyses into interactive experiences, making data insights accessible and engaging. This article series will guide you through mastering Shiny, starting with the basics and gradually introducing more advanced concepts and tools, including powerful packages from Appsilon that enhance Shiny’s capabilities.\n\n\n\nShiny allows you to turn your R scripts into interactive web applications effortlessly. Whether you’re looking to create simple data visualizations or complex, multi-page applications, Shiny offers the flexibility and power needed to meet your objectives. Some key benefits include:\n\nEase of Use: Shiny’s syntax is intuitive, and if you are familiar with R, you can quickly start building applications.\nInteractive Data Exploration: Users can interact with data visualizations, filtering and modifying parameters in real-time to uncover insights.\nRapid Prototyping: Shiny allows for quick development and iteration, making it perfect for prototyping data products.\nIntegration with R: Leverage the full power of R, including its extensive library of packages for data manipulation, visualization, and analysis.\n\n\n\n\nBefore diving into creating your first Shiny application, ensure you have R and RStudio installed. Additionally, you’ll need to install the Shiny package if you haven’t already. Here’s how to set up your environment:\ninstall.packages(\"shiny\", repos = \"https://cloud.r-project.org\")\n\n\n\nA Shiny application consists of two main components:\n\nUI (User Interface): Defines the layout and appearance of your app.\nServer: Contains the logic that runs behind the scenes, processing inputs and generating outputs.\n\nLet’s create a simple Shiny app to demonstrate these components. The following code defines a basic app that allows users to interact with a dataset and visualize its contents.\n\n\n\nWe’ll create an app that displays the famous mtcars dataset. Users can select variables to plot and see the relationship between them.\nlibrary(shiny)\n\n# Define the UI\nui &lt;- fluidPage(\n  titlePanel(\"Mtcars Dataset Explorer\"),\n  sidebarLayout(\n    sidebarPanel(\n      selectInput(\"xvar\", \"X-axis variable\", choices = names(mtcars)),\n      selectInput(\"yvar\", \"Y-axis variable\", choices = names(mtcars), selected = \"mpg\")\n    ),\n    mainPanel(\n      plotOutput(\"scatterPlot\")\n    )\n  )\n)\n\n# Define the server logic\nserver &lt;- function(input, output) {\n  output$scatterPlot &lt;- renderPlot({\n    ggplot(mtcars, aes_string(x = input$xvar, y = input$yvar)) +\n      geom_point() +\n      labs(title = paste(\"Scatter plot of\", input$xvar, \"vs\", input$yvar))\n  })\n}\n\n# Run the application\nshinyApp(ui = ui, server = server)\n\n\n\nMtcars Scatter Plot\n\n\nThis simple example demonstrates the basic structure of a Shiny app, showcasing how user inputs can dynamically influence the output. With this foundation, we are ready to explore more advanced features and customizations in the next chapters, including leveraging powerful Appsilon packages to enhance our Shiny applications.\n\n\n\nBefore we dive into the powerful enhancements offered by Appsilon packages, it’s essential to thoroughly understand the capabilities of “vanilla” Shiny. This chapter will explore what Shiny can do out of the box, including its core features, customization options, and how it facilitates interactive data exploration. By mastering these foundational aspects, you will be well-prepared to leverage additional tools to create even more sophisticated applications.\n\n\n\nVanilla Shiny provides a robust framework for building interactive web applications directly from R. Its key features include:\n\nInteractive Widgets: Shiny offers a variety of input controls like sliders, dropdowns, text inputs, and date selectors. These widgets allow users to interact with your data and analyses dynamically.\nReactive Programming: At the heart of Shiny is its reactivity system, which ensures that the output updates automatically whenever the inputs change. This reactive model simplifies the development of interactive applications.\nDynamic User Interfaces: Shiny allows you to create UIs that change dynamically in response to user inputs. This enables the development of more interactive and responsive applications.\nSeamless Integration with R: Since Shiny is built for R, you can use any R package within your Shiny apps. This includes popular packages for data manipulation (dplyr), visualization (ggplot2), and machine learning (caret).\nExtensibility: Shiny applications can be extended with custom HTML, CSS, and JavaScript, allowing for more advanced customization and functionality.\n\n\n\n\nShiny provides a rich set of input controls that you can use to create interactive applications. Here are some commonly used widgets:\n\nSlider Input: Allows users to select a range of values.\n\nsliderInput(\"obs\", \"Number of observations:\", min = 1, max = 1000, value = 500)\n\nSelect Input: Provides a dropdown menu for users to select from a list of options.\n\nselectInput(\"var\", \"Variable:\", choices = names(mtcars))\n\nText Input: Allows users to enter text.\n\ntextInput(\"caption\", \"Caption:\", \"Data Summary\")\n\nDate Input: Allows users to select a date.\n\ndateInput(\"date\", \"Date:\", value = Sys.Date())\nThese widgets can be combined to create a rich user interface for your applications.\n\n\n\nReactivity is a core concept in Shiny that makes it easy to build interactive applications. Reactive expressions and observers automatically update outputs when their inputs change.\n\nReactive Expressions: Functions that return a value and automatically re-execute when their dependencies change.\n\nreactiveExpression &lt;- reactive({\n  input$sliderValue * 2\n})\n\nObservers: Functions that perform actions rather than returning values, and automatically re-execute when their dependencies change.\n\nobserve({\n  print(input$sliderValue)\n})\nHere’s an example demonstrating reactivity:\nlibrary(shiny)\n\n# Define the UI\nui &lt;- fluidPage(\n  titlePanel(\"Reactive Example\"),\n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\"num\", \"Number of observations:\", 1, 100, 50)\n    ),\n    mainPanel(\n      textOutput(\"value\"),\n      plotOutput(\"histPlot\")\n    )\n  )\n)\n\n# Define the server logic\nserver &lt;- function(input, output) {\n  output$value &lt;- renderText({\n    paste(\"You selected\", input$num, \"observations\")\n  })\n\n  output$histPlot &lt;- renderPlot({\n    hist(rnorm(input$num))\n  })\n}\n\n# Run the application\nshinyApp(ui = ui, server = server)\n\n\n\nReactive Example\n\n\nIn this example:\n\nThe text output (output$value) and the plot output (output$histPlot) are both reactive, updating automatically when the slider input (input$num) changes.\n\n\n\n\nWhile Shiny’s built-in functions are powerful, you may sometimes need more control over the UI’s appearance and behavior. Shiny allows you to use custom HTML and CSS for further customization.\nHere’s an example of incorporating custom HTML and CSS:\nlibrary(shiny)\n\n# Define the UI\nui &lt;- fluidPage(\n  tags$head(\n    tags$style(HTML(\"\n      body { background-color: #f7f7f7; }\n      h1 { color: #2c3e50; }\n      .well { background-color: #ecf0f1; }\n    \"))\n  ),\n  titlePanel(\"Custom Styled App\"),\n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\"num\", \"Number of observations:\", 1, 100, 50)\n    ),\n    mainPanel(\n      plotOutput(\"histPlot\")\n    )\n  )\n)\n\n# Define the server logic\nserver &lt;- function(input, output) {\n  output$histPlot &lt;- renderPlot({\n    hist(rnorm(input$num))\n  })\n}\n\n# Run the application\nshinyApp(ui = ui, server = server)\n\n\n\nCustom Styled App\n\n\nIn this example:\n\nWe used tags$head and tags$style to include custom CSS directly in the Shiny app.\nThe background color, header color, and well panel color have been customized using CSS.\n\n\n\n\nFor even more advanced interactivity and functionality, you can extend Shiny applications with custom JavaScript. Shiny provides hooks for integrating JavaScript code, allowing you to add custom behavior to your apps.\nHere’s an example of adding a custom JavaScript alert when a button is clicked:\nlibrary(shiny)\n\n# Define the UI\nui &lt;- fluidPage(\n  titlePanel(\"JavaScript Integration\"),\n  sidebarLayout(\n    sidebarPanel(\n      actionButton(\"alertButton\", \"Show Alert\")\n    ),\n    mainPanel(\n      plotOutput(\"histPlot\")\n    )\n  ),\n  tags$script(HTML(\"\n    $(document).on('click', '#alertButton', function() {\n      alert('Button clicked!');\n    });\n  \"))\n)\n\n# Define the server logic\nserver &lt;- function(input, output) {\n  output$histPlot &lt;- renderPlot({\n    hist(rnorm(100))\n  })\n}\n\n# Run the application\nshinyApp(ui = ui, server = server)\n\n\n\nJavaScript Integration\n\n\nIn this example:\n\nWe used tags$script to include custom JavaScript directly in the Shiny app.\nA JavaScript alert is displayed when the button is clicked.\n\nBy mastering these core features and customization options, you can create powerful and engaging Shiny applications. In the next chapter, we will explore how to enhance these applications further with Appsilon’s styling packages, adding even more capabilities and visual appeal to your Shiny projects.\n\n\n\nThe user interface (UI) is a critical aspect of any web application, as it determines how users interact with your app and how accessible and engaging it is. In Shiny, the default UI components are functional but can sometimes look plain and lack the polish needed for professional applications. This is where Appsilon’s styling packages come in. By using shiny.semantic, shiny.fluent, and semantic.dashboard, you can create visually appealing and highly interactive UIs that stand out.\n\n\n\nshiny.semantic allows you to use Semantic UI, a front-end framework that provides a wide range of theming options and UI components, within your Shiny applications. This integration helps you create modern, responsive, and user-friendly interfaces without needing extensive knowledge of HTML or CSS.\nTo start using shiny.semantic, you’ll first need to install and load the package:\ninstall.packages(\"shiny.semantic\", repos = \"https://cloud.r-project.org\")\nlibrary(shiny.semantic)\nLet’s enhance our previous mtcars app with shiny.semantic to give it a more modern look:\nlibrary(shiny)\nlibrary(shiny.semantic)\nlibrary(ggplot2)\n\n# Define the UI with shiny.semantic\nui &lt;- semanticPage(\n  title = \"Mtcars Dataset Explorer\",\n  segment(\n    title = \"Mtcars Dataset Explorer\",\n    sidebar_layout(\n      sidebar_panel(\n        selectInput(\"xvar\", \"X-axis variable\", choices = names(mtcars)),\n        selectInput(\"yvar\", \"Y-axis variable\", choices = names(mtcars), selected = \"mpg\")\n      ),\n      main_panel(\n        plotOutput(\"scatterPlot\")\n      )\n    )\n  )\n)\n\n# Define the server logic\nserver &lt;- function(input, output) {\n  output$scatterPlot &lt;- renderPlot({\n    ggplot(mtcars, aes_string(x = input$xvar, y = input$yvar)) +\n      geom_point() +\n      labs(title = paste(\"Scatter plot of\", input$xvar, \"vs\", input$yvar))\n  })\n}\n\n# Run the application\nshinyApp(ui = ui, server = server)\n\n\n\nEnhanced Mtcars App\n\n\nIn this enhanced version:\n\nWe replaced fluidPage with semanticPage to utilize Semantic UI.\nWe used segment and sidebar_layout to structure the UI components.\nThe overall look is more modern and visually appealing compared to the default Shiny components.\n\n\n\n\nFor more complex applications that require a dashboard layout, semantic.dashboard offers powerful tools to create sophisticated dashboards with ease. It extends shiny.semantic and adds pre-styled dashboard components.\nHere’s an example of a dashboard layout for our mtcars app:\nlibrary(shiny)\nlibrary(semantic.dashboard)\nlibrary(ggplot2)\n\n# Define the UI with semantic.dashboard\nui &lt;- dashboardPage(\n  dashboardHeader(title = \"Mtcars Dashboard\"),\n  dashboardSidebar(\n    sidebarMenu(\n      menuItem(\"Dashboard\", tabName = \"dashboard\", icon = icon(\"dashboard\")),\n      menuItem(\"Data Explorer\", tabName = \"dataexplorer\", icon = icon(\"table\"))\n    )\n  ),\n  dashboardBody(\n    tabItems(\n      tabItem(tabName = \"dashboard\",\n              fluidRow(\n                box(title = \"Controls\", width = 4, \n                    selectInput(\"xvar\", \"X-axis variable\", choices = names(mtcars)),\n                    selectInput(\"yvar\", \"Y-axis variable\", choices = names(mtcars), selected = \"mpg\")\n                ),\n                box(title = \"Scatter Plot\", width = 8, plotOutput(\"scatterPlot\"))\n              )\n      ),\n      tabItem(tabName = \"dataexplorer\",\n              dataTableOutput(\"dataTable\")\n      )\n    )\n  )\n)\n\n# Define the server logic\nserver &lt;- function(input, output) {\n  output$scatterPlot &lt;- renderPlot({\n    ggplot(mtcars, aes_string(x = input$xvar, y = input$yvar)) +\n      geom_point() +\n      labs(title = paste(\"Scatter plot of\", input$xvar, \"vs\", input$yvar))\n  })\n  \n  output$dataTable &lt;- renderDataTable({\n    mtcars\n  })\n}\n\n# Run the application\nshinyApp(ui = ui, server = server)\n\n\n\nMtcars Dashboard\n\n\nIn this dashboard version:\n\nWe used dashboardPage, dashboardHeader, dashboardSidebar, and dashboardBody to create a structured layout.\nThe sidebar contains a menu for navigation.\nThe body is divided into two tabs: one for the scatter plot and one for exploring the data table.\n\n\n\n\nshiny.fluent integrates Microsoft’s Fluent UI into Shiny applications, providing a rich set of controls and styles. It is particularly useful for creating applications with a Microsoft Office-like feel.\nHere’s how you can use shiny.fluent to enhance the mtcars app:\nlibrary(shiny)\nlibrary(shiny.fluent)\nlibrary(ggplot2)\n\n# Define the UI with shiny.fluent\nui &lt;- fluentPage(\n  Text(variant = \"xxLarge\", content = \"Mtcars Dataset Explorer\"),\n  Stack(\n    tokens = list(childrenGap = 10),\n    Dropdown.shinyInput(\"xvar\", label = \"X-axis variable\", \n                        options = lapply(names(mtcars), function(x) list(key = x, text = x)),\n                        value = \"mpg\"),\n    Dropdown.shinyInput(\"yvar\", label = \"Y-axis variable\", \n                        options = lapply(names(mtcars), function(x) list(key = x, text = x)),\n                        value = \"hp\"),\n    plotOutput(\"scatterPlot\")\n  )\n)\n\n# Define the server logic\nserver &lt;- function(input, output, session) {\n  output$scatterPlot &lt;- renderPlot({\n    ggplot(mtcars, aes_string(x = input$xvar, y = input$yvar)) +\n      geom_point() +\n      labs(title = paste(\"Scatter plot of\", input$xvar, \"vs\", input$yvar))\n  })\n}\n\n# Run the application\nshinyApp(ui = ui, server = server)\n\n\n\nFluent UI Mtcars App\n\n\nIn this example:\n\nDropdown.shinyInput is used to create dropdowns for the x-axis and y-axis variables.\nThe Dropdown component’s options argument is correctly set up with key and text fields.\nplotOutput is used to display the scatter plot.\nThe server logic captures the input selections and updates the plot accordingly.\n\n\n\n\nEnsuring that your applications are accessible and user-friendly is crucial. Here are some tips:\n\nUse shiny.i18n for Internationalization: shiny.i18n makes it easy to translate your Shiny apps into multiple languages, ensuring they are accessible to a broader audience.\nConsistent Styling: Maintain consistent styles across your application for a professional look and feel.\nResponsive Design: Ensure your app works well on different devices and screen sizes.\n\nBy leveraging these Appsilon packages, you can create visually appealing, user-friendly, and highly interactive Shiny applications. In the next chapter, we will delve into advanced reactivity and routing, further enhancing the interactivity and user experience of your applications.\n\n\n\nWith a solid understanding of Shiny’s core capabilities and how to enhance the UI using Appsilon’s styling packages, it’s time to delve into more advanced features. This chapter focuses on leveraging advanced reactivity with shiny.react and implementing efficient navigation using shiny.router. These tools will help you create more dynamic, responsive, and user-friendly applications.\n\n\n\nshiny.react is a package that brings the power of React.js, a popular JavaScript library for building user interfaces, into Shiny. By using shiny.react, you can create highly responsive and interactive components that enhance the user experience.\nLet’s enhance our previous mtcars app with shiny.react to add more responsive components:\nlibrary(shiny)\nlibrary(shiny.react)\nlibrary(shiny.fluent)\nlibrary(ggplot2)\n\n# Define the UI with shiny.react and shiny.fluent\nui &lt;- fluentPage(\n  Text(variant = \"xxLarge\", content = \"Mtcars Dataset Explorer\"),\n  Stack(\n    tokens = list(childrenGap = 10),\n    Dropdown.shinyInput(\"xvar\", label = \"X-axis variable\", \n                        options = lapply(names(mtcars), function(x) list(key = x, text = x)),\n                        value = \"mpg\"),\n    Dropdown.shinyInput(\"yvar\", label = \"Y-axis variable\", \n                        options = lapply(names(mtcars), function(x) list(key = x, text = x)),\n                        value = \"hp\"),\n    plotOutput(\"scatterPlot\")\n  )\n)\n\n# Define the server logic\nserver &lt;- function(input, output, session) {\n  output$scatterPlot &lt;- renderPlot({\n    ggplot(mtcars, aes_string(x = input$xvar, y = input$yvar)) +\n      geom_point() +\n      labs(title = paste(\"Scatter plot of\", input$xvar, \"vs\", input$yvar))\n  })\n}\n\n# Run the application\nshinyApp(ui = ui, server = server)\nIn this code:\n\nDropdown.shinyInput is used to create dropdown inputs, integrating Fluent UI with Shiny reactivity.\nThe Dropdown component’s options argument is correctly set up with key and text fields.\nThe fluentPage function is used to structure the UI.\n\n\n\n\nAs your Shiny applications grow in complexity, managing navigation and routing becomes crucial. shiny.router is a package that provides a simple way to add routing to your Shiny apps, allowing you to create single-page applications (SPAs) with multiple views.\n\n\n\nWith the basics of Shiny and enhanced UI elements covered, it’s time to delve into the core functionality that makes Shiny a powerful tool for data science and visualization. In this chapter, we will explore how to handle data within Shiny applications, create dynamic reports, and integrate advanced visualization libraries to provide insightful and interactive data presentations.\n\n\n\nEfficient data handling is crucial for any Shiny application, especially when dealing with large datasets or complex analyses. Shiny provides several mechanisms to manage data effectively, including reactive expressions and data caching.\n\n\n\nReactivity is at the heart of Shiny, allowing applications to respond to user inputs dynamically. Here’s an example of how to use reactive expressions to handle data in Shiny:\nlibrary(shiny)\nlibrary(ggplot2)\n\n# Define UI\nui &lt;- fluidPage(\n  titlePanel(\"Reactive Data Example\"),\n  sidebarLayout(\n    sidebarPanel(\n      numericInput(\"obs\", \"Number of observations:\", 1000, min = 1, max = 10000)\n    ),\n    mainPanel(\n      plotOutput(\"distPlot\")\n    )\n  )\n)\n\n# Define server logic\nserver &lt;- function(input, output) {\n  # Reactive expression to generate random data\n  data &lt;- reactive({\n    rnorm(input$obs)\n  })\n  \n  # Render plot\n  output$distPlot &lt;- renderPlot({\n    ggplot(data.frame(x = data()), aes(x)) +\n      geom_histogram(binwidth = 0.2) +\n      labs(title = \"Histogram of Randomly Generated Data\")\n  })\n}\n\n# Run the application\nshinyApp(ui = ui, server = server)\n\n\n\nReactive Data Example\n\n\nIn this example:\n\nA numericInput allows the user to specify the number of observations.\nA reactive expression data() generates random data based on the user input.\nThe renderPlot function uses this reactive data to generate and display a histogram.\n\n\n\n\nShiny can be combined with rmarkdown and knitr to create dynamic reports that update based on user inputs. This is particularly useful for generating customized reports on the fly.\nHere’s an example of a simple Shiny app that generates a report using rmarkdown:\nlibrary(shiny)\nlibrary(rmarkdown)\n\n# Define UI\nui &lt;- fluidPage(\n  titlePanel(\"Dynamic Report Example\"),\n  sidebarLayout(\n    sidebarPanel(\n      numericInput(\"obs\", \"Number of observations:\", 1000, min = 1, max = 10000),\n      downloadButton(\"report\", \"Generate Report\")\n    ),\n    mainPanel(\n      plotOutput(\"distPlot\")\n    )\n  )\n)\n\n# Define server logic\nserver &lt;- function(input, output) {\n  # Reactive expression to generate random data\n  data &lt;- reactive({\n    rnorm(input$obs)\n  })\n  \n  # Render plot\n  output$distPlot &lt;- renderPlot({\n    ggplot(data.frame(x = data()), aes(x)) +\n      geom_histogram(binwidth = 0.2) +\n      labs(title = \"Histogram of Randomly Generated Data\")\n  })\n  \n  # Generate report\n  output$report &lt;- downloadHandler(\n    filename = function() {\n      paste(\"report-\", Sys.Date(), \".html\", sep = \"\")\n    },\n    content = function(file) {\n      tempReport &lt;- file.path(tempdir(), \"report.Rmd\")\n      file.copy(\"report.Rmd\", tempReport, overwrite = TRUE)\n      \n      params &lt;- list(obs = input$obs)\n      \n      rmarkdown::render(tempReport, output_file = file,\n                        params = params,\n                        envir = new.env(parent = globalenv()))\n    }\n  )\n}\n\n# Run the application\nshinyApp(ui = ui, server = server)\n\n\n\nDynamic Report Example\n\n\nFor this example to work, you’ll need a report.Rmd file in your working directory with the following content:\n\n---\ntitle: \"Dynamic Report\"\noutput: html_document\nparams:\n  obs: 1\n---\n\n\n```r\nknitr::opts_chunk$set(echo = TRUE)\n```\n\n## Report\nThis report was generated dynamically using rmarkdown.\n\nThe number of observations selected was `r params$obs`.\n\n```r\ndata &lt;- rnorm(params$obs)\nhist(data, main = \"Histogram of Randomly Generated Data\")\n```\n\n\n\nEnhancing your Shiny applications with Appsilon’s powerful extensions can significantly improve functionality, usability, and visual appeal. This chapter provides an overview of key Appsilon packages, such as shiny.semantic, shiny.fluent, semantic.dashboard, shiny.i18n, shiny.router, and shiny.react.\n\n\n\nshiny.semantic:\n\nIntegrates Semantic UI for modern, responsive designs.\nOffers a wide range of UI components and theming options.\n\nshiny.fluent:\n\nUses Microsoft’s Fluent UI framework for styling.\nProvides consistent and visually appealing UI elements.\n\nsemantic.dashboard:\n\nExtends shiny.semantic to create sophisticated dashboards.\nIncludes pre-styled components for interactive and appealing dashboards.\n\nshiny.i18n:\n\nFacilitates internationalization and localization.\nEnables translation of Shiny apps into multiple languages, improving accessibility.\n\nshiny.router:\n\nImplements routing for single-page applications.\nManages navigation and structure of large applications efficiently.\n\nshiny.react:\n\nIntegrates React.js components into Shiny.\nEnhances interactivity and responsiveness of Shiny applications.\n\n\n\n\n\nUI Enhancement with shiny.semantic and shiny.fluent: Transforming basic Shiny apps into modern, responsive applications using Semantic UI and Fluent UI frameworks.\nCreating Dashboards with semantic.dashboard: Building interactive and visually appealing dashboards using pre-styled components.\nInternationalization with shiny.i18n: Translating Shiny applications to make them accessible to a global audience.\nRouting with shiny.router: Adding navigation and structuring large applications as single-page apps.\nAdvanced Reactivity with shiny.react: Incorporating React.js for highly interactive and responsive UI components.\n\nUsing these Appsilon extensions, you can significantly enhance the capabilities of your Shiny applications. These tools enable you to create more robust, user-friendly, and visually appealing applications, tailored to meet the needs of diverse users and complex projects.\n\n\n\nIn this article, we have explored how to harness the power of Shiny for building interactive web applications in R, leveraging advanced UI frameworks, modular development, and data visualization techniques. By integrating Appsilon’s extensions, you can significantly enhance the functionality, usability, and visual appeal of your Shiny applications.\nWhile this guide covers various aspects of Shiny development, it’s important to note that deploying Shiny applications online is a crucial step that we haven’t delved into in detail. As I’m not an expert in deployment, I recommend the following resources for learning how to deploy Shiny applications:\n\nGetting Started with ShinyApps.io\nIntroduction to Shiny Server\nR Shiny Docker: How To Run Shiny Apps in a Docker Container\nThe Ultimate Guide to Deploying a Shiny App on AWS\nHow To Set Up Shiny Server on Ubuntu 20.04\n\nBy exploring these resources, you can learn how to make your Shiny applications accessible to users worldwide, ensuring they are robust, scalable, and secure.\nThank you for following along with chapters on mastering Shiny and its extensions. I hope you found the information valuable and that it helps you in your journey to creating powerful, interactive web applications with R."
  },
  {
    "objectID": "ds/posts/2024-05-16_Shiny-and-Beyond--Mastering-Interactive-Web-Applications-with-R-and-Appsilon-Packages-d29d91e38ad2.html#introduction-to-shiny-and-interactive-web-applications",
    "href": "ds/posts/2024-05-16_Shiny-and-Beyond--Mastering-Interactive-Web-Applications-with-R-and-Appsilon-Packages-d29d91e38ad2.html#introduction-to-shiny-and-interactive-web-applications",
    "title": "Shiny and Beyond: Mastering Interactive Web Applications with R and Appsilon Packages",
    "section": "",
    "text": "In today’s data-driven world, the ability to create dynamic, interactive web applications is a highly valuable skill. Shiny, a package developed by RStudio, provides an elegant framework for building such applications using R. It enables data scientists and analysts to transform their analyses into interactive experiences, making data insights accessible and engaging. This article series will guide you through mastering Shiny, starting with the basics and gradually introducing more advanced concepts and tools, including powerful packages from Appsilon that enhance Shiny’s capabilities."
  },
  {
    "objectID": "ds/posts/2024-05-16_Shiny-and-Beyond--Mastering-Interactive-Web-Applications-with-R-and-Appsilon-Packages-d29d91e38ad2.html#purpose-and-benefits-of-shiny",
    "href": "ds/posts/2024-05-16_Shiny-and-Beyond--Mastering-Interactive-Web-Applications-with-R-and-Appsilon-Packages-d29d91e38ad2.html#purpose-and-benefits-of-shiny",
    "title": "Shiny and Beyond: Mastering Interactive Web Applications with R and Appsilon Packages",
    "section": "",
    "text": "Shiny allows you to turn your R scripts into interactive web applications effortlessly. Whether you’re looking to create simple data visualizations or complex, multi-page applications, Shiny offers the flexibility and power needed to meet your objectives. Some key benefits include:\n\nEase of Use: Shiny’s syntax is intuitive, and if you are familiar with R, you can quickly start building applications.\nInteractive Data Exploration: Users can interact with data visualizations, filtering and modifying parameters in real-time to uncover insights.\nRapid Prototyping: Shiny allows for quick development and iteration, making it perfect for prototyping data products.\nIntegration with R: Leverage the full power of R, including its extensive library of packages for data manipulation, visualization, and analysis."
  },
  {
    "objectID": "ds/posts/2024-05-16_Shiny-and-Beyond--Mastering-Interactive-Web-Applications-with-R-and-Appsilon-Packages-d29d91e38ad2.html#getting-started-with-shiny",
    "href": "ds/posts/2024-05-16_Shiny-and-Beyond--Mastering-Interactive-Web-Applications-with-R-and-Appsilon-Packages-d29d91e38ad2.html#getting-started-with-shiny",
    "title": "Shiny and Beyond: Mastering Interactive Web Applications with R and Appsilon Packages",
    "section": "",
    "text": "Before diving into creating your first Shiny application, ensure you have R and RStudio installed. Additionally, you’ll need to install the Shiny package if you haven’t already. Here’s how to set up your environment:\ninstall.packages(\"shiny\", repos = \"https://cloud.r-project.org\")"
  },
  {
    "objectID": "ds/posts/2024-05-16_Shiny-and-Beyond--Mastering-Interactive-Web-Applications-with-R-and-Appsilon-Packages-d29d91e38ad2.html#basic-structure-of-a-shiny-app",
    "href": "ds/posts/2024-05-16_Shiny-and-Beyond--Mastering-Interactive-Web-Applications-with-R-and-Appsilon-Packages-d29d91e38ad2.html#basic-structure-of-a-shiny-app",
    "title": "Shiny and Beyond: Mastering Interactive Web Applications with R and Appsilon Packages",
    "section": "",
    "text": "A Shiny application consists of two main components:\n\nUI (User Interface): Defines the layout and appearance of your app.\nServer: Contains the logic that runs behind the scenes, processing inputs and generating outputs.\n\nLet’s create a simple Shiny app to demonstrate these components. The following code defines a basic app that allows users to interact with a dataset and visualize its contents."
  },
  {
    "objectID": "ds/posts/2024-05-16_Shiny-and-Beyond--Mastering-Interactive-Web-Applications-with-R-and-Appsilon-Packages-d29d91e38ad2.html#your-first-simple-app",
    "href": "ds/posts/2024-05-16_Shiny-and-Beyond--Mastering-Interactive-Web-Applications-with-R-and-Appsilon-Packages-d29d91e38ad2.html#your-first-simple-app",
    "title": "Shiny and Beyond: Mastering Interactive Web Applications with R and Appsilon Packages",
    "section": "",
    "text": "We’ll create an app that displays the famous mtcars dataset. Users can select variables to plot and see the relationship between them.\nlibrary(shiny)\n\n# Define the UI\nui &lt;- fluidPage(\n  titlePanel(\"Mtcars Dataset Explorer\"),\n  sidebarLayout(\n    sidebarPanel(\n      selectInput(\"xvar\", \"X-axis variable\", choices = names(mtcars)),\n      selectInput(\"yvar\", \"Y-axis variable\", choices = names(mtcars), selected = \"mpg\")\n    ),\n    mainPanel(\n      plotOutput(\"scatterPlot\")\n    )\n  )\n)\n\n# Define the server logic\nserver &lt;- function(input, output) {\n  output$scatterPlot &lt;- renderPlot({\n    ggplot(mtcars, aes_string(x = input$xvar, y = input$yvar)) +\n      geom_point() +\n      labs(title = paste(\"Scatter plot of\", input$xvar, \"vs\", input$yvar))\n  })\n}\n\n# Run the application\nshinyApp(ui = ui, server = server)\n\n\n\nMtcars Scatter Plot\n\n\nThis simple example demonstrates the basic structure of a Shiny app, showcasing how user inputs can dynamically influence the output. With this foundation, we are ready to explore more advanced features and customizations in the next chapters, including leveraging powerful Appsilon packages to enhance our Shiny applications."
  },
  {
    "objectID": "ds/posts/2024-05-16_Shiny-and-Beyond--Mastering-Interactive-Web-Applications-with-R-and-Appsilon-Packages-d29d91e38ad2.html#exploring-the-capabilities-of-vanilla-shiny",
    "href": "ds/posts/2024-05-16_Shiny-and-Beyond--Mastering-Interactive-Web-Applications-with-R-and-Appsilon-Packages-d29d91e38ad2.html#exploring-the-capabilities-of-vanilla-shiny",
    "title": "Shiny and Beyond: Mastering Interactive Web Applications with R and Appsilon Packages",
    "section": "",
    "text": "Before we dive into the powerful enhancements offered by Appsilon packages, it’s essential to thoroughly understand the capabilities of “vanilla” Shiny. This chapter will explore what Shiny can do out of the box, including its core features, customization options, and how it facilitates interactive data exploration. By mastering these foundational aspects, you will be well-prepared to leverage additional tools to create even more sophisticated applications."
  },
  {
    "objectID": "ds/posts/2024-05-16_Shiny-and-Beyond--Mastering-Interactive-Web-Applications-with-R-and-Appsilon-Packages-d29d91e38ad2.html#core-features-of-vanilla-shiny",
    "href": "ds/posts/2024-05-16_Shiny-and-Beyond--Mastering-Interactive-Web-Applications-with-R-and-Appsilon-Packages-d29d91e38ad2.html#core-features-of-vanilla-shiny",
    "title": "Shiny and Beyond: Mastering Interactive Web Applications with R and Appsilon Packages",
    "section": "",
    "text": "Vanilla Shiny provides a robust framework for building interactive web applications directly from R. Its key features include:\n\nInteractive Widgets: Shiny offers a variety of input controls like sliders, dropdowns, text inputs, and date selectors. These widgets allow users to interact with your data and analyses dynamically.\nReactive Programming: At the heart of Shiny is its reactivity system, which ensures that the output updates automatically whenever the inputs change. This reactive model simplifies the development of interactive applications.\nDynamic User Interfaces: Shiny allows you to create UIs that change dynamically in response to user inputs. This enables the development of more interactive and responsive applications.\nSeamless Integration with R: Since Shiny is built for R, you can use any R package within your Shiny apps. This includes popular packages for data manipulation (dplyr), visualization (ggplot2), and machine learning (caret).\nExtensibility: Shiny applications can be extended with custom HTML, CSS, and JavaScript, allowing for more advanced customization and functionality."
  },
  {
    "objectID": "ds/posts/2024-05-16_Shiny-and-Beyond--Mastering-Interactive-Web-Applications-with-R-and-Appsilon-Packages-d29d91e38ad2.html#exploring-interactive-widgets",
    "href": "ds/posts/2024-05-16_Shiny-and-Beyond--Mastering-Interactive-Web-Applications-with-R-and-Appsilon-Packages-d29d91e38ad2.html#exploring-interactive-widgets",
    "title": "Shiny and Beyond: Mastering Interactive Web Applications with R and Appsilon Packages",
    "section": "",
    "text": "Shiny provides a rich set of input controls that you can use to create interactive applications. Here are some commonly used widgets:\n\nSlider Input: Allows users to select a range of values.\n\nsliderInput(\"obs\", \"Number of observations:\", min = 1, max = 1000, value = 500)\n\nSelect Input: Provides a dropdown menu for users to select from a list of options.\n\nselectInput(\"var\", \"Variable:\", choices = names(mtcars))\n\nText Input: Allows users to enter text.\n\ntextInput(\"caption\", \"Caption:\", \"Data Summary\")\n\nDate Input: Allows users to select a date.\n\ndateInput(\"date\", \"Date:\", value = Sys.Date())\nThese widgets can be combined to create a rich user interface for your applications."
  },
  {
    "objectID": "ds/posts/2024-05-16_Shiny-and-Beyond--Mastering-Interactive-Web-Applications-with-R-and-Appsilon-Packages-d29d91e38ad2.html#understanding-reactivity",
    "href": "ds/posts/2024-05-16_Shiny-and-Beyond--Mastering-Interactive-Web-Applications-with-R-and-Appsilon-Packages-d29d91e38ad2.html#understanding-reactivity",
    "title": "Shiny and Beyond: Mastering Interactive Web Applications with R and Appsilon Packages",
    "section": "",
    "text": "Reactivity is a core concept in Shiny that makes it easy to build interactive applications. Reactive expressions and observers automatically update outputs when their inputs change.\n\nReactive Expressions: Functions that return a value and automatically re-execute when their dependencies change.\n\nreactiveExpression &lt;- reactive({\n  input$sliderValue * 2\n})\n\nObservers: Functions that perform actions rather than returning values, and automatically re-execute when their dependencies change.\n\nobserve({\n  print(input$sliderValue)\n})\nHere’s an example demonstrating reactivity:\nlibrary(shiny)\n\n# Define the UI\nui &lt;- fluidPage(\n  titlePanel(\"Reactive Example\"),\n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\"num\", \"Number of observations:\", 1, 100, 50)\n    ),\n    mainPanel(\n      textOutput(\"value\"),\n      plotOutput(\"histPlot\")\n    )\n  )\n)\n\n# Define the server logic\nserver &lt;- function(input, output) {\n  output$value &lt;- renderText({\n    paste(\"You selected\", input$num, \"observations\")\n  })\n\n  output$histPlot &lt;- renderPlot({\n    hist(rnorm(input$num))\n  })\n}\n\n# Run the application\nshinyApp(ui = ui, server = server)\n\n\n\nReactive Example\n\n\nIn this example:\n\nThe text output (output$value) and the plot output (output$histPlot) are both reactive, updating automatically when the slider input (input$num) changes."
  },
  {
    "objectID": "ds/posts/2024-05-16_Shiny-and-Beyond--Mastering-Interactive-Web-Applications-with-R-and-Appsilon-Packages-d29d91e38ad2.html#customizing-the-ui-with-html-and-css",
    "href": "ds/posts/2024-05-16_Shiny-and-Beyond--Mastering-Interactive-Web-Applications-with-R-and-Appsilon-Packages-d29d91e38ad2.html#customizing-the-ui-with-html-and-css",
    "title": "Shiny and Beyond: Mastering Interactive Web Applications with R and Appsilon Packages",
    "section": "",
    "text": "While Shiny’s built-in functions are powerful, you may sometimes need more control over the UI’s appearance and behavior. Shiny allows you to use custom HTML and CSS for further customization.\nHere’s an example of incorporating custom HTML and CSS:\nlibrary(shiny)\n\n# Define the UI\nui &lt;- fluidPage(\n  tags$head(\n    tags$style(HTML(\"\n      body { background-color: #f7f7f7; }\n      h1 { color: #2c3e50; }\n      .well { background-color: #ecf0f1; }\n    \"))\n  ),\n  titlePanel(\"Custom Styled App\"),\n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\"num\", \"Number of observations:\", 1, 100, 50)\n    ),\n    mainPanel(\n      plotOutput(\"histPlot\")\n    )\n  )\n)\n\n# Define the server logic\nserver &lt;- function(input, output) {\n  output$histPlot &lt;- renderPlot({\n    hist(rnorm(input$num))\n  })\n}\n\n# Run the application\nshinyApp(ui = ui, server = server)\n\n\n\nCustom Styled App\n\n\nIn this example:\n\nWe used tags$head and tags$style to include custom CSS directly in the Shiny app.\nThe background color, header color, and well panel color have been customized using CSS."
  },
  {
    "objectID": "ds/posts/2024-05-16_Shiny-and-Beyond--Mastering-Interactive-Web-Applications-with-R-and-Appsilon-Packages-d29d91e38ad2.html#extending-shiny-with-javascript",
    "href": "ds/posts/2024-05-16_Shiny-and-Beyond--Mastering-Interactive-Web-Applications-with-R-and-Appsilon-Packages-d29d91e38ad2.html#extending-shiny-with-javascript",
    "title": "Shiny and Beyond: Mastering Interactive Web Applications with R and Appsilon Packages",
    "section": "",
    "text": "For even more advanced interactivity and functionality, you can extend Shiny applications with custom JavaScript. Shiny provides hooks for integrating JavaScript code, allowing you to add custom behavior to your apps.\nHere’s an example of adding a custom JavaScript alert when a button is clicked:\nlibrary(shiny)\n\n# Define the UI\nui &lt;- fluidPage(\n  titlePanel(\"JavaScript Integration\"),\n  sidebarLayout(\n    sidebarPanel(\n      actionButton(\"alertButton\", \"Show Alert\")\n    ),\n    mainPanel(\n      plotOutput(\"histPlot\")\n    )\n  ),\n  tags$script(HTML(\"\n    $(document).on('click', '#alertButton', function() {\n      alert('Button clicked!');\n    });\n  \"))\n)\n\n# Define the server logic\nserver &lt;- function(input, output) {\n  output$histPlot &lt;- renderPlot({\n    hist(rnorm(100))\n  })\n}\n\n# Run the application\nshinyApp(ui = ui, server = server)\n\n\n\nJavaScript Integration\n\n\nIn this example:\n\nWe used tags$script to include custom JavaScript directly in the Shiny app.\nA JavaScript alert is displayed when the button is clicked.\n\nBy mastering these core features and customization options, you can create powerful and engaging Shiny applications. In the next chapter, we will explore how to enhance these applications further with Appsilon’s styling packages, adding even more capabilities and visual appeal to your Shiny projects."
  },
  {
    "objectID": "ds/posts/2024-05-16_Shiny-and-Beyond--Mastering-Interactive-Web-Applications-with-R-and-Appsilon-Packages-d29d91e38ad2.html#ui-design-with-appsilons-styling-packages",
    "href": "ds/posts/2024-05-16_Shiny-and-Beyond--Mastering-Interactive-Web-Applications-with-R-and-Appsilon-Packages-d29d91e38ad2.html#ui-design-with-appsilons-styling-packages",
    "title": "Shiny and Beyond: Mastering Interactive Web Applications with R and Appsilon Packages",
    "section": "",
    "text": "The user interface (UI) is a critical aspect of any web application, as it determines how users interact with your app and how accessible and engaging it is. In Shiny, the default UI components are functional but can sometimes look plain and lack the polish needed for professional applications. This is where Appsilon’s styling packages come in. By using shiny.semantic, shiny.fluent, and semantic.dashboard, you can create visually appealing and highly interactive UIs that stand out."
  },
  {
    "objectID": "ds/posts/2024-05-16_Shiny-and-Beyond--Mastering-Interactive-Web-Applications-with-R-and-Appsilon-Packages-d29d91e38ad2.html#using-shiny.semantic-for-elegant-uis",
    "href": "ds/posts/2024-05-16_Shiny-and-Beyond--Mastering-Interactive-Web-Applications-with-R-and-Appsilon-Packages-d29d91e38ad2.html#using-shiny.semantic-for-elegant-uis",
    "title": "Shiny and Beyond: Mastering Interactive Web Applications with R and Appsilon Packages",
    "section": "",
    "text": "shiny.semantic allows you to use Semantic UI, a front-end framework that provides a wide range of theming options and UI components, within your Shiny applications. This integration helps you create modern, responsive, and user-friendly interfaces without needing extensive knowledge of HTML or CSS.\nTo start using shiny.semantic, you’ll first need to install and load the package:\ninstall.packages(\"shiny.semantic\", repos = \"https://cloud.r-project.org\")\nlibrary(shiny.semantic)\nLet’s enhance our previous mtcars app with shiny.semantic to give it a more modern look:\nlibrary(shiny)\nlibrary(shiny.semantic)\nlibrary(ggplot2)\n\n# Define the UI with shiny.semantic\nui &lt;- semanticPage(\n  title = \"Mtcars Dataset Explorer\",\n  segment(\n    title = \"Mtcars Dataset Explorer\",\n    sidebar_layout(\n      sidebar_panel(\n        selectInput(\"xvar\", \"X-axis variable\", choices = names(mtcars)),\n        selectInput(\"yvar\", \"Y-axis variable\", choices = names(mtcars), selected = \"mpg\")\n      ),\n      main_panel(\n        plotOutput(\"scatterPlot\")\n      )\n    )\n  )\n)\n\n# Define the server logic\nserver &lt;- function(input, output) {\n  output$scatterPlot &lt;- renderPlot({\n    ggplot(mtcars, aes_string(x = input$xvar, y = input$yvar)) +\n      geom_point() +\n      labs(title = paste(\"Scatter plot of\", input$xvar, \"vs\", input$yvar))\n  })\n}\n\n# Run the application\nshinyApp(ui = ui, server = server)\n\n\n\nEnhanced Mtcars App\n\n\nIn this enhanced version:\n\nWe replaced fluidPage with semanticPage to utilize Semantic UI.\nWe used segment and sidebar_layout to structure the UI components.\nThe overall look is more modern and visually appealing compared to the default Shiny components."
  },
  {
    "objectID": "ds/posts/2024-05-16_Shiny-and-Beyond--Mastering-Interactive-Web-Applications-with-R-and-Appsilon-Packages-d29d91e38ad2.html#building-dashboards-with-semantic.dashboard",
    "href": "ds/posts/2024-05-16_Shiny-and-Beyond--Mastering-Interactive-Web-Applications-with-R-and-Appsilon-Packages-d29d91e38ad2.html#building-dashboards-with-semantic.dashboard",
    "title": "Shiny and Beyond: Mastering Interactive Web Applications with R and Appsilon Packages",
    "section": "",
    "text": "For more complex applications that require a dashboard layout, semantic.dashboard offers powerful tools to create sophisticated dashboards with ease. It extends shiny.semantic and adds pre-styled dashboard components.\nHere’s an example of a dashboard layout for our mtcars app:\nlibrary(shiny)\nlibrary(semantic.dashboard)\nlibrary(ggplot2)\n\n# Define the UI with semantic.dashboard\nui &lt;- dashboardPage(\n  dashboardHeader(title = \"Mtcars Dashboard\"),\n  dashboardSidebar(\n    sidebarMenu(\n      menuItem(\"Dashboard\", tabName = \"dashboard\", icon = icon(\"dashboard\")),\n      menuItem(\"Data Explorer\", tabName = \"dataexplorer\", icon = icon(\"table\"))\n    )\n  ),\n  dashboardBody(\n    tabItems(\n      tabItem(tabName = \"dashboard\",\n              fluidRow(\n                box(title = \"Controls\", width = 4, \n                    selectInput(\"xvar\", \"X-axis variable\", choices = names(mtcars)),\n                    selectInput(\"yvar\", \"Y-axis variable\", choices = names(mtcars), selected = \"mpg\")\n                ),\n                box(title = \"Scatter Plot\", width = 8, plotOutput(\"scatterPlot\"))\n              )\n      ),\n      tabItem(tabName = \"dataexplorer\",\n              dataTableOutput(\"dataTable\")\n      )\n    )\n  )\n)\n\n# Define the server logic\nserver &lt;- function(input, output) {\n  output$scatterPlot &lt;- renderPlot({\n    ggplot(mtcars, aes_string(x = input$xvar, y = input$yvar)) +\n      geom_point() +\n      labs(title = paste(\"Scatter plot of\", input$xvar, \"vs\", input$yvar))\n  })\n  \n  output$dataTable &lt;- renderDataTable({\n    mtcars\n  })\n}\n\n# Run the application\nshinyApp(ui = ui, server = server)\n\n\n\nMtcars Dashboard\n\n\nIn this dashboard version:\n\nWe used dashboardPage, dashboardHeader, dashboardSidebar, and dashboardBody to create a structured layout.\nThe sidebar contains a menu for navigation.\nThe body is divided into two tabs: one for the scatter plot and one for exploring the data table."
  },
  {
    "objectID": "ds/posts/2024-05-16_Shiny-and-Beyond--Mastering-Interactive-Web-Applications-with-R-and-Appsilon-Packages-d29d91e38ad2.html#creating-fluent-uis-with-shiny.fluent",
    "href": "ds/posts/2024-05-16_Shiny-and-Beyond--Mastering-Interactive-Web-Applications-with-R-and-Appsilon-Packages-d29d91e38ad2.html#creating-fluent-uis-with-shiny.fluent",
    "title": "Shiny and Beyond: Mastering Interactive Web Applications with R and Appsilon Packages",
    "section": "",
    "text": "shiny.fluent integrates Microsoft’s Fluent UI into Shiny applications, providing a rich set of controls and styles. It is particularly useful for creating applications with a Microsoft Office-like feel.\nHere’s how you can use shiny.fluent to enhance the mtcars app:\nlibrary(shiny)\nlibrary(shiny.fluent)\nlibrary(ggplot2)\n\n# Define the UI with shiny.fluent\nui &lt;- fluentPage(\n  Text(variant = \"xxLarge\", content = \"Mtcars Dataset Explorer\"),\n  Stack(\n    tokens = list(childrenGap = 10),\n    Dropdown.shinyInput(\"xvar\", label = \"X-axis variable\", \n                        options = lapply(names(mtcars), function(x) list(key = x, text = x)),\n                        value = \"mpg\"),\n    Dropdown.shinyInput(\"yvar\", label = \"Y-axis variable\", \n                        options = lapply(names(mtcars), function(x) list(key = x, text = x)),\n                        value = \"hp\"),\n    plotOutput(\"scatterPlot\")\n  )\n)\n\n# Define the server logic\nserver &lt;- function(input, output, session) {\n  output$scatterPlot &lt;- renderPlot({\n    ggplot(mtcars, aes_string(x = input$xvar, y = input$yvar)) +\n      geom_point() +\n      labs(title = paste(\"Scatter plot of\", input$xvar, \"vs\", input$yvar))\n  })\n}\n\n# Run the application\nshinyApp(ui = ui, server = server)\n\n\n\nFluent UI Mtcars App\n\n\nIn this example:\n\nDropdown.shinyInput is used to create dropdowns for the x-axis and y-axis variables.\nThe Dropdown component’s options argument is correctly set up with key and text fields.\nplotOutput is used to display the scatter plot.\nThe server logic captures the input selections and updates the plot accordingly."
  },
  {
    "objectID": "ds/posts/2024-05-16_Shiny-and-Beyond--Mastering-Interactive-Web-Applications-with-R-and-Appsilon-Packages-d29d91e38ad2.html#accessibility-and-usability-tips",
    "href": "ds/posts/2024-05-16_Shiny-and-Beyond--Mastering-Interactive-Web-Applications-with-R-and-Appsilon-Packages-d29d91e38ad2.html#accessibility-and-usability-tips",
    "title": "Shiny and Beyond: Mastering Interactive Web Applications with R and Appsilon Packages",
    "section": "",
    "text": "Ensuring that your applications are accessible and user-friendly is crucial. Here are some tips:\n\nUse shiny.i18n for Internationalization: shiny.i18n makes it easy to translate your Shiny apps into multiple languages, ensuring they are accessible to a broader audience.\nConsistent Styling: Maintain consistent styles across your application for a professional look and feel.\nResponsive Design: Ensure your app works well on different devices and screen sizes.\n\nBy leveraging these Appsilon packages, you can create visually appealing, user-friendly, and highly interactive Shiny applications. In the next chapter, we will delve into advanced reactivity and routing, further enhancing the interactivity and user experience of your applications."
  },
  {
    "objectID": "ds/posts/2024-05-16_Shiny-and-Beyond--Mastering-Interactive-Web-Applications-with-R-and-Appsilon-Packages-d29d91e38ad2.html#advanced-reactivity-and-routing",
    "href": "ds/posts/2024-05-16_Shiny-and-Beyond--Mastering-Interactive-Web-Applications-with-R-and-Appsilon-Packages-d29d91e38ad2.html#advanced-reactivity-and-routing",
    "title": "Shiny and Beyond: Mastering Interactive Web Applications with R and Appsilon Packages",
    "section": "",
    "text": "With a solid understanding of Shiny’s core capabilities and how to enhance the UI using Appsilon’s styling packages, it’s time to delve into more advanced features. This chapter focuses on leveraging advanced reactivity with shiny.react and implementing efficient navigation using shiny.router. These tools will help you create more dynamic, responsive, and user-friendly applications."
  },
  {
    "objectID": "ds/posts/2024-05-16_Shiny-and-Beyond--Mastering-Interactive-Web-Applications-with-R-and-Appsilon-Packages-d29d91e38ad2.html#advanced-reactivity-with-shiny.react",
    "href": "ds/posts/2024-05-16_Shiny-and-Beyond--Mastering-Interactive-Web-Applications-with-R-and-Appsilon-Packages-d29d91e38ad2.html#advanced-reactivity-with-shiny.react",
    "title": "Shiny and Beyond: Mastering Interactive Web Applications with R and Appsilon Packages",
    "section": "",
    "text": "shiny.react is a package that brings the power of React.js, a popular JavaScript library for building user interfaces, into Shiny. By using shiny.react, you can create highly responsive and interactive components that enhance the user experience.\nLet’s enhance our previous mtcars app with shiny.react to add more responsive components:\nlibrary(shiny)\nlibrary(shiny.react)\nlibrary(shiny.fluent)\nlibrary(ggplot2)\n\n# Define the UI with shiny.react and shiny.fluent\nui &lt;- fluentPage(\n  Text(variant = \"xxLarge\", content = \"Mtcars Dataset Explorer\"),\n  Stack(\n    tokens = list(childrenGap = 10),\n    Dropdown.shinyInput(\"xvar\", label = \"X-axis variable\", \n                        options = lapply(names(mtcars), function(x) list(key = x, text = x)),\n                        value = \"mpg\"),\n    Dropdown.shinyInput(\"yvar\", label = \"Y-axis variable\", \n                        options = lapply(names(mtcars), function(x) list(key = x, text = x)),\n                        value = \"hp\"),\n    plotOutput(\"scatterPlot\")\n  )\n)\n\n# Define the server logic\nserver &lt;- function(input, output, session) {\n  output$scatterPlot &lt;- renderPlot({\n    ggplot(mtcars, aes_string(x = input$xvar, y = input$yvar)) +\n      geom_point() +\n      labs(title = paste(\"Scatter plot of\", input$xvar, \"vs\", input$yvar))\n  })\n}\n\n# Run the application\nshinyApp(ui = ui, server = server)\nIn this code:\n\nDropdown.shinyInput is used to create dropdown inputs, integrating Fluent UI with Shiny reactivity.\nThe Dropdown component’s options argument is correctly set up with key and text fields.\nThe fluentPage function is used to structure the UI."
  },
  {
    "objectID": "ds/posts/2024-05-16_Shiny-and-Beyond--Mastering-Interactive-Web-Applications-with-R-and-Appsilon-Packages-d29d91e38ad2.html#implementing-routing-with-shiny.router",
    "href": "ds/posts/2024-05-16_Shiny-and-Beyond--Mastering-Interactive-Web-Applications-with-R-and-Appsilon-Packages-d29d91e38ad2.html#implementing-routing-with-shiny.router",
    "title": "Shiny and Beyond: Mastering Interactive Web Applications with R and Appsilon Packages",
    "section": "",
    "text": "As your Shiny applications grow in complexity, managing navigation and routing becomes crucial. shiny.router is a package that provides a simple way to add routing to your Shiny apps, allowing you to create single-page applications (SPAs) with multiple views."
  },
  {
    "objectID": "ds/posts/2024-05-16_Shiny-and-Beyond--Mastering-Interactive-Web-Applications-with-R-and-Appsilon-Packages-d29d91e38ad2.html#integrating-data-science-and-visualization",
    "href": "ds/posts/2024-05-16_Shiny-and-Beyond--Mastering-Interactive-Web-Applications-with-R-and-Appsilon-Packages-d29d91e38ad2.html#integrating-data-science-and-visualization",
    "title": "Shiny and Beyond: Mastering Interactive Web Applications with R and Appsilon Packages",
    "section": "",
    "text": "With the basics of Shiny and enhanced UI elements covered, it’s time to delve into the core functionality that makes Shiny a powerful tool for data science and visualization. In this chapter, we will explore how to handle data within Shiny applications, create dynamic reports, and integrate advanced visualization libraries to provide insightful and interactive data presentations."
  },
  {
    "objectID": "ds/posts/2024-05-16_Shiny-and-Beyond--Mastering-Interactive-Web-Applications-with-R-and-Appsilon-Packages-d29d91e38ad2.html#data-handling-in-shiny",
    "href": "ds/posts/2024-05-16_Shiny-and-Beyond--Mastering-Interactive-Web-Applications-with-R-and-Appsilon-Packages-d29d91e38ad2.html#data-handling-in-shiny",
    "title": "Shiny and Beyond: Mastering Interactive Web Applications with R and Appsilon Packages",
    "section": "",
    "text": "Efficient data handling is crucial for any Shiny application, especially when dealing with large datasets or complex analyses. Shiny provides several mechanisms to manage data effectively, including reactive expressions and data caching."
  },
  {
    "objectID": "ds/posts/2024-05-16_Shiny-and-Beyond--Mastering-Interactive-Web-Applications-with-R-and-Appsilon-Packages-d29d91e38ad2.html#reactive-data-handling",
    "href": "ds/posts/2024-05-16_Shiny-and-Beyond--Mastering-Interactive-Web-Applications-with-R-and-Appsilon-Packages-d29d91e38ad2.html#reactive-data-handling",
    "title": "Shiny and Beyond: Mastering Interactive Web Applications with R and Appsilon Packages",
    "section": "",
    "text": "Reactivity is at the heart of Shiny, allowing applications to respond to user inputs dynamically. Here’s an example of how to use reactive expressions to handle data in Shiny:\nlibrary(shiny)\nlibrary(ggplot2)\n\n# Define UI\nui &lt;- fluidPage(\n  titlePanel(\"Reactive Data Example\"),\n  sidebarLayout(\n    sidebarPanel(\n      numericInput(\"obs\", \"Number of observations:\", 1000, min = 1, max = 10000)\n    ),\n    mainPanel(\n      plotOutput(\"distPlot\")\n    )\n  )\n)\n\n# Define server logic\nserver &lt;- function(input, output) {\n  # Reactive expression to generate random data\n  data &lt;- reactive({\n    rnorm(input$obs)\n  })\n  \n  # Render plot\n  output$distPlot &lt;- renderPlot({\n    ggplot(data.frame(x = data()), aes(x)) +\n      geom_histogram(binwidth = 0.2) +\n      labs(title = \"Histogram of Randomly Generated Data\")\n  })\n}\n\n# Run the application\nshinyApp(ui = ui, server = server)\n\n\n\nReactive Data Example\n\n\nIn this example:\n\nA numericInput allows the user to specify the number of observations.\nA reactive expression data() generates random data based on the user input.\nThe renderPlot function uses this reactive data to generate and display a histogram."
  },
  {
    "objectID": "ds/posts/2024-05-16_Shiny-and-Beyond--Mastering-Interactive-Web-Applications-with-R-and-Appsilon-Packages-d29d91e38ad2.html#dynamic-reporting-with-shiny",
    "href": "ds/posts/2024-05-16_Shiny-and-Beyond--Mastering-Interactive-Web-Applications-with-R-and-Appsilon-Packages-d29d91e38ad2.html#dynamic-reporting-with-shiny",
    "title": "Shiny and Beyond: Mastering Interactive Web Applications with R and Appsilon Packages",
    "section": "",
    "text": "Shiny can be combined with rmarkdown and knitr to create dynamic reports that update based on user inputs. This is particularly useful for generating customized reports on the fly.\nHere’s an example of a simple Shiny app that generates a report using rmarkdown:\nlibrary(shiny)\nlibrary(rmarkdown)\n\n# Define UI\nui &lt;- fluidPage(\n  titlePanel(\"Dynamic Report Example\"),\n  sidebarLayout(\n    sidebarPanel(\n      numericInput(\"obs\", \"Number of observations:\", 1000, min = 1, max = 10000),\n      downloadButton(\"report\", \"Generate Report\")\n    ),\n    mainPanel(\n      plotOutput(\"distPlot\")\n    )\n  )\n)\n\n# Define server logic\nserver &lt;- function(input, output) {\n  # Reactive expression to generate random data\n  data &lt;- reactive({\n    rnorm(input$obs)\n  })\n  \n  # Render plot\n  output$distPlot &lt;- renderPlot({\n    ggplot(data.frame(x = data()), aes(x)) +\n      geom_histogram(binwidth = 0.2) +\n      labs(title = \"Histogram of Randomly Generated Data\")\n  })\n  \n  # Generate report\n  output$report &lt;- downloadHandler(\n    filename = function() {\n      paste(\"report-\", Sys.Date(), \".html\", sep = \"\")\n    },\n    content = function(file) {\n      tempReport &lt;- file.path(tempdir(), \"report.Rmd\")\n      file.copy(\"report.Rmd\", tempReport, overwrite = TRUE)\n      \n      params &lt;- list(obs = input$obs)\n      \n      rmarkdown::render(tempReport, output_file = file,\n                        params = params,\n                        envir = new.env(parent = globalenv()))\n    }\n  )\n}\n\n# Run the application\nshinyApp(ui = ui, server = server)\n\n\n\nDynamic Report Example\n\n\nFor this example to work, you’ll need a report.Rmd file in your working directory with the following content:\n\n---\ntitle: \"Dynamic Report\"\noutput: html_document\nparams:\n  obs: 1\n---\n\n\n```r\nknitr::opts_chunk$set(echo = TRUE)\n```\n\n## Report\nThis report was generated dynamically using rmarkdown.\n\nThe number of observations selected was `r params$obs`.\n\n```r\ndata &lt;- rnorm(params$obs)\nhist(data, main = \"Histogram of Randomly Generated Data\")\n```"
  },
  {
    "objectID": "ds/posts/2024-05-16_Shiny-and-Beyond--Mastering-Interactive-Web-Applications-with-R-and-Appsilon-Packages-d29d91e38ad2.html#enhancing-shiny-with-appsilons-extensions",
    "href": "ds/posts/2024-05-16_Shiny-and-Beyond--Mastering-Interactive-Web-Applications-with-R-and-Appsilon-Packages-d29d91e38ad2.html#enhancing-shiny-with-appsilons-extensions",
    "title": "Shiny and Beyond: Mastering Interactive Web Applications with R and Appsilon Packages",
    "section": "",
    "text": "Enhancing your Shiny applications with Appsilon’s powerful extensions can significantly improve functionality, usability, and visual appeal. This chapter provides an overview of key Appsilon packages, such as shiny.semantic, shiny.fluent, semantic.dashboard, shiny.i18n, shiny.router, and shiny.react."
  },
  {
    "objectID": "ds/posts/2024-05-16_Shiny-and-Beyond--Mastering-Interactive-Web-Applications-with-R-and-Appsilon-Packages-d29d91e38ad2.html#key-extensions",
    "href": "ds/posts/2024-05-16_Shiny-and-Beyond--Mastering-Interactive-Web-Applications-with-R-and-Appsilon-Packages-d29d91e38ad2.html#key-extensions",
    "title": "Shiny and Beyond: Mastering Interactive Web Applications with R and Appsilon Packages",
    "section": "",
    "text": "shiny.semantic:\n\nIntegrates Semantic UI for modern, responsive designs.\nOffers a wide range of UI components and theming options.\n\nshiny.fluent:\n\nUses Microsoft’s Fluent UI framework for styling.\nProvides consistent and visually appealing UI elements.\n\nsemantic.dashboard:\n\nExtends shiny.semantic to create sophisticated dashboards.\nIncludes pre-styled components for interactive and appealing dashboards.\n\nshiny.i18n:\n\nFacilitates internationalization and localization.\nEnables translation of Shiny apps into multiple languages, improving accessibility.\n\nshiny.router:\n\nImplements routing for single-page applications.\nManages navigation and structure of large applications efficiently.\n\nshiny.react:\n\nIntegrates React.js components into Shiny.\nEnhances interactivity and responsiveness of Shiny applications."
  },
  {
    "objectID": "ds/posts/2024-05-16_Shiny-and-Beyond--Mastering-Interactive-Web-Applications-with-R-and-Appsilon-Packages-d29d91e38ad2.html#summary-of-examples",
    "href": "ds/posts/2024-05-16_Shiny-and-Beyond--Mastering-Interactive-Web-Applications-with-R-and-Appsilon-Packages-d29d91e38ad2.html#summary-of-examples",
    "title": "Shiny and Beyond: Mastering Interactive Web Applications with R and Appsilon Packages",
    "section": "",
    "text": "UI Enhancement with shiny.semantic and shiny.fluent: Transforming basic Shiny apps into modern, responsive applications using Semantic UI and Fluent UI frameworks.\nCreating Dashboards with semantic.dashboard: Building interactive and visually appealing dashboards using pre-styled components.\nInternationalization with shiny.i18n: Translating Shiny applications to make them accessible to a global audience.\nRouting with shiny.router: Adding navigation and structuring large applications as single-page apps.\nAdvanced Reactivity with shiny.react: Incorporating React.js for highly interactive and responsive UI components.\n\nUsing these Appsilon extensions, you can significantly enhance the capabilities of your Shiny applications. These tools enable you to create more robust, user-friendly, and visually appealing applications, tailored to meet the needs of diverse users and complex projects."
  },
  {
    "objectID": "ds/posts/2024-05-16_Shiny-and-Beyond--Mastering-Interactive-Web-Applications-with-R-and-Appsilon-Packages-d29d91e38ad2.html#conclusion",
    "href": "ds/posts/2024-05-16_Shiny-and-Beyond--Mastering-Interactive-Web-Applications-with-R-and-Appsilon-Packages-d29d91e38ad2.html#conclusion",
    "title": "Shiny and Beyond: Mastering Interactive Web Applications with R and Appsilon Packages",
    "section": "",
    "text": "In this article, we have explored how to harness the power of Shiny for building interactive web applications in R, leveraging advanced UI frameworks, modular development, and data visualization techniques. By integrating Appsilon’s extensions, you can significantly enhance the functionality, usability, and visual appeal of your Shiny applications.\nWhile this guide covers various aspects of Shiny development, it’s important to note that deploying Shiny applications online is a crucial step that we haven’t delved into in detail. As I’m not an expert in deployment, I recommend the following resources for learning how to deploy Shiny applications:\n\nGetting Started with ShinyApps.io\nIntroduction to Shiny Server\nR Shiny Docker: How To Run Shiny Apps in a Docker Container\nThe Ultimate Guide to Deploying a Shiny App on AWS\nHow To Set Up Shiny Server on Ubuntu 20.04\n\nBy exploring these resources, you can learn how to make your Shiny applications accessible to users worldwide, ensuring they are robust, scalable, and secure.\nThank you for following along with chapters on mastering Shiny and its extensions. I hope you found the information valuable and that it helps you in your journey to creating powerful, interactive web applications with R."
  },
  {
    "objectID": "ds/posts/2024-08-16_Why-Every-Data-Scientist-Needs-the-janitor-Package-da37e4dcfe24.html",
    "href": "ds/posts/2024-08-16_Why-Every-Data-Scientist-Needs-the-janitor-Package-da37e4dcfe24.html",
    "title": "Why Every Data Scientist Needs the janitor Package",
    "section": "",
    "text": "222222222222222222222222222222222222222 \n ### Lessons from Will Hunting and McGayver\nIn the world of data science, data cleaning is often seen as one of the most time-consuming and least glamorous tasks. Yet, it’s also one of the most critical. Without clean data, even the most sophisticated algorithms and models can produce misleading results. This is where the janitor package in R comes into play, serving as the unsung hero that quietly handles the nitty-gritty work of preparing data for analysis.\nMuch like the janitors we often overlook in our daily lives, the janitor package works behind the scenes to ensure everything runs smoothly. It takes care of the small but essential tasks that, if neglected, could bring a project to a halt. The package simplifies data cleaning with a set of intuitive functions that are both powerful and easy to use, making it an indispensable tool for any data scientist.\nTo better understand the importance of janitor, we can draw parallels to two iconic figures from pop culture: Will Hunting, the genius janitor from Good Will Hunting, and McGayver, the handyman known for his ability to solve any problem with minimal resources. Just as Will Hunting and McGayver possess hidden talents that make a huge impact, the janitor package holds a set of powerful functions that can transform messy datasets into clean, manageable ones, enabling data scientists to focus on the more complex aspects of their work.\n\nWill Hunting: The Genius Janitor\nWill Hunting, the protagonist of Good Will Hunting, is an unassuming janitor at the Massachusetts Institute of Technology (MIT). Despite his modest job, Will possesses a genius-level intellect, particularly in mathematics. His hidden talent is discovered when he solves a complex math problem left on a blackboard, something that had stumped even the brightest minds at the university. This revelation sets off a journey that challenges his self-perception and the expectations of those around him.\nThe story of Will Hunting is a perfect metaphor for the janitor package in R. Just as Will performs crucial tasks behind the scenes at MIT, the janitor package operates in the background of data science projects. It handles the essential, albeit often overlooked, work of data cleaning, ensuring that data is in the best possible shape for analysis. Like Will, who is initially underestimated but ultimately proves invaluable, janitor is a tool that may seem simple at first glance but is incredibly powerful and essential for any serious data scientist.\nWithout proper data cleaning, even the most advanced statistical models can produce incorrect or misleading results. The janitor package, much like Will Hunting, quietly ensures that the foundations are solid, allowing the more complex and visible work to shine.\n\n\nMcGayver: The Handyman Who Fixes Everything\nIn your school days, you might have known someone who was a jack-of-all-trades, able to fix anything with whatever tools or materials were on hand. Perhaps this person was affectionately nicknamed “McGayver,” a nod to the famous TV character MacGyver, who was known for solving complex problems with everyday objects. This school janitor, like McGayver, was indispensable — working in the background, fixing leaks, unclogging drains, and keeping everything running smoothly. Without him, things would quickly fall apart.\nThis is exactly how the janitor package functions in the world of data science. Just as your school’s McGayver could solve any problem with a handful of tools, the janitor package offers a set of versatile functions that can clean up the messiest of datasets with minimal effort. Whether it’s removing empty rows and columns, cleaning up column names, or handling duplicates, janitor has a tool for the job. And much like McGayver, it accomplishes these tasks efficiently and effectively, often with a single line of code.\nThe genius of McGayver wasn’t just in his ability to fix things, but in how he could use simple tools to do so. In the same way, janitor simplifies tasks that might otherwise require complex code or multiple steps. It allows data scientists to focus on the bigger picture, confident that the foundations of their data are solid.\n\n\nProblem-Solving with and without janitor\nIn this section, we’ll dive into specific data cleaning problems that data scientists frequently encounter. For each problem, we’ll first show how it can be solved using base R, and then demonstrate how the janitor package offers a more streamlined and efficient solution.\n\n1. clean_names(): Tidying Up Column Names\nProblem:\nColumn names in datasets are often messy — containing spaces, special characters, or inconsistent capitalization — which can make data manipulation challenging. Consistent, tidy column names are essential for smooth data analysis.\nBase R Solution: To clean column names manually, you would need to perform several steps, such as converting names to lowercase, replacing spaces with underscores, and removing special characters. Here’s an example using base R:\n# Creating dummy empty data frame\ndf = data.frame(a = NA, b = NA, c = NA, d = NA)\n\n# Original column names\nnames(df) &lt;- c(\"First Name\", \"Last Name\", \"Email Address\", \"Phone Number\")\n\n# Cleaning the names manually\nnames(df) &lt;- tolower(names(df))                        # Convert to lowercase\nnames(df) &lt;- gsub(\" \", \"_\", names(df))                 # Replace spaces with underscores\nnames(df) &lt;- gsub(\"[^[:alnum:]_]\", \"\", names(df))      # Remove special characters\n\n# Resulting column names\nnames(df)\n# [1] \"first_name\" \"last_name\" \"email_address\" \"phone_number\"\nThis approach requires multiple lines of code, each handling a different aspect of cleaning.\njanitor Solution: With the janitor package, the same result can be achieved with a single function:\n# creating dummy empty data frame\ndf = data.frame(a = NA, b = NA, c = NA, d = NA)\nnames(df) &lt;- c(\"First Name\", \"Last Name\", \"Email Address\", \"Phone Number\")\n\nlibrary(janitor)\n\n# Using clean_names() to tidy up column names\ndf &lt;- clean_names(df)\n\n# Resulting column names\nnames(df)\n# [1] \"first_name\" \"last_name\" \"email_address\" \"phone_number\"\nWhy janitor Is Better: The clean_names() function simplifies the entire process into one step, automatically applying a set of best practices to clean and standardize column names. This not only saves time but also reduces the chance of making errors in your code. By using clean_names(), you ensure that your column names are consistently formatted and ready for analysis, without the need for manual intervention.\n\n\n2. tabyl and adorn_ Functions: Creating Frequency Tables and Adding Totals or Percentages\nProblem:\nWhen analyzing categorical data, it’s common to create frequency tables or cross-tabulations. Additionally, you might want to add totals or percentages to these tables to get a clearer picture of your data distribution.\nBase R Solution: Creating a frequency table and adding totals or percentages manually requires several steps. Here’s an example using base R:\n# Sample data\ndf &lt;- data.frame(\n  gender = c(\"Male\", \"Female\", \"Female\", \"Male\", \"Female\"),\n  age_group = c(\"18-24\", \"18-24\", \"25-34\", \"25-34\", \"35-44\")\n)\n\n# Creating a frequency table using base R\ntable(df$gender, df$age_group)\n\n#        18-24 25-34 35-44\n# Female     1     1     1\n# Male       1     1     0\n\n# Adding row totals\naddmargins(table(df$gender, df$age_group), margin = 1)\n\n#         18-24 25-34 35-44\n# Female     1     1     1\n# Male       1     1     0\n# Sum        2     2     1\n\n# Calculating percentages\nprop.table(table(df$gender, df$age_group), margin = 1) * 100\n\n#           18-24    25-34    35-44\n# Female 33.33333 33.33333 33.33333\n# Male   50.00000 50.00000  0.00000\nThis method involves creating tables, adding margins manually, and calculating percentages separately, which can become cumbersome, especially with larger datasets.\njanitor Solution: With the janitor package, you can create a frequency table and easily add totals or percentages using tabyl() and adorn_* functions:\n# Sample data\ndf &lt;- data.frame(\n  gender = c(\"Male\", \"Female\", \"Female\", \"Male\", \"Female\"),\n  age_group = c(\"18-24\", \"18-24\", \"25-34\", \"25-34\", \"35-44\")\n)\n\nlibrary(janitor)\n\n# Piping all together\ntable_df &lt;- df %&gt;%\n  tabyl(gender, age_group) %&gt;%\n  adorn_totals(\"row\") %&gt;%\n  adorn_percentages(\"row\") %&gt;%\n  adorn_pct_formatting()\n\ntable_df\n\n# gender 18-24 25-34 35-44\n# Female 33.3% 33.3% 33.3%\n#   Male 50.0% 50.0%  0.0%\n#  Total 40.0% 40.0% 20.0%\nWhy janitor Is Better: The tabyl() function automatically generates a clean frequency table, while adorn_totals() and adorn_percentages() easily add totals and percentages without the need for additional code. This approach is not only quicker but also reduces the complexity of your code. The janitor functions handle the formatting and calculations for you, making it easier to produce professional-looking tables that are ready for reporting or further analysis.\n\n\n3. row_to_names(): Converting a Row of Data into Column Names\nProblem:\nSometimes, datasets are structured with the actual column names stored in one of the rows rather than the header. Before starting the analysis, you need to promote this row to be the header of the data frame.\nBase R Solution: Without janitor, converting a row to column names can be done with the following steps using base R:\n# Sample data with column names in the first row\ndf &lt;- data.frame(\n  X1 = c(\"Name\", \"John\", \"Jane\", \"Doe\"),\n  X2 = c(\"Age\", \"25\", \"30\", \"22\"),\n  X3 = c(\"Gender\", \"Male\", \"Female\", \"Male\")\n)\n\n# Step 1: Extract the first row as column names\ncolnames(df) &lt;- df[1, ]\n\n# Step 2: Remove the first row from the data frame\ndf &lt;- df[-1, ]\n\n# Resulting data frame\ndf\nThis method involves manually extracting the row, assigning it as the header, and then removing the original row from the data.\njanitor Solution: With janitor, this entire process is streamlined into a single function:\n# Sample data with column names in the first row\ndf &lt;- data.frame(\n  X1 = c(\"Name\", \"John\", \"Jane\", \"Doe\"),\n  X2 = c(\"Age\", \"25\", \"30\", \"22\"),\n  X3 = c(\"Gender\", \"Male\", \"Female\", \"Male\")\n)\n\ndf &lt;- row_to_names(df, row_number = 1)\n\n# Resulting data frame\ndf\nWhy janitor Is Better: The row_to_names() function from janitor simplifies this operation by directly promoting the specified row to the header in one go, eliminating the need for multiple steps. This function is more intuitive and reduces the chance of errors, allowing you to quickly structure your data correctly and move on to analysis.\n\n\n4. remove_constant(): Identifying and Removing Columns with Constant Values\nProblem:\nIn some datasets, certain columns may contain the same value across all rows. These constant columns provide no useful information for analysis and can clutter your dataset. Removing them is essential for streamlining your data.\nBase R Solution: Identifying and removing constant columns without janitor requires writing a custom function or applying several steps. Here’s an example using base R:\n# Sample data with constant and variable columns\ndf &lt;- data.frame(\n  ID = c(1, 2, 3, 4, 5),\n  Gender = c(\"Male\", \"Male\", \"Male\", \"Male\", \"Male\"), # Constant column\n  Age = c(25, 30, 22, 40, 35)\n)\n\n# Identifying constant columns manually\nconstant_cols &lt;- sapply(df, function(col) length(unique(col)) == 1)\n\n# Removing constant columns\ndf &lt;- df[, !constant_cols]\n\n# Resulting data frame\ndf\n\n  ID Age\n1  1  25\n2  2  30\n3  3  22\n4  4  40\n5  5  35\nThis method involves checking each column for unique values and then filtering out the constant ones, which can be cumbersome.\njanitor Solution: With janitor, you can achieve the same result with a simple, one-line function:\ndf &lt;- data.frame(\n  ID = c(1, 2, 3, 4, 5),\n  Gender = c(\"Male\", \"Male\", \"Male\", \"Male\", \"Male\"), # Constant column\n  Age = c(25, 30, 22, 40, 35)\n)\n\ndf &lt;- remove_constant(df)\n\n  ID Age\n1  1  25\n2  2  30\n3  3  22\n4  4  40\n5  5  35\nWhy janitor Is Better: The remove_constant() function from janitor is a straightforward and efficient solution to remove constant columns. It automates the process, ensuring that no valuable time is wasted on writing custom functions or manually filtering columns. This function is particularly useful when working with large datasets, where manually identifying constant columns would be impractical.\n\n\n5. remove_empty(): Eliminating Empty Rows and Columns\nProblem:\nDatasets often contain rows or columns that are entirely empty, especially after merging or importing data from various sources. These empty rows and columns don’t contribute any useful information and can complicate data analysis, so they should be removed.\nBase R Solution: Manually identifying and removing empty rows and columns can be done, but it requires multiple steps. Here’s how you might approach it using base R:\n# Sample data with empty rows and columns\ndf &lt;- data.frame(\n  ID = c(1, 2, NA, 4, 5),\n  Name = c(\"John\", \"Jane\", NA, NA,NA),\n  Age = c(25, 30, NA, NA, NA),\n  Empty_Col = c(NA, NA, NA, NA, NA) # An empty column\n)\n\n# Removing empty rows\ndf &lt;- df[rowSums(is.na(df)) != ncol(df), ]\n\n# Removing empty columns\ndf &lt;- df[, colSums(is.na(df)) != nrow(df)]\n\n# Resulting data frame\ndf\n\n  ID Name Age\n1  1 John  25\n2  2 Jane  30\n4  4 &lt;NA&gt;  NA\n5  5 &lt;NA&gt;  NA\nThis method involves checking each row and column for completeness and then filtering out those that are entirely empty, which can be cumbersome and prone to error.\njanitor Solution: With janitor, you can remove both empty rows and columns in a single, straightforward function call:\n# Sample data with empty rows and columns\ndf &lt;- data.frame(\n  ID = c(1, 2, NA, 4, 5),\n  Name = c(\"John\", \"Jane\", NA, NA,NA),\n  Age = c(25, 30, NA, NA, NA),\n  Empty_Col = c(NA, NA, NA, NA, NA) # An empty column\n)\n\ndf &lt;- remove_empty(df, which = c(\"cols\", \"rows\"))\n\ndf\n\n  ID Name Age\n1  1 John  25\n2  2 Jane  30\n4  4 &lt;NA&gt;  NA\n5  5 &lt;NA&gt;  NA\nWhy janitor Is Better: The remove_empty() function from janitor makes it easy to eliminate empty rows and columns with minimal effort. You can specify whether you want to remove just rows, just columns, or both, making the process more flexible and less error-prone. This one-line solution significantly simplifies the task and ensures that your dataset is clean and ready for analysis.\n\n\n6. get_dupes(): Detecting and Extracting Duplicate Rows\nProblem:\nDuplicate rows in a dataset can lead to biased or incorrect analysis results. Identifying and managing duplicates is crucial to ensure the integrity of your data.\nBase R Solution: Detecting and extracting duplicate rows manually can be done using base R with the following approach:\n# Sample data with duplicate rows\ndf &lt;- data.frame(\n  ID = c(1, 2, 3, 3, 4, 5, 5),\n  Name = c(\"John\", \"Jane\", \"Doe\", \"Doe\", \"Alice\", \"Bob\", \"Bob\"),\n  Age = c(25, 30, 22, 22, 40, 35, 35)\n)\n\n# Identifying duplicate rows\ndupes &lt;- df[duplicated(df) | duplicated(df, fromLast = TRUE), ]\n\n# Resulting data frame with duplicates\ndupes\n\nID Name Age\n3  3  Doe  22\n4  3  Doe  22\n6  5  Bob  35\n7  5  Bob  35\nThis approach uses duplicated() to identify duplicate rows. While it’s effective, it requires careful handling to ensure all duplicates are correctly identified and extracted, especially in more complex datasets.\njanitor Solution: With janitor, identifying and extracting duplicate rows is greatly simplified using the get_dupes() function:\n# Sample data with duplicate rows\ndf &lt;- data.frame(\n  ID = c(1, 2, 3, 3, 4, 5, 5),\n  Name = c(\"John\", \"Jane\", \"Doe\", \"Doe\", \"Alice\", \"Bob\", \"Bob\"),\n  Age = c(25, 30, 22, 22, 40, 35, 35)\n)\n\n# Using get_dupes() to find duplicate rows\ndupes &lt;- get_dupes(df)\n\n# Resulting data frame with duplicates\ndupes\n\n# It gives us additional info how many repeats of each row we have\n  ID Name Age dupe_count\n1  3  Doe  22          2\n2  3  Doe  22          2\n3  5  Bob  35          2\n4  5  Bob  35          2\nWhy janitor Is Better: The get_dupes() function from janitor not only identifies duplicate rows but also provides additional information, such as the number of times each duplicate appears, in an easy-to-read format. This functionality is particularly useful when dealing with large datasets, where even a straightforward method like duplicated() can become cumbersome. With get_dupes(), you gain a more detailed and user-friendly overview of duplicates, ensuring the integrity of your data.\n\n\n7. round_half_up, signif_half_up, and round_to_fraction: Rounding Numbers with Precision\nProblem:\nRounding numbers is a common task in data analysis, but different situations require different types of rounding. Sometimes you need to round to the nearest integer, other times to a specific fraction, or you might need to ensure that rounding is consistent in cases like 5.5 rounding up to 6.\nBase R Solution: Rounding numbers in base R can be done using round() or signif(), but these functions don't always handle edge cases or specific requirements like rounding half up or to a specific fraction:\n# Sample data\nnumbers &lt;- c(1.25, 2.5, 3.75, 4.125, 5.5)\n\n# Rounding using base R's round() function\nrounded &lt;- round(numbers, 1)  # Rounds to one decimal place\n\n# Rounding to significant digits using signif()\nsignificant &lt;- signif(numbers, 2)\n\n# Resulting rounded values\n\nrounded\n[1] 1.2 2.5 3.8 4.1 5.5\n\nsignificant\n[1] 1.2 2.5 3.8 4.1 5.5\nWhile these functions are useful, they may not provide the exact rounding behavior you need in certain situations, such as consistently rounding half values up or rounding to specific fractions.\njanitor Solution: The janitor package provides specialized functions like round_half_up(), signif_half_up(), and round_to_fraction() to handle these cases with precision:\n# Using round_half_up() to round numbers with half up logic\nrounded_half_up &lt;- round_half_up(numbers, 1)\n\n# Using signif_half_up() to round to significant digits with half up logic\nsignificant_half_up &lt;- signif_half_up(numbers, 2)\n\n# Using round_to_fraction() to round numbers to the nearest fraction\nrounded_fraction &lt;- round_to_fraction(numbers, denominator = 4)\n\nrounded_half_up\n[1] 1.3 2.5 3.8 4.1 5.5\n\nsignificant_half_up\n[1] 1.3 2.5 3.8 4.1 5.5\n\nrounded_fraction\n[1] 1.25 2.50 3.75 4.00 5.50\nWhy janitor Is Better: The janitor functions round_half_up(), signif_half_up(), and round_to_fraction() offer more precise control over rounding operations compared to base R functions. These functions are particularly useful when you need to ensure consistent rounding behavior, such as always rounding 5.5 up to 6, or when rounding to the nearest fraction (e.g., quarter or eighth). This level of control can be critical in scenarios where rounding consistency affects the outcome of an analysis or report.\n\n\n8. chisq.test() and fisher.test(): Simplifying Hypothesis Testing\nProblem:\nWhen working with categorical data, it’s often necessary to test for associations between variables using statistical tests like the Chi-squared test (chisq.test()) or Fisher’s exact test (fisher.test()). Preparing your data and setting up these tests manually can be complex, particularly when dealing with larger datasets with multiple categories.\nBase R Solution: Here’s how you might approach this using a more complex dataset with base R:\n# Sample data with multiple categories\ndf &lt;- data.frame(\n  Treatment = c(\"A\", \"A\", \"B\", \"B\", \"C\", \"C\", \"A\", \"B\", \"C\", \"A\", \"B\", \"C\"),\n  Outcome = c(\"Success\", \"Failure\", \"Success\", \"Failure\", \"Success\", \"Failure\",\n              \"Success\", \"Success\", \"Failure\", \"Failure\", \"Success\", \"Failure\"),\n  Gender = c(\"Male\", \"Female\", \"Male\", \"Female\", \"Male\", \"Female\", \"Male\",\n             \"Female\", \"Male\", \"Female\", \"Male\", \"Female\")\n)\n\n# Creating a contingency table\ncontingency_table &lt;- table(df$Treatment, df$Outcome, df$Gender)\n\n# Performing Chi-squared test (on a 2D slice of the table)\nchisq_result &lt;- chisq.test(contingency_table[,, \"Male\"])\n\n# Performing Fisher's exact test (on the same 2D slice)\nfisher_result &lt;- fisher.test(contingency_table[,, \"Male\"])\n\n# Results\nchisq_result\n\n Pearson's Chi-squared test\n\ndata:  contingency_table[, , \"Male\"]\nX-squared = 2.4, df = 2, p-value = 0.3012\n\nfisher_result\n\n Fisher's Exact Test for Count Data\n\ndata:  contingency_table[, , \"Male\"]\np-value = 1\nalternative hypothesis: two.sided\nThis approach involves creating a multidimensional contingency table and then slicing it to apply the tests. This can become cumbersome and requires careful management of the data structure.\njanitor Solution: Using janitor, you can achieve the same results with a more straightforward approach:\n# Sample data with multiple categories\ndf &lt;- data.frame(\n  Treatment = c(\"A\", \"A\", \"B\", \"B\", \"C\", \"C\", \"A\", \"B\", \"C\", \"A\", \"B\", \"C\"),\n  Outcome = c(\"Success\", \"Failure\", \"Success\", \"Failure\", \"Success\", \"Failure\",\n              \"Success\", \"Success\", \"Failure\", \"Failure\", \"Success\", \"Failure\"),\n  Gender = c(\"Male\", \"Female\", \"Male\", \"Female\", \"Male\", \"Female\", \"Male\",\n             \"Female\", \"Male\", \"Female\", \"Male\", \"Female\")\n)\n\nlibrary(janitor)\n\n# Creating a tabyl to perform Chi-squared and Fisher's exact tests for Male participants\ndf_male &lt;- df %&gt;%\n  filter(Gender == \"Male\") %&gt;%\n  tabyl(Treatment, Outcome)\n\n# Performing Chi-squared test\nchisq_result &lt;- chisq.test(df_male)\n\n# Performing Fisher's exact test\nfisher_result &lt;- fisher.test(df_male)\n\n# Results\nchisq_result\n\n Pearson's Chi-squared test\n\ndata:  df_male\nX-squared = 2.4, df = 2, p-value = 0.3012\n\nfisher_result\n\n Fisher's Exact Test for Count Data\n\ndata:  df_male\np-value = 1\nalternative hypothesis: two.sided\nWhy janitor Is Better: The janitor approach simplifies the process by integrating the creation of contingency tables (tabyl()) with the execution of hypothesis tests (chisq.test() and fisher.test()). This reduces the need for manual data slicing and ensures that the data is correctly formatted for testing. This streamlined process is particularly advantageous when dealing with larger, more complex datasets, where manually managing the structure could lead to errors. The result is a faster, more reliable workflow for testing associations between categorical variables.\n\n\n\nThe Unsung Heroes of Data Science\nIn both the physical world and the realm of data science, there are tasks that often go unnoticed but are crucial for the smooth operation of larger systems. Janitors, for example, quietly maintain the cleanliness and functionality of buildings, ensuring that everyone else can work comfortably and efficiently. Without their efforts, even the most well-designed spaces would quickly descend into chaos.\nSimilarly, the janitor package in R plays an essential, yet often underappreciated, role in data science. Data cleaning might not be the most glamorous aspect of data analysis, but it’s undoubtedly one of the most critical. Just as a building cannot function properly without regular maintenance, a data analysis project cannot yield reliable results without clean, well-prepared data.\nThe functions provided by the janitor package — whether it’s tidying up column names, removing duplicates, or simplifying complex rounding tasks — are the data science equivalent of the work done by janitors and handymen in the physical world. They ensure that the foundational aspects of your data are in order, allowing you to focus on the more complex, creative aspects of analysis and interpretation.\nReliable data cleaning is not just about making datasets look neat; it’s about ensuring the accuracy and integrity of the insights derived from that data. Inaccurate or inconsistent data can lead to flawed conclusions, which can have significant consequences in any field — from business decisions to scientific research. By automating and simplifying the data cleaning process, the janitor package helps prevent such issues, ensuring that the results of your analysis are as robust and trustworthy as possible.\nIn short, while the janitor package may work quietly behind the scenes, its impact on the overall success of data science projects is profound. It is the unsung hero that keeps your data — and, by extension, your entire analysis — on solid ground.\nThroughout this article, we’ve delved into how the janitor package in R serves as an indispensable tool for data cleaning, much like the often-overlooked but essential janitors and handymen in our daily lives. By comparing its functions to traditional methods using base R, we’ve demonstrated how janitor simplifies and streamlines tasks that are crucial for any data analysis project.\nThe story of Will Hunting, the genius janitor, and the analogy of your school’s “McGayver” highlight how unnoticed figures can make extraordinary contributions with their unique skills. Similarly, the janitor package, though it operates quietly in the background, has a significant impact on data preparation. It handles the nitty-gritty tasks — cleaning column names, removing duplicates, rounding numbers precisely — allowing data scientists to focus on generating insights and building models.\nWe also explored how functions like clean_names(), tabyl(), row_to_names(), remove_constants(), remove_empty(), get_dupes(), and round_half_up() drastically reduce the effort required to prepare your data. These tools save time, ensure data consistency, and minimize errors, making them indispensable for any data professional.\nMoreover, we emphasized the critical role of data cleaning in ensuring reliable analysis outcomes. Just as no building can function without the janitors who maintain it, no data science workflow should be without tools like the janitor package. It is the unsung hero that ensures your data is ready for meaningful analysis, enabling you to trust your results and make sound decisions.\nIn summary, the janitor package is more than just a set of utility functions — it’s a crucial ally in the data scientist’s toolkit. By handling the essential, behind-the-scenes work of data cleaning, janitor helps ensure that your analyses are built on a solid foundation. So, if you haven’t already integrated janitor into your workflow, now is the perfect time to explore its capabilities and see how it can elevate your data preparation process.\nConsider adding janitor to your R toolkit today. Explore its functions and experience firsthand how it can streamline your workflow and enhance the quality of your data analysis. Your data — and your future analyses — will thank you.\nCanonical link\nExported from Medium on December 19, 2024."
  },
  {
    "objectID": "ds/posts/2024-10-10_R-You-Ready--Git-Your-Code-Under-Control--c77e23b53552.html",
    "href": "ds/posts/2024-10-10_R-You-Ready--Git-Your-Code-Under-Control--c77e23b53552.html",
    "title": "R You Ready? Git Your Code Under Control!",
    "section": "",
    "text": "R You Ready? Git Your Code Under Control!\n\n\n\nImage\n\n\nHey there, ready to get your R code under control? Whether you’re working on your own or in a small team, managing your code can sometimes feel like juggling too many things at once. But don’t worry — there’s an easy way to stay on top of everything. That’s where Git comes in.\nThink of Git like a trusty sidekick for your coding adventures. It helps you keep track of every little change, save different versions of your work, and easily rewind if things don’t go as planned. Plus, if you’re working with others, Git makes collaboration a breeze. No more messy files or accidental overwrites!\nNow, I know what you might be thinking — “Do I really need this if I work solo?” Absolutely! Even when you’re the only one writing code, Git gives you the peace of mind that your work is safe, and you can always go back to earlier versions if something breaks.\nAnd here’s the best part: using Git with RStudio is super simple. You don’t need to touch the command line unless you want to. RStudio has a nice, intuitive Git tab that lets you do everything with just a few clicks. We’ll walk through how to set it up, what the key Git commands mean, and how you can start using Git today to simplify your workflow. Trust me, you’ll wonder how you ever coded without it!\n\n\nWhy Use Git? The Benefits for R Users\nAlright, let’s talk about why Git is a game-changer for your R projects. You might be working on an analysis one day, tweaking some code the next, and suddenly, you need to go back to an earlier version of your script because something went wrong. Without version control, that can turn into a bit of a nightmare — trying to remember what changed and when, or worse, redoing hours of work.\nGit helps with all that and more. Here’s why you’ll want it by your side:\n\nVersion Control Made Easy\nThink of Git like a detailed logbook for your code. Every time you make a change, you can “commit” that change to your log. That way, you always know what you changed and why. If something breaks down the line, you can scroll back through the history and undo the specific change that caused the issue. It’s like having a time machine for your code!\nIn RStudio, using Git is even easier. You don’t have to remember complex commands — there’s a visual Git tab where you can see all your changes and commit them with just a couple of clicks. It’s a lot more straightforward than you might think.\n\n\n\nImage\n\n\nAnd when you click any of these buttons, a modal window appears.\n\n\n\nImage\n\n\nIf you prefer working in the console, no problem. You can run the git commit command there:\ngit commit -m \"Brief description of what changed\"\n\n\nSafety Net for Your Code\nGit provides peace of mind by saving different versions of your code as you go. If something goes wrong, you can easily compare changes between versions, or revert files that have issues. While RStudio doesn’t offer a one-click rollback feature, you can still see the differences between file versions using the Diff tool.\nThe Diff feature in RStudio’s Git tab highlights exactly what’s been changed in your code line by line. If you spot a mistake, you can easily undo changes before committing them, or selectively stage only the lines you want to keep.\n\n\n\nImage\n\n\nHere’s how you’d use it in the console:\ngit diff\nThis command shows the differences between your working directory and the latest commit. It’s a lifesaver when you need to see what’s changed without committing right away.\nFor small adjustments, you can also use the Revert button in the RStudio Git tab to undo local changes before they’re committed. It’s like hitting “undo” in your editor, but for your version history.\nFor console:\ngit checkout -- &lt;file&gt;\nThis command reverts a file back to its previous state.\n\n\nSmooth Collaboration\nIf you’re working with a team, Git takes care of version conflicts and merges automatically. No more passing files back and forth or worrying about overwriting someone else’s work. Even if you’re flying solo, Git is still a huge help. You can create separate “branches” for different ideas, test them out, and then merge the best solution back into your main project.\n\n\n\nImage\n\n\nGit in RStudio or from the command line works the same way, so you get the best of both worlds.\n\n\n\nGetting Started with Git in RStudio: Simple, Visual, and Intuitive\nSo, you’re ready to dive into Git, but the idea of typing commands into the terminal feels a bit daunting? No worries — RStudio has you covered. The good news is that you don’t need to touch the command line to use Git effectively. RStudio offers a visual Git interface that makes the whole process smooth and intuitive, even if you’re brand new to version control.\n\nThe Git Tab in RStudio: A Visual Dashboard for Version Control\nThe Git tab in RStudio acts as your version control dashboard. It’s like having all the essential Git commands right there at your fingertips — no typing, just clicking. This tab shows you what’s changed in your project, allows you to stage files, commit your work, and push changes to a remote repository like GitHub.\nWhen you’ve made changes to your files, RStudio automatically detects them and lists them in the Git tab. You’ll see the files that have been modified, and you can decide which changes you want to commit. It’s as simple as selecting the files you want to include and hitting the Commit button.\n\n\nSetting Up Git in RStudio\nBefore you start, you need to make sure Git is configured in RStudio. If you haven’t already set it up, don’t worry — it’s quick and painless.\n\nInstall Git: If Git isn’t installed on your computer yet, RStudio will prompt you to install it or help you locate it on your system.\nLink Your Project to Git: To add Git to an existing project, go to the Tools menu, select Project Options, and then click on Version Control. Here, you can initialize a Git repository, which basically means you’re telling Git to start tracking changes in your project.\nConnect to GitHub (Optional): If you want to store your code on GitHub (highly recommended for backups and collaboration), you can link your RStudio project to a GitHub repository. This way, you can push your changes to GitHub with just a click.\n\n\n\nPoint-and-Click Simplicity: Key Git Features in RStudio\nHere’s a quick rundown of the most common actions you’ll be performing with Git in RStudio, and how easy it is to do them through the interface:\nStage Files (Add): Before committing changes, you need to stage them. Staging is like preparing files to be included in the next version of your project. In the Git tab, just check the boxes next to the files you want to stage, and they’re ready to go.\nIn the console, you’d use:\ngit add &lt;file&gt;\nCommit Changes: Once your files are staged, hit the Commit button, add a brief message describing your changes, and voilà — your changes are saved to the project’s version history.\nFor console:\ngit commit -m \"Commit message\"\nPush to GitHub (or another remote): After committing your changes, you’ll want to back them up or share them with your team. RStudio makes this super easy with the Push button, which sends your changes to your GitHub repository.\nConsole equivalent:\ngit push\nPull Updates from GitHub: If you’re working with others or just need to sync up with the latest version stored on GitHub, use the Pull button to fetch updates.\nConsole equivalent:\ngit pull\nDiff (Check What’s Changed): Want to see exactly what you changed before committing? The Diff tool highlights line-by-line differences between the current version and the previous one. It’s perfect for making sure everything looks right before you commit your work.\nIn the console, you’d use:\ngit diff\n\n\n\nEssential Git Commands Explained with Real-Life Comparisons\nNow that we’re diving deeper into Git, let’s make things even easier by comparing Git commands to everyday situations you’re already familiar with. Trust me, Git may sound technical, but once you get the hang of it, it’s no more complicated than organizing a filing cabinet or working on a group project.\n\n1. Commit: Saving a Checkpoint\nImagine you’re writing a book. Every time you finish a chapter or a significant section, you save it as a draft. That way, if you decide to change something later or realize a mistake, you can always go back to the previous version.\nA commit in Git is like saving one of these checkpoints. You’re creating a snapshot of your code at that moment, along with a note explaining what you changed. So, if something goes wrong later, you can look back and easily see the state of your project at each step along the way.\nIn RStudio, it’s as simple as clicking “Commit,” jotting down a quick note (like “Finished data cleaning section”), and you’re done!\n\n\n2. Add: Staging Your Changes\nLet’s stick with the book analogy. Before you finalize a new chapter, you gather all your notes and edits, maybe mark a few key points, and decide what you want to include in the draft. This is like staging files in Git.\nWhen you add files to be staged, you’re telling Git, “Hey, I want to include these in my next commit.” You’re getting them ready, but the commit doesn’t happen until you say, “Okay, I’m happy with these changes, let’s save them as a checkpoint.”\nIn RStudio, it’s as easy as checking the boxes next to the files you want to stage.\n\n\n3. Push: Sending Changes to the Cloud\nPicture this: You’ve been working on an important document on your laptop, but you want to make sure it’s saved somewhere safer, like in the cloud or on another computer. Pushing your changes in Git is just like backing up your work to Google Drive or Dropbox. You’re sending your local changes to a remote location (like GitHub) so you don’t lose anything, and others can access the latest version if you’re working with a team.\nIn RStudio, a quick click on “Push” gets your updates safely stored in the cloud.\n\n\n4. Pull: Updating with the Latest Changes\nLet’s say you’re collaborating on that same book with a co-author. They’ve been working on their chapters, and you’ve been working on yours. Before you can put everything together, you need to see the latest version of their work. This is where pulling comes in.\nIn Git, when you pull, you’re fetching the most recent changes from the remote repository (like GitHub) and updating your local copy to match. It ensures you’re always working with the most up-to-date version of the project, whether it’s from a collaborator or just an updated backup.\nIn RStudio, the Pull button is your friend for grabbing the latest changes.\n\n\n5. Diff: Spotting the Differences\nEver compare two versions of a document and try to figure out what’s changed? Maybe you highlight the edits or use track changes in Word. That’s basically what Diff does in Git.\nThe Diff tool in RStudio lets you see exactly what lines of code were added, removed, or changed between two versions of your project. It’s super helpful when you want to review your work before committing or if you’re collaborating and need to check what’s different from the last time you pulled changes.\nThink of it like using track changes in a shared document — it shows what’s new and what’s different at a glance.\n\n\n6. Ignore: Keeping the Junk Out\nNot every piece of information is worth saving or tracking. Let’s say you’re cleaning up your house and decide you don’t need to keep every receipt, flyer, or random scrap of paper — you toss those into the trash or recycling. That’s essentially what Ignore does in Git.\nYou can tell Git to ignore certain files — like temporary files or large datasets that don’t need to be tracked — so your project stays clean and clutter-free. In RStudio, you can right-click on a file and choose to ignore it, keeping only the important stuff in your version history.\n\n\n7. Revert: Undoing a Mistake\nImagine you’ve rearranged your living room furniture, but after a few hours, you realize the old setup was better. You move everything back to how it was. That’s what Revert does in Git — it lets you undo changes you haven’t committed yet.\nIf you’ve made edits to your code and realize they weren’t quite right, Revert allows you to go back to the previous state, no harm done. It’s like hitting the undo button in RStudio, taking your file back to how it looked before your latest changes.\n\n\n8. Merge: Combining Different Versions\nFinally, let’s talk about Merging — something we do in real life all the time. Think of merge like planning a party with a group of friends. One friend is in charge of decorations, another handles food, and you’re organizing the guest list. At some point, you need to bring everything together to make sure the party happens seamlessly. In Git, this is what merging is — combining the work from different branches (or people) into one cohesive project.\nIf you’ve been working on a feature in a separate branch, Merge lets you combine it with the main project. In RStudio, this is straightforward, but for more complicated merges, it’s often best done on GitHub or through a more advanced tool.\n\n\n\nReal-World Examples: Git in Action for R Programmers\nNow that we’ve covered the essential Git commands, let’s see how they actually play out in everyday coding scenarios. Whether you’re working alone or as part of a team, Git can help you stay organized, avoid costly mistakes, and collaborate smoothly. Here are a couple of real-life examples of how you can use Git in your R projects.\n\nScenario 1: Solo Project — Keeping Your Code Organized and Safe\nLet’s say you’re working on a data analysis project in R. You’re experimenting with different methods — trying out one model, then switching to another, tweaking parameters, and running different tests. Before you know it, you’ve got multiple versions of your code, and it’s hard to remember which one was working best.\nWithout Git, you might end up with a bunch of files named something like analysis_v1.R, analysis_final.R, or even worse, analysis_FINAL_final.R. We’ve all been there, right? It’s messy, and you risk losing track of which version does what.\nHow Git Helps:\n\nCommit Regularly: Each time you make progress or try something new, you commit your changes. This way, you have a clear history of every change you’ve made. You can always go back to an earlier version if something stops working.\nBranch for Experimentation: Instead of editing your main script directly, you can create a new branch and experiment with new ideas without messing up your original code. If your experiment works, you can merge it back into the main branch. If not, no harm done!\nTrack Changes with Diff: By using the Diff tool, you can easily see what changed between your latest commit and the previous version. It’s super helpful when you’re debugging and trying to figure out where things went wrong.\n\n\n\nScenario 2: Team Project — Collaborating Without Confusion\nNow imagine you’re working with a small team on a larger R project. Maybe you’re all contributing to a package or a shared analysis. One person is handling the data cleaning, someone else is working on visualizations, and you’re building models. Without version control, it would be a nightmare trying to combine everyone’s work without overwriting files or creating conflicts.\nHow Git Helps:\n\nSeparate Branches for Each Contributor: Each person can work on their own branch, focusing on their part of the project. For example, you might have a branch for data cleaning, a branch for visualizations, and another for modeling. This way, no one’s work interferes with anyone else’s. Everyone has their own space to work in.\nPull to Stay Up-to-Date: Before starting your day’s work, you pull the latest changes from the main repository to make sure you’re working with the most up-to-date files. This way, you’re always in sync with your teammates, and you avoid nasty surprises when it’s time to merge everything.\nMerging Work Smoothly: Once each person has finished their part, they merge their branch back into the main project. Git handles the merging process and will let you know if there are any conflicts that need to be resolved. No more accidentally overwriting someone else’s code!\nResolving Conflicts: Sometimes, two people might edit the same part of the project at the same time. Git helps you identify these conflicts and provides tools to resolve them. Instead of losing changes, you can decide whose work to keep or combine them manually.\n\n\n\nWhy It Matters:\nWhether you’re a solo programmer or working in a team, these scenarios show how Git can make your work easier and safer. For solo projects, it’s all about keeping your work organized, avoiding mistakes, and being able to experiment freely. For team projects, Git prevents the chaos of file versions and conflicting changes, allowing everyone to work together smoothly.\nGit doesn’t just keep track of your code — it gives you confidence. It frees you from worrying about losing progress or making irreversible mistakes. And when working with others, it ensures that collaboration is smooth, clear, and conflict-free.\n\n\n\nBusting Myths: Git is Not Just for “Techies”\nWhen people hear the word “Git,” they often think it’s something reserved for hardcore developers, working on giant projects with thousands of lines of code. But here’s the truth: Git is for everyone! Whether you’re an R programmer, data analyst, or someone who just dabbles in coding, Git can make your life easier. Let’s debunk some of the most common myths about Git and show why you should give it a try.\n\nMyth 1: “Git is too complicated for me.”\nLet’s be real — learning anything new can feel a little overwhelming at first. But Git? It’s not as complicated as it sounds, especially when you’re using it through an interface like RStudio. You don’t need to memorize a bunch of commands or master the command line to use Git effectively. The Git tab in RStudio is designed to make things simple.\nYou’re not alone in thinking this! Many people, especially those outside traditional software development, feel the same way initially. But once you start using Git, it’s a lot like saving files in any regular program — just with the added bonus of tracking every version and change.\nThink of it like this: If you can upload photos to the cloud, manage files in a folder, or send an email, you’re fully capable of using Git! With just a few clicks in RStudio, you can commit your changes, push them to a remote repository, and pull the latest updates.\n\n\nMyth 2: “I don’t need Git because I work alone.”\nThis one’s really common, but Git is incredibly useful even if you’re flying solo. You might think, “Why would I need to track versions of my code if it’s just me?” Well, think of Git as your personal safety net.\nWhen you’re working alone, it’s easy to accidentally overwrite something or lose track of the exact changes that broke (or fixed!) your code. Without Git, it’s hard to go back and recover old versions without creating messy files like project_final_final_v2.R. Git eliminates this headache. Every change you make is tracked, and you can always go back to earlier versions with ease.\nPlus, even solo workers often collaborate eventually — maybe you’ll share your project with a colleague, get feedback from a mentor, or open-source it on GitHub. With Git already in place, you’ll be ready for those moments without needing to scramble to get things organized.\n\n\nMyth 3: “Git is only for large projects.”\nAnother common misconception is that Git is overkill for small projects. But Git scales to fit any size project, whether you’re working on a massive codebase or just a simple R script.\nLet’s say you’re working on a small analysis that only spans a few files. It might seem manageable at first, but as the project evolves, things can quickly get out of hand. Even for small projects, Git helps you keep everything tidy and lets you track changes as the project grows. You can also branch out when trying new ideas, ensuring your experiments don’t mess up your main work.\nAnd remember, Git is not just for code. It can be used for any file you want to version control, from scripts and markdown files to documentation or even presentations. Whether your project is small or large, Git can help you stay organized from start to finish.\n\n\nMyth 4: “I need to know the command line to use Git.”\nNope, not at all! If the idea of typing commands into a terminal makes you nervous, you’ll be happy to know that RStudio takes care of that for you. With its friendly Git tab, you can do all the core tasks — committing, pushing, pulling, diffing — without ever touching the command line.\nThe interface is intuitive: buttons for committing, checkboxes for staging, and visual tools for reviewing changes. It’s like using any other piece of software, where a few clicks get the job done. You can still learn the command line if you want to (it’s powerful!), but it’s not required to start using Git.\n\n\nMyth 5: “If I make a mistake, Git will mess up my project.”\nThis myth often stops people from trying Git, but it couldn’t be further from the truth. One of the best things about Git is that it’s built to help you avoid mistakes, and if you do make one, Git makes it super easy to fix. Whether it’s rolling back to a previous version or reverting a specific file, Git gives you the tools to correct errors without losing work.\nInstead of messing up your project, Git actually protects you. If something goes wrong, you can always go back to a known good state, whether that’s undoing an uncommitted change or restoring a previous version of your code.\n\n\n\nReady to Git Started?\nYou’ve made it this far, and now you’re ready to take control of your R projects with Git. Whether you’re working alone or with a team, Git is the tool that keeps your code organized, safe, and easy to manage. It doesn’t matter if you’re a seasoned developer or just starting your journey with R — Git helps you avoid chaos and gives you peace of mind, knowing that every version of your work is saved and recoverable.\nLet’s quickly recap why Git is so valuable:\n\nVersion Control Made Simple: No more cluttered folders filled with “final_final_v2” files. Every change is tracked, and you can easily revisit earlier versions.\nExperiment with Confidence: Want to try something new? Create a branch, experiment, and merge it back if it works. If it doesn’t, no harm done!\nSmooth Collaboration: Working with others becomes seamless. You can all contribute to the same project without worrying about overwriting each other’s work.\nA Personal Safety Net: Even if you’re flying solo, Git ensures that mistakes aren’t the end of the world. You can always undo, roll back, and recover previous versions with ease.\n\nAnd remember, RStudio’s Git tab makes all of this incredibly easy with its intuitive interface. There’s no need to feel overwhelmed or intimidated. With just a few clicks, you can commit changes, push them to GitHub, pull updates from collaborators, and much more.\nSo, what’s next?\n\nTry It Out\nThe best way to learn Git is by doing. Start small — set up Git for a personal R project, make a few commits, and get a feel for how it works in RStudio. You don’t have to dive in all at once. The more you use it, the more natural it will feel.\n\n\nExplore More Resources\nIf you want to dive deeper into Git, here are some great resources to continue your learning:\n\nGit and GitHub for Beginners: There are plenty of video tutorials and interactive guides that walk you through the basics, step by step.\nRStudio’s Git Integration Documentation: RStudio provides excellent resources to help you understand how to integrate Git into your workflow.\nGit Cheat Sheet: Keep a Git cheat sheet handy! It’s a quick reference for those times when you can’t quite remember the command you need.\n\n\n\nFinal Thoughts\nGit is one of those tools that, once you start using it, you’ll wonder how you ever lived without it. It’s not just for huge, complex projects — it’s for anyone who writes code, whether big or small. By incorporating Git into your RStudio workflow, you’ll have a powerful version control system at your fingertips, giving you the freedom to experiment, collaborate, and keep your work safe.\nSo, R you ready? It’s time to Git your code under control!"
  },
  {
    "objectID": "ds/posts/2024-10-24_Table-It-Like-a-Pro--Print-Ready-Tables-in-R-ff1856611008.html",
    "href": "ds/posts/2024-10-24_Table-It-Like-a-Pro--Print-Ready-Tables-in-R-ff1856611008.html",
    "title": "Table It Like a Pro: Print-Ready Tables in R",
    "section": "",
    "text": "Table It Like a Pro: Print-Ready Tables in R\n\n\n\nImage\n\n\nYou’ve probably been there — mention to someone that you work with data, and they say, “Oh, so you just make tables?” Tables, right? How hard could it be? It’s just rows and columns. But we know better, don’t we? Tables aren’t just a random dumping ground for numbers; they’re the quiet superheroes of the data world. They give structure to chaos, they summarize the story our data is trying to tell, and they can make or break how well our insights land with an audience.\nFor us, a well-crafted table is more than just numbers on a page — it’s a tool of communication, a piece of art (well, almost), and a key part of our data workflow.\n\n\nThe Unsung Heroes: Why Tables Matter in Data Analysis\nTables might not get the same attention as those slick graphs or shiny dashboards, but don’t be fooled — they’re the real workhorses behind the scenes. Sure, charts are great for showing trends or making things visual, but when it comes to details, tables take the crown. Let’s face it, when you need to communicate something concrete and precise, you reach for a table.\nThink about your day-to-day as a data person. How many times have you had to provide a summary for a project? Or maybe your boss asks for a snapshot of key performance indicators (KPIs), or someone needs to see how metrics have changed over time. What’s your go-to solution? Yep, a table.\nTables are the Swiss Army knives of data presentation — they can do pretty much anything. They’re ideal when you need to:\n\nSummarize Results: Whether it’s showing averages, counts, percentages, or more complex stats, a table is perfect for giving a clear, detailed snapshot.\nCompare Metrics: Want to compare sales figures across regions or show how customer satisfaction has changed quarter to quarter? A table has you covered.\nOrganize Data: Tables allow you to take heaps of messy data and make it organized, giving it a structure that’s easier to digest.\nShare Reports: Need to drop some numbers into a PDF or export them into Excel? Tables are versatile and can easily transition from R into professional documents.\n\nBut tables aren’t just about dumping numbers in neat rows and columns. A well-made table can tell a story, condensing complex information into something a reader can scan in seconds and still get all the key insights. A bad table? That’s a surefire way to confuse people, overload them with data, and make sure no one actually reads what you’re trying to say.\nGood tables help bridge the gap between raw data and the story it’s telling. For example, let’s say you’re analyzing sales performance. A simple table can show sales by region, by product, by time period — you name it. Suddenly, what was just a sea of numbers becomes a meaningful comparison: “Oh look, sales in the Northeast jumped by 20% this quarter, while the Southwest dipped slightly.”\nIt’s this flexibility and power that make tables such an important part of our job. And let’s be honest — whether you’re sending off reports to a client or presenting your findings to your team, a well-crafted table can make you look like you’ve got your act together. It says, “Hey, I didn’t just throw some data together — I organized it.”\nTables are the unsung heroes because they do the grunt work of presenting detailed information without the flash — but with all the substance. In the end, they’re not just about presenting data; they’re about ensuring that data is understood.\n\n\nTables in R: More Than Meets the Eye\nNow that we’ve set the stage for why tables are so crucial, let’s talk about how R makes it all happen. If you’re new to R, you might think creating tables is as simple as printing out a few rows and columns — and sure, that’s one way to start. But as you’ll see, there’s so much more you can do.\nLet’s begin with the basics. If you’ve got a dataset loaded into R, you can print it right to your console with something as simple as print() or even just calling the object’s name. It’s quick, it’s dirty, and it works when you need to peek at your data. But the real power comes when you start to customize your output, because let’s face it, the default look of console tables? Pretty bare-bones.\nFor a quick improvement, you’ve got kable() from the knitr package, which lets you turn basic R output into nicely formatted Markdown tables. It’s a great way to start if you’re looking to add tables directly into documents, be they Markdown, HTML, or PDF. Here’s a simple example:\nlibrary(knitr)\n\n# Basic table in Markdown\nkable(head(mtcars), format = \"markdown\")\n\n\n\nImage\n\n\nThis gives you a clean, easy-to-read table that can fit right into your R Markdown reports. It’s simple, but it instantly upgrades the way your data is presented. Whether you’re working on an internal project or sending off client-facing reports, you want your tables to be clear, and kable() makes that happen with minimal effort.\n\n\nPrinting Tables to PDFs, Word Docs, and HTML\nNow, let’s step it up a notch. What if you need to include your tables in more polished reports — like a PDF or Word document? R has your back with the rmarkdown and officer packages. These allow you to knit your R scripts directly into these formats, and — bonus! — they keep your tables looking slick.\nFor example, if you’re knitting to a PDF document, you can still use kable() for your tables. Here’s a quick look at how you can do that:\n---\ntitle: \"My PDF Report\"\noutput: pdf_document\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(knitr)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: pakiet 'knitr' został zbudowany w wersji R 4.4.2\n```\n\n\n:::\n\n```{.r .cell-code}\nkable(head(mtcars), format = \"latex\")\n```\n\n::: {.cell-output-display}\n\n\\begin{tabular}{l|r|r|r|r|r|r|r|r|r|r|r}\n\\hline\n  & mpg & cyl & disp & hp & drat & wt & qsec & vs & am & gear & carb\\\\\n\\hline\nMazda RX4 & 21.0 & 6 & 160 & 110 & 3.90 & 2.620 & 16.46 & 0 & 1 & 4 & 4\\\\\n\\hline\nMazda RX4 Wag & 21.0 & 6 & 160 & 110 & 3.90 & 2.875 & 17.02 & 0 & 1 & 4 & 4\\\\\n\\hline\nDatsun 710 & 22.8 & 4 & 108 & 93 & 3.85 & 2.320 & 18.61 & 1 & 1 & 4 & 1\\\\\n\\hline\nHornet 4 Drive & 21.4 & 6 & 258 & 110 & 3.08 & 3.215 & 19.44 & 1 & 0 & 3 & 1\\\\\n\\hline\nHornet Sportabout & 18.7 & 8 & 360 & 175 & 3.15 & 3.440 & 17.02 & 0 & 0 & 3 & 2\\\\\n\\hline\nValiant & 18.1 & 6 & 225 & 105 & 2.76 & 3.460 & 20.22 & 1 & 0 & 3 & 1\\\\\n\\hline\n\\end{tabular}\n\n\n:::\n:::\n\n\n\nImage\n\n\nBy switching the format to latex, you’re telling R to produce a PDF-ready table. But what if your boss (or client) loves Word documents? Not a problem! With officer, you can generate tables in a .docx file that look sharp and professional. Here’s a quick peek at how to do that:\nlibrary(officer)\nlibrary(flextable)\n\ndoc &lt;- read_docx()\n\n# Add a title and table\ndoc &lt;- body_add_par(doc, \"Table of Cars\", style = \"heading 1\")\ndoc &lt;- body_add_flextable(doc, flextable(head(mtcars)))\n\nprint(doc, target = \"my_report.docx\")\n\n\n\nImage\n\n\nSuddenly, you’ve got a Word document with a table that looks like it belongs in a professional report. This is the kind of flexibility that makes R such a powerhouse when it comes to data presentation — whether you’re generating quick Markdown tables or polished Word and PDF documents.\n\n\nLeveling Up: Working with Excel in R\nExcel is still a big deal in many workplaces, and let’s be honest, it’s not going anywhere. If you’re handling reports, budgets, or performance tracking, chances are good that someone’s going to ask you for an Excel file. Luckily, R can easily handle Excel files — whether you’re reading data in or exporting results out.\n\n\nReading and Writing Excel Files\nFirst up, let’s talk about reading from Excel. With the readxl package, importing an Excel file into R is as simple as calling read_excel(). But what if you want to export your data from R into an Excel-friendly format? That’s where openxlsx or writexl come in. Here’s how you can take a dataset like the ggplot2::diamonds dataset and write it to an Excel file:\nlibrary(openxlsx)\nlibrary(ggplot2)\n\n# Taking a sample of the diamonds dataset for demo\ndiamonds_sample &lt;- diamonds[sample(nrow(diamonds), 100), ]\n\n# Writing the data to an Excel file\nwrite.xlsx(diamonds_sample, \"diamonds_sample.xlsx\", sheetName = \"Diamonds Sample\")\nWith this, you’ve written a sample of the diamonds dataset to an Excel file with just a couple of lines. You can quickly export your data, whether it’s a quick analysis or a detailed report, for others to work with in Excel.\n\n\nHandling Excel Workbooks and Formatting\nWhat if you need more than just one simple table — let’s say multiple sheets, or maybe you want to add some styling to make the data presentation look polished? openxlsx gives you full control over these things.\nLet’s go ahead and create a workbook with two sheets: one containing a sample of the diamonds dataset and another with a summary of the data. Plus, we’ll add some styling to make the Excel file look professional.\n# Create a new workbook\nwb &lt;- createWorkbook()\n\n# Add two worksheets\naddWorksheet(wb, \"Diamonds Data\")\naddWorksheet(wb, \"Summary\")\n\n# Write the diamonds sample data to the first sheet\nwriteData(wb, sheet = \"Diamonds Data\", diamonds_sample)\n\n# Create a summary of the diamonds dataset\nsummary_data &lt;- data.frame(\n  Mean_Price = mean(diamonds_sample$price),\n  Median_Carat = median(diamonds_sample$carat),\n  Max_Price = max(diamonds_sample$price)\n)\n\n# Write the summary to the second sheet\nwriteData(wb, sheet = \"Summary\", summary_data)\n\n# Apply styling to the header of the Diamonds Data sheet\nheaderStyle &lt;- createStyle(textDecoration = \"bold\", fontColour = \"#FFFFFF\", fgFill = \"#4F81BD\")\naddStyle(wb, sheet = \"Diamonds Data\", style = headerStyle, rows = 1, cols = 1:ncol(diamonds_sample), gridExpand = TRUE)\n\n# Save the workbook\nsaveWorkbook(wb, \"styled_diamonds_report.xlsx\", overwrite = TRUE)\n\n\n\nImage\n\n\nHere’s what this code does:\n\nWe create an Excel workbook with two sheets: one for our diamonds data sample and one for a quick summary.\nWe write both the sample data and summary into their respective sheets.\nThen, we style the header row of the data table, giving it a nice blue background with bold, white text for clarity.\nFinally, we save the workbook as styled_diamonds_report.xlsx.\n\nThe ability to customize the style, structure, and formatting of your Excel workbooks directly from R can save tons of time. Plus, automating this kind of report generation ensures consistency and professionalism.\nTables aren’t confined to R alone — thanks to these tools, you can seamlessly move your data between R and Excel, and even bring R’s automation power into Excel workflows.\n\n\nFormatting Excellence: Creating Print-Ready Tables with gt and Friends\nSo, you’ve got your data ready, and you’ve generated some tables. But here’s the thing — those default tables may not cut it when you’re aiming for a professional, polished look. Whether you’re preparing a report for stakeholders or a publication for a journal, you’ll want your tables to shine. That’s where the gt package comes in.\ngt stands for “Grammar of Tables,” and it’s a fantastic package for creating high-quality, beautifully formatted tables in R. It gives you control over almost every aspect of your table’s appearance—from styling text and adding footnotes to adjusting column widths and more.\n\n\nCreating Your First Table with gt\nLet’s start by creating a simple, yet nicely formatted table using the gt package. We’ll use the ggplot2::diamonds dataset again to generate a table with a few key columns, and we’ll style it for a professional look:\nlibrary(gt)\nlibrary(ggplot2)\n\n# Take a small sample of the diamonds dataset for our table\ndiamonds_sample &lt;- diamonds[sample(nrow(diamonds), 5), ]\n\n# Create a basic gt table\ngt_table &lt;- gt(diamonds_sample) %&gt;%\n  tab_header(\n    title = \"Diamonds Data Sample\",\n    subtitle = \"A snapshot of key attributes\"\n  ) %&gt;%\n  fmt_number(\n    columns = c(carat, price),\n    decimals = 2\n  ) %&gt;%\n  cols_label(\n    carat = \"Carat Weight\",\n    cut = \"Cut Quality\",\n    color = \"Diamond Color\",\n    clarity = \"Clarity Rating\",\n    price = \"Price (USD)\"\n  ) %&gt;%\n  tab_style(\n    style = list(\n      cell_text(weight = \"bold\")\n    ),\n    locations = cells_column_labels(everything())\n  ) %&gt;%\n  tab_footnote(\n    footnote = \"Data from ggplot2's diamonds dataset.\",\n    locations = cells_title(groups = \"title\")\n  )\n\n# Print the table\ngt_table\n\n\n\nImage\n\n\nIn this example, we:\n\nTake a small random sample of the diamonds dataset.\nCreate a gt table and set a title and subtitle for context.\nFormat the carat and price columns to show two decimal places.\nRename the column headers to something more descriptive.\nApply some basic bold styling to the column labels.\nAdd a footnote to the table to explain the data source.\n\nWith gt, this table looks polished and ready for a report or presentation, not just a quick console dump. The customization options mean you can make your tables look exactly the way you want them.\n\n\nExporting Tables to HTML, PDF, and More\nOne of the best things about gt is that it’s not just for the console. You can easily export your tables to different formats—like HTML for web pages or LaTeX for PDFs. Here’s how you can export the table we just created:\n# Save the table as an HTML file\ngtsave(gt_table, \"diamonds_table.html\")\n\n# Or save it as a PNG image\ngtsave(gt_table, \"diamonds_table.png\")\n\n\n\nImage\n\n\nThis flexibility lets you integrate your tables into various types of reports, whether you’re working on a web-based report, a PDF for publication, or just need a static image for presentations.\n\n\nUsing kableExtra for More Advanced Formatting\nIf you need even more advanced table customization, kableExtra is another excellent package to explore. It extends kable() from the knitr package, allowing for advanced formatting like row grouping, column spanning, and more.\nHere’s an example of a nicely formatted table using kableExtra, which allows for more complex layouts, like adding row groups:\n\n\n\nImage\n\n\nIn this case:\n\nWe select a few columns from the diamonds dataset.\nWe use kable() to create a basic table, and then apply kableExtra styling options to add features like striping and hover effects for HTML.\nWe also add a custom header row above the main table.\n\nWith kableExtra, you can quickly add professional touches like multi-level headers, row grouping, or different visual styles.\n\n\nWhy Use These Formatting Packages?\nWhether you go with gt for its user-friendly table creation and stunning formatting options, or kableExtra for more advanced customization, the goal is the same: producing polished, professional-looking tables that do more than just hold data—they communicate it clearly and attractively. These packages transform your tables from a plain grid into something that enhances your reports, presentations, or publications.\n\n\nConclusion\nBy now, it should be clear that tables are more than just a simple way to display data — they’re a powerful tool for communicating insights, presenting results, and helping others make sense of the story your data is telling. From basic console prints to beautifully formatted, publication-ready tables, R offers a wide variety of tools to turn raw data into organized, insightful tables that look as good as they perform.\nWe’ve explored how to:\n\nCreate basic tables with knitr::kable() for Markdown and document outputs.\nWork with Excel files using openxlsx to read, write, and style Excel sheets.\nGenerate polished, professional tables using gt and kableExtra, ensuring your tables are not just informative but also visually compelling.\n\nAs you’ve seen, tables are far from “just tables.” They’re the unsung heroes of data analysis, bringing structure and clarity to even the most complex datasets. With the right tools and formatting, they can elevate your work and make your data-driven reports stand out.\n\n\nWhat’s Next? Interactive Tables!\nNow that we’ve mastered the art of creating print-ready tables in R, it’s time to take things a step further. In the next article, we’ll dive into the world of interactive tables — exploring how to add sorting, filtering, and more using R’s powerful toolkits like Shiny, R Markdown, and Quarto. Imagine tables where users can engage with your data, making it even easier to explore and understand.\nGet ready to make your tables not just informative but interactive. Stay tuned!"
  },
  {
    "objectID": "ds/posts/mastering_purrr.html",
    "href": "ds/posts/mastering_purrr.html",
    "title": "Mastering purrr: From Basic Maps to Functional Magic in R",
    "section": "",
    "text": "purrr image\n\n\nWelcome back to the world of purrr! Last time (about a year ago), we spun a metaphorical yarn about the wonders of purrr in R. Today, we’re rolling up our sleeves and diving into a hands-on tutorial. We’re going to explore how purrr makes working with lists and vectors a breeze, transforming and manipulating them like a data wizard.\nWith purrr, you can apply functions to each element of a list or vector, manipulate them, check conditions, and so much more. It’s all about making your data dance to your commands with elegance and efficiency. Ready to unleash some functional magic?\n\nAre map Functions Like apply Functions?\nYou might be wondering, “Aren’t map functions just fancy versions of apply functions?” It’s a fair question! Both map and apply functions help you apply a function to elements in a data structure, but purrr takes it to a whole new level.\nHere’s why purrr and its map functions are worth your attention:\n\nConsistency: purrr functions have a consistent naming scheme, making them easier to learn and remember.\nType Safety: map functions in purrr return outputs of consistent types, reducing unexpected errors.\nIntegration: Seamlessly integrate with other tidyverse packages, making your data wrangling pipeline smoother.\n\nLet’s see a quick comparison:\nlibrary(tidyverse)\n\n# Using lapply (base R)\nnumbers &lt;- list(1, 2, 3, 4, 5)\nsquared_lapply &lt;- lapply(numbers, function(x) x^2)\n\n# Using map (purrr)\nsquared_map &lt;- map(numbers, ~ .x^2)\n\nprint(squared_lapply)\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 4\n\n[[3]]\n[1] 9\n\n[[4]]\n[1] 16\n\n[[5]]\n[1] 25\n\nprint(squared_map)\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 4\n\n[[3]]\n[1] 9\n\n[[4]]\n[1] 16\n\n[[5]]\n[1] 25\nBoth do the same thing, but purrr’s map function is more readable and concise, especially when paired with the tidyverse syntax.\nHere’s another example with a built-in dataset:\n# Using lapply with a built-in dataset\niris_split &lt;- split(iris, iris$Species)\nmean_sepal_length_lapply &lt;- lapply(iris_split, function(df) mean(df$Sepal.Length))\n\n# Using map with a built-in dataset\nmean_sepal_length_map &lt;- map(iris_split, ~ mean(.x$Sepal.Length))\n\nprint(mean_sepal_length_lapply)\n\n$setosa\n[1] 5.006\n\n$versicolor\n[1] 5.936\n\n$virginica\n[1] 6.588\n\nprint(mean_sepal_length_map)\n\n$setosa\n[1] 5.006\n\n$versicolor\n[1] 5.936\n\n$virginica\n[1] 6.588\nAgain, the purrr version is cleaner and easier to understand at a glance.\nConvinced? Let’s move on to explore simple maps and their variants to see more of purrr’s magic. Ready?\n\n\nSimple Maps and Their Variants\nNow that we know why purrr’s map functions are so cool, let’s dive into some practical examples. The map function family is like a Swiss Army knife for data transformation. It comes in different flavors depending on the type of output you want: logical, integer, character, or double.\nLet’s start with the basic map function:\nlibrary(tidyverse)\n\n# Basic map example\nnumbers &lt;- list(1, 2, 3, 4, 5)\nsquared_numbers &lt;- map(numbers, ~ .x^2)\nsquared_numbers\nEasy, right? Yes, but we have one twist here. Result is returned as list, and we don’t always need list. So now, let’s look at the type-specific variants. These functions ensure that the output is of a specific type, which can help avoid unexpected surprises in your data processing pipeline.\n\nLogical (map_lgl):\n\n# Check if each number is even\nis_even &lt;- map_lgl(numbers, ~ .x %% 2 == 0)\nis_even\n\n[1] FALSE  TRUE FALSE  TRUE FALSE\n\n# it is not list anymore, it is logical vector\n\nInteger (map_int):\n\n# Double each number and return as integers\ndoubled_integers &lt;- map_int(numbers, ~ .x * 2)\ndoubled_integers\n\n[1]  2  4  6  8 10\n\nCharacter (map_chr):\n\n# Convert each number to a string\nnumber_strings &lt;- map_chr(numbers, ~ paste(\"Number\", .x))\nnumber_strings\n\n[1] \"Number 1\" \"Number 2\" \"Number 3\" \"Number 4\" \"Number 5\"\n\nDouble (map_dbl):\n\n# Half each number and return as doubles\nhalved_doubles &lt;- map_dbl(numbers, ~ .x / 2)\nhalved_doubles\n\n[1] 0.5 1.0 1.5 2.0 2.5\nLet’s apply this to a built-in dataset to see it in action:\n# Using map_dbl on the iris dataset to get the mean of each numeric column\niris_means &lt;- iris %&gt;%\n  select(-Species) %&gt;%\n  map_dbl(mean)\niris_means\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n    5.843333     3.057333     3.758000     1.199333 \nHere, we’ve calculated the mean of each numeric column in the iris dataset, and the result is a named vector of doubles.\nPretty neat, huh? The map family makes it easy to ensure your data stays in the format you expect.\nReady to see how purrr handles multiple vectors with map2 and pmap?\n\n\nNot Only One Vector: map2 and pmap + Variants\nSo far, we’ve seen how map functions work with a single vector or list. But what if you have multiple vectors and want to apply a function to corresponding elements from each? Enter map2 and pmap.\n\nmap2: This function applies a function to corresponding elements of two vectors or lists.\npmap: This function applies a function to corresponding elements of multiple lists.\n\nLet’s start with map2:\nlibrary(tidyverse)\n\n# Two vectors to work with\nvec1 &lt;- c(1, 2, 3)\nvec2 &lt;- c(4, 5, 6)\n\n# Adding corresponding elements of two vectors\nsum_vecs &lt;- map2(vec1, vec2, ~ .x + .y)\nsum_vecs\n\n[[1]]\n[1] 5\n\n[[2]]\n[1] 7\n\n[[3]]\n[1] 9\nHere, map2 takes elements from vec1 and vec2 and adds them together.\nNow, let’s step it up with pmap:\n# Creating a tibble for multiple lists\ndf &lt;- tibble(\n  a = 1:3,\n  b = 4:6,\n  c = 7:9\n)\n\n# Summing corresponding elements of multiple lists\nsum_pmap &lt;- pmap(df, ~ ..1 + ..2 + ..3)\nsum_pmap\n\n[[1]]\n[1] 12\n\n[[2]]\n[1] 15\n\n[[3]]\n[1] 18\nIn this example, pmap takes elements from columns a, b, and c of the tibble and sums them up.\nLook at syntax in those two examples. In map2, we give two vectors or lists, and then we are reffering to them as .x and .y. Further in pmap example we have data.frame, but it can be a list of lists, and we need to refer to them with numbers like ..1, ..2 and ..3 (and more if needed).\n\n\nVariants of map2 and pmap\nJust like map, map2 and pmap have type-specific variants. Let’s see a couple of examples using data structures already defined above:\n\nmap2_dbl:\n\n# Multiplying corresponding elements of two vectors and returning doubles\nproduct_vecs &lt;- map2_dbl(vec1, vec2, ~ .x * .y)\nproduct_vecs\n\n[1]  4 10 18\n\npmap_chr:\n\n# Concatenating corresponding elements of multiple lists into strings\nconcat_pmap &lt;- pmap_chr(df, ~ paste(..1, ..2, ..3, sep = \"-\"))\nconcat_pmap\n\n[1] \"1-4-7\" \"2-5-8\" \"3-6-9\"\nThese variants ensure that your results are of the expected type, just like the basic map variants.\nWith map2 and pmap, you can handle more complex data transformations involving multiple vectors or lists with ease.\nReady to move on and see what lmap and imap can do for you?\n\n\nUsing imap for Indexed Mapping and Conditional Maps with _if and _at\nLet’s combine our exploration of imap with the conditional mapping functions map_if and map_at. These functions give you more control over how and when functions are applied to your data, making your code more precise and expressive.\n\nimap: Indexed Mapping\nThe imap function is a handy tool when you need to include the index or names of elements in your function calls. This is particularly useful for tasks where the position or name of an element influences the operation performed on it.\nHere’s a practical example with a named list:\nlibrary(tidyverse)\n\n# A named list of scores\nnamed_scores &lt;- list(math = 90, science = 85, history = 78)\n\n# Create descriptive strings for each score\nscore_descriptions &lt;- imap(named_scores, ~ paste(.y, \"score is\", .x))\nscore_descriptions\n\n$math\n[1] \"math score is 90\"\n\n$science\n[1] \"science score is 85\"\n\n$history\n[1] \"history score is 78\"\nIn this example:\n\nWe have a named list named_scores with subject scores.\nWe use imap to create a descriptive string for each score that includes the subject name and the score.\n\n\n\nConditional Maps with map_if and map_at\nSometimes, you don’t want to apply a function to all elements of a list or vector — only to those that meet certain conditions. This is where map_if and map_at come into play.\nmap_if: Conditional Mapping\nUse map_if to apply a function to elements that satisfy a specific condition (predicate).\n# Mixed list of numbers and characters\nmixed_list &lt;- list(1, \"a\", 3, \"b\", 5)\n\n# Double only the numeric elements\ndoubled_numbers &lt;- map_if(mixed_list, is.numeric, ~ .x * 2)\ndoubled_numbers\n\n[[1]]\n[1] 2\n\n[[2]]\n[1] \"a\"\n\n[[3]]\n[1] 6\n\n[[4]]\n[1] \"b\"\n\n[[5]]\n[1] 10\nIn this example:\n\nWe have a mixed list of numbers and characters.\nWe use map_if to double only the numeric elements, leaving the characters unchanged.\n\nmap_at: Specific Element Mapping\nUse map_at to apply a function to specific elements of a list or vector, identified by their indices or names.\n# A named list of mixed types\nspecific_list &lt;- list(a = 1, b = \"hello\", c = 3, d = \"world\")\n\n# Convert only the character elements to uppercase\nuppercase_chars &lt;- map_at(specific_list, c(\"b\", \"d\"), ~ toupper(.x))\nuppercase_chars\n\n$a\n[1] 1\n\n$b\n[1] \"HELLO\"\n\n$c\n[1] 3\n\n$d\n[1] \"WORLD\"\nIn this example:\n\nWe have a named list with mixed types.\nWe use map_at to convert only the specified character elements to uppercase.\n\nCombining imap, map_if, and map_at allows you to handle complex data transformation tasks with precision and clarity. These functions make it easy to tailor your operations to the specific needs of your data.\nShall we move on to the next chapter to explore walk and its friends for side-effect operations?\n\n\n\nMake Something Happen Outside of Data: walk and Its Friends\nSometimes, you want to perform operations that have side effects, like printing, writing to a file, or plotting, rather than returning a transformed list or vector. This is where the walk family of functions comes in handy. These functions are designed to be used for their side effects, as they return NULL.\n\nwalk\nThe basic walk function applies a function to each element of a list or vector and performs actions like printing or saving files.\nlibrary(tidyverse)\n\n# A list of numbers\nnumbers &lt;- list(1, 2, 3, 4, 5)\n\n# Print each number\nwalk(numbers, ~ print(.x))\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\nIn this example, walk prints each element of the numbers list.\n\n\nwalk2\nWhen you have two lists or vectors and you want to perform side-effect operations on their corresponding elements, walk2 is your friend.\n# Two vectors to work with\nvec1 &lt;- c(\"apple\", \"banana\", \"cherry\")\nvec2 &lt;- c(\"red\", \"yellow\", \"dark red\")\n\n# Print each fruit with its color\nwalk2(vec1, vec2, ~ cat(.x, \"is\", .y, \"\\n\"))\n\napple is red \nbanana is yellow \ncherry is dark red \nHere, walk2 prints each fruit with its corresponding color.\n\n\niwalk\niwalk is the side-effect version of imap. It includes the index or names of the elements, which can be useful for logging or debugging.\n# A named list of scores\nnamed_scores &lt;- list(math = 90, science = 85, history = 78)\n\n# Print each subject with its score\niwalk(named_scores, ~ cat(\"The score for\", .y, \"is\", .x, \"\\n\"))\n\nThe score for math is 90 \nThe score for science is 85 \nThe score for history is 78 \nIn this example, iwalk prints each subject name with its corresponding score.\n\n\nPractical Example with Built-in Data\nLet’s use a built-in dataset and perform some side-effect operations. Suppose you want to save plots of each numeric column in the mtcars dataset to separate files.\n# Directory to save plots\ndir.create(\"plots\")\n\n# Save histograms of each numeric column to files\nwalk(names(mtcars), ~ {\n  if (is.numeric(mtcars[[.x]])) {\n    plot_path &lt;- paste0(\"plots/\", .x, \"_histogram.png\")\n    png(plot_path)\n    hist(mtcars[[.x]], main = paste(\"Histogram of\", .x), xlab = .x)\n    dev.off()\n  }\n})\n\n\n\nmtcars histogram\n\n\nIn this example:\n\nWe create a directory called “plots”.\nWe use walk to iterate over the names of the mtcars dataset.\nFor each numeric column, we save a histogram to a PNG file.\n\nThis is a practical demonstration of how walk can be used for side-effect operations such as saving files.\n\n\n\nWhy Do We Need modify Then?\nSometimes you need to tweak elements within a list or vector without completely transforming them. This is where modify functions come in handy. They allow you to make specific changes to elements while preserving the overall structure of your data.\n\nmodify\nThe modify function applies a transformation to each element of a list or vector and returns the modified list or vector.\nlibrary(tidyverse)\n\n# A list of numbers\nnumbers &lt;- list(1, 2, 3, 4, 5)\n\n# Add 10 to each number\nmodified_numbers &lt;- modify(numbers, ~ .x + 10)\nmodified_numbers\n\n[[1]]\n[1] 11\n\n[[2]]\n[1] 12\n\n[[3]]\n[1] 13\n\n[[4]]\n[1] 14\n\n[[5]]\n[1] 15\nIn this example, modify adds 10 to each element of the numbers list.\n\n\nmodify_if\nmodify_if is used to conditionally modify elements that meet a specified condition (predicate).\n# Modify only the even numbers by multiplying them by 2\nmodified_if &lt;- modify_if(numbers, ~ .x %% 2 == 0, ~ .x * 2)\nmodified_if\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 4\n\n[[3]]\n[1] 3\n\n[[4]]\n[1] 8\n\n[[5]]\n[1] 5\nHere, modify_if multiplies only the even numbers by 2.\n\n\nmodify_at\nmodify_at allows you to specify which elements to modify based on their indices or names.\n# A named list of mixed types\nnamed_list &lt;- list(a = 1, b = \"hello\", c = 3, d = \"world\")\n\n# Convert only the specified elements to uppercase\nmodified_at &lt;- modify_at(named_list, c(\"b\", \"d\"), ~ toupper(.x))\nmodified_at\n\n$a\n[1] 1\n\n$b\n[1] \"HELLO\"\n\n$c\n[1] 3\n\n$d\n[1] \"WORLD\"\nIn this example, modify_at converts the specified character elements to uppercase.\n\n\nmodify with Built-in Dataset\nLet’s use the iris dataset to demonstrate how modify functions can be applied in a practical scenario. Suppose we want to normalize numeric columns by dividing each value by the maximum value in its column.\n# Normalizing numeric columns in the iris dataset\nnormalized_iris &lt;- iris %&gt;%\n  modify_at(vars(Sepal.Length, Sepal.Width, Petal.Length, Petal.Width), \n            ~ .x / max(.x))\n\nhead(normalized_iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1    0.6455696   0.7954545    0.2028986        0.08  setosa\n2    0.6202532   0.6818182    0.2028986        0.08  setosa\n3    0.5949367   0.7272727    0.1884058        0.08  setosa\n4    0.5822785   0.7045455    0.2173913        0.08  setosa\n5    0.6329114   0.8181818    0.2028986        0.08  setosa\n6    0.6835443   0.8863636    0.2463768        0.16  setosa\n\nhead(iris)\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\nIn this example:\n\nWe use modify_at to specify the numeric columns of the iris dataset.\nEach value in these columns is divided by the maximum value in its respective column, normalizing the data.\n\nmodify functions offer a powerful way to make targeted changes to your data, providing flexibility and control.\n\n\n\nPredicates: Does Data Satisfy Our Assumptions? every, some, and none\nWhen working with data, it’s often necessary to check if certain conditions hold across elements in a list or vector. This is where predicate functions like every, some, and none come in handy. These functions help you verify whether elements meet specified criteria, making your data validation tasks easier and more expressive.\n\nevery\nThe every function checks if all elements in a list or vector satisfy a given predicate. If all elements meet the condition, it returns TRUE; otherwise, it returns FALSE.\nlibrary(tidyverse)\n\n# A list of numbers\nnumbers &lt;- list(2, 4, 6, 8)\n\n# Check if all numbers are even\nall_even &lt;- every(numbers, ~ .x %% 2 == 0)\nall_even\n\n[1] TRUE\nIn this example, every checks if all elements in the numbers list are even.\n\n\nsome\nThe some function checks if at least one element in a list or vector satisfies a given predicate. If any element meets the condition, it returns TRUE; otherwise, it returns FALSE.\n# Check if any number is greater than 5\nany_greater_than_five &lt;- some(numbers, ~ .x &gt; 5)\nany_greater_than_five\n\n[1] TRUE\nHere, some checks if any element in the numbers list is greater than 5.\n\n\nnone\nThe none function checks if no elements in a list or vector satisfy a given predicate. If no elements meet the condition, it returns TRUE; otherwise, it returns FALSE.\n# Check if no number is odd\nnone_odd &lt;- none(numbers, ~ .x %% 2 != 0)\nnone_odd\n\n[1] TRUE\nIn this example, none checks if no elements in the numbers list are odd.\n\n\nPractical Example with Built-in Dataset\nLet’s use the mtcars dataset to demonstrate how these predicate functions can be applied in a practical scenario. Suppose we want to check various conditions on the columns of this dataset.\n# Check if all cars have more than 10 miles per gallon (mpg)\nall_mpg_above_10 &lt;- mtcars %&gt;%\n  select(mpg) %&gt;%\n  map_lgl(~ every(.x, ~ .x &gt; 10))\nall_mpg_above_10\n\nmpg\nTRUE\n\n# Check if some cars have more than 150 horsepower (hp)\nsome_hp_above_150 &lt;- mtcars %&gt;%\n  select(hp) %&gt;%\n  map_lgl(~ some(.x, ~ .x &gt; 150))\nsome_hp_above_150\n\nhp\nTRUE\n\n# Check if no car has more than 8 cylinders\nnone_cyl_above_8 &lt;- mtcars %&gt;%\n  select(cyl) %&gt;%\n  map_lgl(~ none(.x, ~ .x &gt; 8))\nnone_cyl_above_8\n\ncyl\nTRUE\nIn this example:\n\nWe check if all cars in the mtcars dataset have more than 10 mpg using every.\nWe check if some cars have more than 150 horsepower using some.\nWe check if no car has more than 8 cylinders using none.\n\nThese predicate functions provide a straightforward way to validate your data against specific conditions, making your analysis more robust.\n\n\n\nWhat If Not: keep and discard\nWhen you’re working with lists or vectors, you often need to filter elements based on certain conditions. The keep and discard functions from purrr are designed for this purpose. They allow you to retain or remove elements that meet specified criteria, making it easy to clean and subset your data.\n\nkeep\nThe keep function retains elements that satisfy a given predicate. If an element meets the condition, it is kept; otherwise, it is removed.\nlibrary(tidyverse)\n\n# A list of mixed numbers\nnumbers &lt;- list(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n\n# Keep only the even numbers\neven_numbers &lt;- keep(numbers, ~ .x %% 2 == 0)\neven_numbers\n\n[[1]]\n[1] 2\n\n[[2]]\n[1] 4\n\n[[3]]\n[1] 6\n\n[[4]]\n[1] 8\n\n[[5]]\n[1] 10\nIn this example, keep retains only the even numbers from the numbers list.\n\n\ndiscard\nThe discard function removes elements that satisfy a given predicate. If an element meets the condition, it is discarded; otherwise, it is kept.\n# Discard the even numbers\nodd_numbers &lt;- discard(numbers, ~ .x %% 2 == 0)\nodd_numbers\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 3\n\n[[3]]\n[1] 5\n\n[[4]]\n[1] 7\n\n[[5]]\n[1] 9\nHere, discard removes the even numbers, leaving only the odd numbers in the numbers list.\n\n\n\nPractical Example with Built-in Dataset\nLet’s use the iris dataset to demonstrate how keep and discard can be applied in a practical scenario. Suppose we want to filter rows based on specific conditions for the Sepal.Length column.\nlibrary(tidyverse)\n\n# Keep rows where Sepal.Length is greater than 5.0\niris_keep &lt;- iris %&gt;%\n  split(1:nrow(.)) %&gt;%\n  keep(~ .x$Sepal.Length &gt; 5.0) %&gt;%\n  bind_rows()\nhead(iris_keep)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          5.4         3.9          1.7         0.4  setosa\n3          5.4         3.7          1.5         0.2  setosa\n4          5.8         4.0          1.2         0.2  setosa\n5          5.7         4.4          1.5         0.4  setosa\n6          5.4         3.9          1.3         0.4  setosa\n\n# Discard rows where Sepal.Length is less than or equal to 5.0\niris_discard &lt;- iris %&gt;%\n  split(1:nrow(.)) %&gt;%\n  discard(~ .x$Sepal.Length &lt;= 5.0) %&gt;%\n  bind_rows()\nhead(iris_discard)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          5.4         3.9          1.7         0.4  setosa\n3          5.4         3.7          1.5         0.2  setosa\n4          5.8         4.0          1.2         0.2  setosa\n5          5.7         4.4          1.5         0.4  setosa\n6          5.4         3.9          1.3         0.4  setosa\nIn this example:\n\nWe split the iris dataset into a list of rows.\nWe apply keep to retain rows where Sepal.Length is greater than 5.0.\nWe apply discard to remove rows where Sepal.Length is less than or equal to 5.0.\nFinally, we use bind_rows() to combine the list back into a data frame.\n\n\nCombining keep and discard with mtcars\nSimilarly, let’s fix the mtcars example:\n# Keep cars with mpg greater than 20 and discard cars with hp less than 100\nfiltered_cars &lt;- mtcars %&gt;%\n  split(1:nrow(.)) %&gt;%\n  keep(~ .x$mpg &gt; 20) %&gt;%\n  discard(~ .x$hp &lt; 100) %&gt;%\n  bind_rows()\n\nfiltered_cars\n\n                mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4      21.0   6 160.0 110 3.90 2.620 16.46  0  1    4     4\nMazda RX4 Wag  21.0   6 160.0 110 3.90 2.875 17.02  0  1    4     4\nHornet 4 Drive 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3     1\nLotus Europa   30.4   4  95.1 113 3.77 1.513 16.90  1  1    5     2\nVolvo 142E     21.4   4 121.0 109 4.11 2.780 18.60  1  1    4     2\nIn this combined example:\n\nWe split the mtcars dataset into a list of rows.\nWe use keep to retain cars with mpg greater than 20.\nWe use discard to remove cars with hp less than 100.\nWe combine the filtered list back into a data frame using bind_rows().\n\n\n\n\nDo Things in Order of List/Vector: accumulate, reduce\nSometimes, you need to perform cumulative or sequential operations on your data. This is where accumulate and reduce come into play. These functions allow you to apply a function iteratively across elements of a list or vector, either accumulating results at each step or reducing the list to a single value.\n\naccumulate\nThe accumulate function applies a function iteratively to the elements of a list or vector and returns a list of intermediate results.\nLet’s start with a simple example:\nlibrary(tidyverse)\n\n# A list of numbers\nnumbers &lt;- list(1, 2, 3, 4, 5)\n\n# Cumulative sum of the numbers\ncumulative_sum &lt;- accumulate(numbers, `+`)\ncumulative_sum\n\n[1]  1  3  6 10 15\n\n\nreduce\nThe reduce function applies a function iteratively to reduce the elements of a list or vector to a single value.\nHere’s a basic example:\n# Sum of the numbers\ntotal_sum &lt;- reduce(numbers, `+`)\ntotal_sum\n\n[1] 15\n\n\nPractical Example with Built-in Dataset\nLet’s use the mtcars dataset to demonstrate how accumulate and reduce can be applied in a practical scenario.\nUsing accumulate with mtcars\nSuppose we want to calculate the cumulative sum of the miles per gallon (mpg) for each car.\n# Cumulative sum of mpg values\ncumulative_mpg &lt;- mtcars %&gt;%\n  pull(mpg) %&gt;%\n  accumulate(`+`)\ncumulative_mpg\n\n[1]  21.0  42.0  64.8  86.2 104.9 123.0 137.3 161.7 184.5 203.7 221.5 237.9 255.2 270.4 280.8 291.2 305.9 338.3 368.7\n[20] 402.6 424.1 439.6 454.8 468.1 487.3 514.6 540.6 571.0 586.8 606.5 621.5 642.9\nIn this example, accumulate gives us a cumulative sum of the mpg values for the cars in the mtcars dataset.\nUsing reduce with mtcars\nNow, let’s say we want to find the product of all mpg values:\n# Product of mpg values\nproduct_mpg &lt;- mtcars %&gt;%\n  pull(mpg) %&gt;%\n  reduce(`*`)\nproduct_mpg\n\n[1] 1.264241e+41\nIn this example, reduce calculates the product of all mpg values in the mtcars dataset.\n\n\n\nDo It Another Way: compose and negate\nCreating flexible and reusable functions is a hallmark of efficient programming. purrr provides tools like compose and negate to help you build and manipulate functions more effectively. These tools allow you to combine multiple functions into one or invert the logic of a predicate function.\n\ncompose\nThe compose function combines multiple functions into a single function that applies them sequentially. This can be incredibly useful for creating pipelines of operations.\nHere’s a basic example:\nlibrary(tidyverse)\n\n# Define some simple functions\nadd1 &lt;- function(x) x + 1\nsquare &lt;- function(x) x * x\n\n# Compose them into a single function\nadd1_and_square &lt;- compose(square, add1)\n\n# Apply the composed function\nresult &lt;- add1_and_square(2)  # (2 + 1)^2 = 9\nresult\n\n[1] 9\nIn this example:\n\nWe define two simple functions: add1 and square.\nWe use compose to create a new function, add1_and_square, which first adds 1 to its input and then squares the result.\nWe apply the composed function to the number 2, yielding 9.\n\n\n\nPractical Example with Built-in Dataset\nLet’s use compose with a more practical example involving the mtcars dataset. Suppose we want to create a function that first scales the horsepower (hp) by 10 and then calculates the logarithm.\n# Define scaling and log functions\nscale_by_10 &lt;- function(x) x * 10\nsafe_log &lt;- safely(log, otherwise = NA)\n\n# Compose them into a single function\nscale_and_log &lt;- compose(safe_log, scale_by_10)\n\n# Apply the composed function to the hp column\nmtcars &lt;- mtcars %&gt;%\n  mutate(log_scaled_hp = map_dbl(hp, ~ scale_and_log(.x)$result))\n\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb log_scaled_hp\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4     4      7.003065\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4     4      7.003065\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4     1      6.835185\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3     1      7.003065\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3     2      7.467371\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3     1      6.956545\nIn this example:\n\nWe define two functions: scale_by_10 and safe_log.\nWe compose these functions into scale_and_log.\nWe apply the composed function to the hp column of the mtcars dataset and add the results as a new column.\n\n\n\nnegate\nThe negate function creates a new function that returns the logical negation of a predicate function. This is useful when you want to invert the logic of a condition.\nHere’s a simple example:\n# Define a simple predicate function\nis_even &lt;- function(x) x %% 2 == 0\n\n# Negate the predicate function\nis_odd &lt;- negate(is_even)\n\n# Apply the negated function\nresults &lt;- map_lgl(1:10, is_odd)\nresults\n\n [1]  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE\nIn this example:\n\nWe define a predicate function is_even to check if a number is even.\nWe use negate to create a new function is_odd that returns the opposite result.\nWe apply is_odd to the numbers 1 through 10.\n\n\n\nPractical Example with Built-in Dataset\nLet’s use negate in a practical scenario with the iris dataset. Suppose we want to filter out rows where the Sepal.Length is not greater than 5.0.\n# Define a predicate function\nis_long_sepal &lt;- function(x) x &gt; 5.0\n\n# Negate the predicate function\nis_not_long_sepal &lt;- negate(is_long_sepal)\n\n# Filter out rows where Sepal.Length is not greater than 5.0\niris_filtered &lt;- iris %&gt;%\n  split(1:nrow(.)) %&gt;%\n  discard(~ is_not_long_sepal(.x$Sepal.Length)) %&gt;%\n  bind_rows()\n\nhead(iris_filtered)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          5.4         3.9          1.7         0.4  setosa\n3          5.4         3.7          1.5         0.2  setosa\n4          5.8         4.0          1.2         0.2  setosa\n5          5.7         4.4          1.5         0.4  setosa\n6          5.4         3.9          1.3         0.4  setosa\nIn this example:\n\nWe define a predicate function is_long_sepal to check if Sepal.Length is greater than 5.0.\nWe use negate to create a new function is_not_long_sepal that returns the opposite result.\nWe use discard to remove rows where Sepal.Length is not greater than 5.0, then combine the filtered list back into a data frame.\n\nWith compose and negate, you can create more flexible and powerful functions, allowing for more concise and readable code.\n\n\n\nConclusion\nCongratulations! You’ve journeyed through the world of purrr, mastering a wide array of functions and techniques to manipulate and transform your data. From basic mapping to creating powerful function compositions, purrr equips you with tools to make your data wrangling tasks more efficient and expressive.\nWhether you’re applying functions conditionally, dealing with side effects, or validating your data, purrr has you covered. Keep exploring and experimenting with these functions to unlock the full potential of functional programming in R.\n\n\nGift for patient readers\nI decided to give you some useful, yet not trivial use cases of purrr functions.\n\nDefine list of function to apply on data\napply_funs &lt;- function(x, ...) purrr::map_dbl(list(...), ~ .x(x))\nWant to apply multiple functions to a single vector and get a tidy result? Meet apply_funs, your new best friend! This nifty little function takes a value and a bunch of functions, then maps each function to the vector, returning the results as a neat vector.\nLet’s break it down:\n\nx: The value you want to transform.\n...: A bunch of functions you want to apply to x.\npurrr::map_dbl: Maps each function in the list to x and returns the results as a vector of doubles.\n\nSuppose that you want to apply 3 summary functions on vector of numbers. Here’s how you can do it:\nnumber &lt;- 1:48\n\nresults &lt;- apply_funs(number, mean, median, sd)\nresults\n\n[1] 24.5 24.5 14.0\n\n\nUsing pmap as equivalent of Python’s zip\nSometimes you need to zip two tables or columns together. In Python there is zip function for it, but we do not have twin function in R, unless you use pmap. I will not make it longer, so check it out in one of my previous articles.\n\n\nRendering parameterized RMarkdown reports\nAssuming that you have kind of report you use for each salesperson, there is possibility, that you are changing parameters manually to generate report for person X, for date range Y, for product Z. Why not prepare lists of people, time range, and list of products, and then based on them generate series of reports by one click only."
  },
  {
    "objectID": "ds/posts/2022-09-20_Humanist-in-firm-grip-with-world-of-maths--2261ac3f2cd.html",
    "href": "ds/posts/2022-09-20_Humanist-in-firm-grip-with-world-of-maths--2261ac3f2cd.html",
    "title": "Humanist in firm grip with world of maths…",
    "section": "",
    "text": "Let my first post be a short introduction of me as a person. My life and career was not always focused on science, at least strict “mathematical” science.\nMy mind was open to different branches of science. As young kid I was fascinated by chemistry and astronomy. That’s why I still remember latin names of several constellations. Then came period in my life when I have focused on social sciences. Being maybe little bit too young I read Hawking and Hawkins. Then historical books (like for example Norman Davies series about Europe and Poland), world’s mythology and ancient and prehistory. Then came historical novels, political fiction, spy, medical and judicial thrillers. And that was all before I turned 16. I have two qualities which can be considered as blessings, but curses as well. Curiosity and exceptional (not perfect but very capable) memory. Because of this wide spectrum of interests I was not able to choose my further way, and as rather humanist I went to media studies.\nMaybe it is not very humble, but because of knowing history and myself I think about myself as Renessaince man. Of course Leonardo da Vinci or Michalangello Buonarotti is far beyond my reach, but I’d like to follow their steps.\nBut usually (especially in Poland) occupations which are not specialized, doesn’t pay well. I started working in customer service, then sales and logistics and some not really fascinating jobs, when winter came…\nReally quiet winter, especially because of very harsh frost. I started to read some web development tutorials. After few weeks I made simple website, and that was an impulse… to look for some more technical job. And my adventure on borderlands of “real” IT began. After 2 years of MarTech and Marketing Automation tasks I took into world of numbers… as analyst and BI developer.\nHow I see this world? What tools and skills I use in everyday job? Be patient, It’ll come soon."
  },
  {
    "objectID": "ds/posts/2022-09-27_Tools---you-re-defining-them-or-they-define-you--c6f93c650ff7.html",
    "href": "ds/posts/2022-09-27_Tools---you-re-defining-them-or-they-define-you--c6f93c650ff7.html",
    "title": "Tools — you’re defining them or they define you?",
    "section": "",
    "text": "Sometimes I wonder which direction is stronger. I realize that order in which I got to know each tool is really defining the way I work, my routine and way I’m developing my toolbox. As I mentioned in previous post I’m self-educated in a matter of IT and data analytics, so my way was little bit random.\nOf course, first analytical tool I get to know was Excel (wider spreadsheets). Some people said that Excel is the only thing you need, that you can do virtually everything with this program. To a certain degree I would agree with this sentence. I constructed Warhammer Fantasy Roleplay Game 2 ed. character generator in Excel about 14 years ago. But it is also the most underused tools compared to number of computers where it is installed. Even in business Excel, Google Sheets and few other spreadsheets are usually used as tabular notebook and substitute for single-table database.\nSpreadsheets are omnipresent, they are in almost every company, so I use Excel as well, but nowadays only purpose I prefer to use it is data exchange. I use it as data sources or way to deliver transformed data to end user.\nMy next step in data world is not really usual. It is Kibana. This service is a part of ELK environment. As it is not tool designed for analytics or business intelligence, so using it for this purpose was hassle. (At least 6–7 years ago. I didn’t use it since then.) Visualization elements could be used to monitor sales, production using streamed data from Elastic Search precalculated database. I know that it was not the way I should work, but it was only beginning.\nThen comes time of “real analytics” or BI. About 5 years ago I was fascinated with data visualisation and found Tableau. I was delighted that without knowing any programming languages and without deep knowledge of databases, one could perform analysis that is on moderate level and get interesting insights. I really appreciated these opportunities and developed myself. I met some inspiring people, I found some more complex analytics ideas, ideas for new projects. And I realized that Tableau will not be enough, that any BI Tool will not be enough to do analytics. And I not only tried Tableau, but also PowerBI, Qlik and one tool which BI Analysts would say that that is not BI, Google Data Studio.\nThat’s why I turned my eyes to programming languages used for computing and data science: Python and R. Of course there is Scala or Stan as well nowadays, but those was my first choice. Today I would say that perfect situation would be to be bilingual, but for sake of early learning I chose R. Especially because colleagues had basics already.\nNowaday I am big R enthusiast with councious need to develop my Python skills. Last period working as analyst I would divide my time between Tableau and R in 20% to 80% ratio. With lower layer consists of SQL for almost every task and projects. What am I achieving with R: - EDA - further analysis - forecasting - modeling data - visualize data - produce reports - execute ETL processes.\nNext post would be zoomed and focused on one of tools mentioned above. But at the end of this I’ll mention my current tech stack: Tableau, PowerBI (start-level), SQL (MySQL, MariaDB, SQL Server), R language, Python (Pandas and Numpy), and last but not least, Excel."
  },
  {
    "objectID": "ds/posts/2022-10-03_If-you-don-t-know-it--it-s-only-temporary-state--b0487eb38e5a.html",
    "href": "ds/posts/2022-10-03_If-you-don-t-know-it--it-s-only-temporary-state--b0487eb38e5a.html",
    "title": "If you don’t know it, it’s only temporary state…",
    "section": "",
    "text": "I’ve read many headings on Linkedin and other job related, IT-related sites that agreed on one topic. If you are working with data (anyhow) or even want to have analytical role, the first thing you have to learn and master is SQL.\nSome people spell it ‘sequel’, some spell it by a letter… which doesn’t really matter. More important is that this language work with the base concept of data analytics, databases. Important!!! SQL is not programming language, it is Structured Query Language, and it means that you cannot write program with it but rather prepare “data background” for programs using data queries.\nSome of you would say, “Hey, but there are procedures, triggers and other stuff which can be used to perform very difficult and complex tasks”. Of course, just like you can write website in Notepad, animate in Excel and many other weird things using tools and concepts that are not designed to this purpose. And finally I could admit that there is possibility to make analytical job without even touching SQL, but not for long.\nSQL as a language have four main so-called subsets:\n\nDML — data manipulation language — you can manipulate specific records of data. Its commands are: INSERT, UPDATE, DELETE.\nDDL — data definition language — you can manipulate whole structures of data as tables or databases. Commands: CREATE, DROP, ALTER.\nDCL — data control language — you can control users and grant them specific level of privileges. Thats why some users could clear the table, and other not so responsible, should have only access to commands of fourth subset, not to destroy anything. Commands: GRANT, REVOKE, DENY.\nDQL — data query language. May be very small, because has only one command (SELECT), but usual analyst is using this part of SQL.\n\nSo do you need to know every single subset? From my rather short career in data (about 5y) I would say, that it depends in what kind of department you work and what are your collegues competencies. If department have data engineers or ETL specialists, probably DQL will be just enough. But on the other hand, there are teams that have all team of all-embracing individuals. And sometimes these guys just want to test something on database designed by them. Don’t do it at home…\nOr rather exactly do it at home, because some RDBMS can be installed locally on your Windows or Mac. And it can be great opportunity to exercise SQL, but also build “data base” for your web app, machine learning models etc. I already make some attempts to SQL Server Express, MySQL and MariaDB. So called “NO-SQL” databases are still ahead of me in means of make it and play with it.\nAs I worked with Tableau I used Tableau to construct complex queries to optimize refreshing times. In R eRa, almost 90% of tasks started as:\ndata = dbGetQuery(conn, “SELECT ……”).\nDifferent RDBMS have so called flavours and database specific functions, syntax. If you want to master them all be prepared for long time learning. Some people use only a half or even less commands available and doing great job.\nAnd at the end. Do you know what is the smallest correct command which you can use in query?\n“SELECT 5” which gives you only number five in results. And the longest… sky is your limit (and computer performance).\nIn the next post I’ll present you basic elements of SQL language and later some complex stuff to work with JSON’s and other weird things."
  },
  {
    "objectID": "ds/posts/2022-10-18_Sequel-of-SQL--5cbbb8f03a62.html",
    "href": "ds/posts/2022-10-18_Sequel-of-SQL--5cbbb8f03a62.html",
    "title": "Sequel of SQL…",
    "section": "",
    "text": "There are a lot publishing analysts, data scientists and other technical freaks who are not only using knowledge about databases, but also share it. And usually beginnings are exactly the same: SELECT * FROM Table.\nAnd only few paragraphs later (or maybe 1–2 posts) there is a warning not to do it. What exactly? Do not use asterisk unless it is really necessary. That is why I want to warn you in the first post. Asterisk have its purpose, but newbies tend too overuse it. They always want to see ALL.\nIf you’re using databases it will take only time and stress, but there are some services (BigQuery as first in a row) that also cost for data volume which comes from query. Then this simple and friendly asterisk can be really expensive.\nAfter few years using SQL I have also one topic I would be grateful if somebody told me about earlier. That order of executing query is not the same as writing it. And of course first thing to learn is always syntax and then deeper ideas behind it, but I consider execution order is really intuitive as well.\nLet compare this two orders:\nSyntax:\n\nSELECT (opt. DISTINCT) -&gt; FROM -&gt; (opt. JOINs) -&gt; WHERE -&gt; (opt. GROUP BY -&gt; HAVING -&gt; ORDER BY -&gt; LIMIT)\nAnd in human language:\nTake distinct value of this/these fields from that table but connected to other tables, then filter something out and you can also group your data, filter again but using grouped aggregates, then put it in certain order and cut specific number of values. — PRETTY SPAGHETTI\n\nExecution order:\n\nFROM (opt. JOINs) -&gt; WHERE -&gt; GROUP BY -&gt; HAVING -&gt; SELECT -&gt; DISTINCT -&gt; ORDER BY -&gt; LIMIT\n\n\nAnd in less machine way:\nWe are looking in table, but we know that some info is in other tables so we need to join it. We know that not everything is necessary to get so now is the time for filtering. After filtering we can do some stuff not only on row level, but also on groups, which can be again filtered. Now we have our big table in which everything should be included so get to details. I only need this, this and those fields, and one aggregate (for example average, I can do it because I grouped data before). If there are doubles/duplicates I’m getting rid off them with distinct, then put it specific order and maybe limit its numbers. And I have for example 5 cashiers with highest sales last week.\n\nMaybe description is longer, but I feel like this order could represent/reflect my own way of thinking. Sometimes I know what exactly I want to get from tables, but sometimes it is just exploration and execution order is much more natural in mind flow.\nFrankly, it is I think only case when my mind is closer to technical depth of language then its actual syntax.\nNext stop in world of SQL will be logic of sets, which mean exactly… JOINS.\nHave a good time reading."
  },
  {
    "objectID": "ds/posts/2022-10-27_If-you-know-English--you-would-be-able-to-code-in-R--5f80377c74c3.html",
    "href": "ds/posts/2022-10-27_If-you-know-English--you-would-be-able-to-code-in-R--5f80377c74c3.html",
    "title": "If you know English, you would be able to code in R…",
    "section": "",
    "text": "Every programming languages has its own advantages and disadvantages. Every has the most appropriate paradigm (sometimes it is only appropriate…), every has own syntax, but what I’ve observed so far, every programming language finally is not enough for programmers. That’s why new languages are created, but also inside certain languages frameworks, and dialects appeared. And finally that’s why new versions are still under development.\n\nSQL is nice, right? So why limiting rows looks different in MS SQL and MySQL?\nPython is used in versions 2.XX and 3.XX, where many things are made differently.\nWhy we need Angular or React, if we have CSS, JS and HTML?\n* Yes, I know that not every language here is a programming languages, but query and markup languages follow these rules as well.\n\nAnd what is the answer for questions above?\nEvolution.\nSome people just used certain languages and get the ideas like:\n- Why not do that this way?\n- Ok, I understand but I need someone else to understand it as well.\n- Hey, all is understandable, but these words looks unfriendly.\n- I need this language to be more able to transfer my thoughts.\nBut what was at the very beginning? As usually in computer science… Zeroes and ones. This is so called machine language which tells computer what kind of sequence means what actions and results. But we understand nothing at all (except individuals).\nThen comes second generation of programming languages — assemblers. Usually there are no more only zeroes and ones. Hexadecimal system appears for example. It still not readable for human being, except experts, but one operation written in this language is still one operation on processor.\nLater comes the third generation (3GL), when common people could finally guess what is going on. Abstraction goes up, but performance weakens.\nBut why? Development should mean being better in performance, shouldn’t it? Like in biological evolution: bacteria feed itself much faster and much more effective than mammals, because processes are simpler (under the hood), not looks simpler. For example elephant has to spend huge amount of energy to gain some. Process looks simpler, but is cosmically more difficult inside.\n3GL languages are like higher forms of animal evolution. We see it as much easier to read and even write, but there is some “magic” involved. This magic is translation to lower level language to machine language at the end. and this translation is the reason why performance is the cost of nicer language.\nIn third generation there are: all C’s (C, Objective-C, C++, C#), Python, Scala, Ruby, Java, Fortran, BASIC and many more. They are difference between them, some are more difficult, some easier, they use different paradigms, but usually they are general purpose languages.\nAnd here comes the knight on white horse… the fourth generation of languages. I omitted word “programming”, because not all of them are strictly programming languages. In this generation there are usually highly specialized domain specific or purpose specific languages as SQL, Matlab or our long awaited friend… R.\nBut they are usually very readable and understandable for common person. And I said few paragraphs above, they have to be translated to lower levels, what cost some performance. From my experience speed of writing usually rewards speed of execution.\nWhat was this long story above for? Because this was another thing about computer science that I learned about not early enough. This story could show you if your journey with data science is not starting in wrong place. It can let you know that your level of abstraction is closer to another languages, without kicking you out from programming world.\nIn post title I mentioned that if you speak English (or maybe even know English on “understanding” level), you would be able to code in R. Probably the same could be said about another high level languages, but I’ll focus on R. Why?\n- because it is almost pure language (with its own grammar, even grammars ;D)\n- because it is domain specific for position I worked and work now: Data analysis.\nAs I wrote above R has grammars, but what does it mean. That like in other languages there are some dialects, which can change many things, from readability to performance.\nLet me tell you about few basic. There is base R where you write as creators of language wanted you to do it, then there is “tidy R” with philosophy of tidy (tabular) data and Hadley Wickham, and finally “data.table” which comes with better performance, but looks little bit less readable on first sight. I personally prefer tidy approach.\nOh, and there is also grammar of graphics in ggplot library based on Leland Wilkinson idea about grammar of graphics, and few smaller.\nAnd finally proof for claim from the title. Imagine that you have database/table/datasource about pupils in schools in your county containing age, class, weight, height and gender. And here is your sample code ( %&gt;% should be read as “and then”).\nschool_kids %&gt;%\nfilter(age == 12) %&gt;%\ngroup_by(gender) %&gt;%\nsummarise(mean_weight = mean(weight), mean_height = mean(height)) \nAnd in English:\nTAKE school_kids TABLE AND THEN\nFILTER KIDS WHO ARE 12 YO AND THEN\nGROUP THEM BY gender AND THEN\nAND GIVE ME THEIR mean weight and height.\nThis so called piping (or chaining) can be much longer and more sophisticated, but this way of writing could represent human order of thinking which in domain like data analysis or data science can be very big facilitation.\nJust learn English, if it is not your native language.\nMy next post will be next step into world of R and specifically “tidyverse”."
  }
]